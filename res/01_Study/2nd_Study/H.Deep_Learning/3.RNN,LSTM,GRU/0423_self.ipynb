{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9803bf9e-1224-43fa-a1a2-9096fce28afd",
   "metadata": {},
   "source": [
    "- 워드 임베딩 : 단어를 벡터(행렬)로 표현\n",
    "    - 희소 표현 (원-핫 벡터)\n",
    "        - 원핫인코딩과 유사, 표현하고자 하는 단어 인덱스 값만 1이고 나머지는 0으로 표현되는 벡터로 표현하는 방법\n",
    "        - 공간적인 낭비가 심하고 계산 속도가 느림\n",
    "    - 밀집 표현 (토큰화)\n",
    "        - 벡터 차원을 분석가가 정한 임의의 값으로 모든 단어 벡터의 차원을 맞춤\n",
    "        - 희소 표현에 비해 벡터의 차원이 조밀해지므로 밀집 벡터라고 부름\n",
    "            - 텐서플로우 - 케라스에서는 토큰화된 단어 벡터를 임베딩 층을 통과시켜 RNN, LSTM, GRU를 이용하여 학습이 가능하도록 처리됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997c706-9c57-4407-89c3-938fe23e3a4e",
   "metadata": {},
   "source": [
    "- 텐서플로우 토큰화 객체 : Tokenizer\n",
    "    - num_words : 인식 가능한 최대 벡터 길이(단어 수)를 지정\n",
    "    - oov_token : (검증 데이터에) 학습 데이터에 포함되지 않은 단어가 있을 경우 대체할 단어"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf86f2-c687-4060-bc9b-4c033d75e5aa",
   "metadata": {},
   "source": [
    "- 한글의 토큰화\n",
    "    - 불용어 : 분석에 쓰지 않을 불필요한 단어\n",
    "        - 해당 불용어를 문장에서 제거할 필요가 있음\n",
    "    - 형태소 분리를 먼저 진행하고 불용어를 제거한 이후 한글의 토큰화를 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba622b6f-1a22-425c-b5eb-acb2cb02c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 로드\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# from hossam.util import *\n",
    "# from hossam.plot import *\n",
    "# from hossam.tensor import *\n",
    "\n",
    "import requests\n",
    "from konlpy.tag import Mecab\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b489da1-0ae7-4f66-b4aa-7be002d2e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 샘플 데이터\n",
    "train_text = [\"You are the Best\", \"You are the Nice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26e7d91-c7be-42ea-ab7a-235720a1c0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.preprocessing.text.Tokenizer at 0x7fd0792a39a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화 객체 생성\n",
    "tokenizer = Tokenizer(num_words=10, oov_token=\"<OOV>\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16482f99-fe34-4ec1-a38d-79c2a0f01b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'you': 2, 'are': 3, 'the': 4, 'best': 5, 'nice': 6}\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 학습\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# 각 단어에 부여된 인덱스 번호 확인\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d2d6be-5061-494c-8005-b116efa611d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 4, 5], [1, 3, 4, 6]]\n"
     ]
    }
   ],
   "source": [
    "# 학습 결과의 적용\n",
    "train_text = [\"We are the Best\", \"We are the Nice\"]\n",
    "sequences = tokenizer.texts_to_sequences(train_text)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80c21061-284f-4b59-9424-df45f78c4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 토큰화 샘플 데이터\n",
    "\n",
    "poem = \"\"\"\n",
    "흘러내린 머리카락이 흐린 호박빛 아래 빛난다.\n",
    "난 유영한다. 차분하게 과거에 살면서 현재의 공기를 마신다.\n",
    "가로등이 깜빡인다.\n",
    "나도 깜빡여준다.\n",
    "머리카락이 흩날린다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae94fb6-851f-4cfc-8719-070831128eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가', '가까스로', '가령', '각', '각각']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 목록 로드\n",
    "with open('./data/stopwords-ko.txt') as f:\n",
    "    tmp = f.readlines()\n",
    "    stopwords = [i.replace(\"\\n\", \"\") for i in tmp]\n",
    "\n",
    "stopwords[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4902fffd-7ec9-4646-8c11-b7fd3a12711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['머리카락', '호박', '빛', '아래', '난', '유영', '과거', '현재', '공기', '가로등', '나', '머리카락']\n"
     ]
    }
   ],
   "source": [
    "# Token(형태소) 분리\n",
    "if sys.platform == \"win32\":\n",
    "    mecab = Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")\n",
    "else:\n",
    "    mecab = Mecab()\n",
    "    \n",
    "nouns = mecab.nouns(poem)\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83474a56-ddfa-44ea-b57a-db3c0250854d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['머리카락', '호박', '빛', '아래', '난', '유영', '과거', '현재', '공기', '가로등', '머리카락']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 추출된 명사에서 불용어를 제외하여 새로운 리스트 생성\n",
    "train_text = [x for x in nouns if x not in stopwords]\n",
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b66d975-38d3-45cf-82aa-58eb0ce25dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, '머리카락': 2, '호박': 3, '빛': 4, '아래': 5, '난': 6, '유영': 7, '과거': 8, '현재': 9, '공기': 10, '가로등': 11}\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 토큰화\n",
    "tokenizer = Tokenizer(num_words=len(nouns), oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "print(tokenizer.word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
