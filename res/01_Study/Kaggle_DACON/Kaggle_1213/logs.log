2023-12-18 22:24:50,478:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:24:50,479:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:24:50,479:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:24:50,480:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:00,862:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:00,863:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:00,864:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:00,864:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:44,651:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:44,652:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:44,652:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:44,653:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:29:45,514:INFO:PyCaret ClassificationExperiment
2023-12-18 22:29:45,514:INFO:Logging name: clf-default-name
2023-12-18 22:29:45,515:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-18 22:29:45,515:INFO:version 3.2.0
2023-12-18 22:29:45,515:INFO:Initializing setup()
2023-12-18 22:29:45,515:INFO:self.USI: dc26
2023-12-18 22:29:45,516:INFO:self._variable_keys: {'n_jobs_param', 'y_train', 'seed', 'html_param', '_available_plots', 'logging_param', 'USI', 'log_plots_param', 'X_train', 'is_multiclass', 'target_param', 'gpu_n_jobs_param', 'X', 'data', 'gpu_param', 'exp_name_log', 'pipeline', 'memory', '_ml_usecase', 'y_test', 'idx', 'fold_shuffle_param', 'fix_imbalance', 'y', 'fold_groups_param', 'exp_id', 'X_test', 'fold_generator'}
2023-12-18 22:29:45,516:INFO:Checking environment
2023-12-18 22:29:45,516:INFO:python_version: 3.10.9
2023-12-18 22:29:45,517:INFO:python_build: ('main', 'Mar  1 2023 18:18:15')
2023-12-18 22:29:45,517:INFO:machine: AMD64
2023-12-18 22:29:45,517:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-18 22:29:45,518:INFO:Memory: svmem(total=34190274560, available=21021925376, percent=38.5, used=13168349184, free=21021925376)
2023-12-18 22:29:45,518:INFO:Physical Core: 24
2023-12-18 22:29:45,518:INFO:Logical Core: 32
2023-12-18 22:29:45,518:INFO:Checking libraries
2023-12-18 22:29:45,519:INFO:System:
2023-12-18 22:29:45,519:INFO:    python: 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]
2023-12-18 22:29:45,519:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-18 22:29:45,519:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-18 22:29:45,520:INFO:PyCaret required dependencies:
2023-12-18 22:29:45,526:INFO:                 pip: 22.3.1
2023-12-18 22:29:45,527:INFO:          setuptools: 65.6.3
2023-12-18 22:29:45,527:INFO:             pycaret: 3.2.0
2023-12-18 22:29:45,527:INFO:             IPython: 8.10.0
2023-12-18 22:29:45,528:INFO:          ipywidgets: 7.6.5
2023-12-18 22:29:45,528:INFO:                tqdm: 4.64.1
2023-12-18 22:29:45,528:INFO:               numpy: 1.23.5
2023-12-18 22:29:45,528:INFO:              pandas: 1.5.3
2023-12-18 22:29:45,529:INFO:              jinja2: 3.1.2
2023-12-18 22:29:45,529:INFO:               scipy: 1.10.1
2023-12-18 22:29:45,529:INFO:              joblib: 1.3.2
2023-12-18 22:29:45,530:INFO:             sklearn: 1.2.1
2023-12-18 22:29:45,530:INFO:                pyod: 1.1.2
2023-12-18 22:29:45,531:INFO:            imblearn: 0.10.1
2023-12-18 22:29:45,531:INFO:   category_encoders: 2.6.3
2023-12-18 22:29:45,531:INFO:            lightgbm: 4.1.0
2023-12-18 22:29:45,532:INFO:               numba: 0.56.4
2023-12-18 22:29:45,532:INFO:            requests: 2.28.1
2023-12-18 22:29:45,532:INFO:          matplotlib: 3.6.0
2023-12-18 22:29:45,533:INFO:          scikitplot: 0.3.7
2023-12-18 22:29:45,533:INFO:         yellowbrick: 1.5
2023-12-18 22:29:45,533:INFO:              plotly: 5.9.0
2023-12-18 22:29:45,534:INFO:    plotly-resampler: Not installed
2023-12-18 22:29:45,534:INFO:             kaleido: 0.2.1
2023-12-18 22:29:45,534:INFO:           schemdraw: 0.15
2023-12-18 22:29:45,534:INFO:         statsmodels: 0.13.5
2023-12-18 22:29:45,535:INFO:              sktime: 0.21.1
2023-12-18 22:29:45,535:INFO:               tbats: 1.1.3
2023-12-18 22:29:45,535:INFO:            pmdarima: 2.0.4
2023-12-18 22:29:45,536:INFO:              psutil: 5.9.0
2023-12-18 22:29:45,536:INFO:          markupsafe: 2.1.1
2023-12-18 22:29:45,536:INFO:             pickle5: Not installed
2023-12-18 22:29:45,536:INFO:         cloudpickle: 2.0.0
2023-12-18 22:29:45,537:INFO:         deprecation: 2.1.0
2023-12-18 22:29:45,537:INFO:              xxhash: 3.4.1
2023-12-18 22:29:45,537:INFO:           wurlitzer: Not installed
2023-12-18 22:29:45,537:INFO:PyCaret optional dependencies:
2023-12-18 22:29:45,589:INFO:                shap: Not installed
2023-12-18 22:29:45,589:INFO:           interpret: Not installed
2023-12-18 22:29:45,590:INFO:                umap: Not installed
2023-12-18 22:29:45,590:INFO:     ydata_profiling: Not installed
2023-12-18 22:29:45,590:INFO:  explainerdashboard: Not installed
2023-12-18 22:29:45,590:INFO:             autoviz: Not installed
2023-12-18 22:29:45,591:INFO:           fairlearn: Not installed
2023-12-18 22:29:45,591:INFO:          deepchecks: Not installed
2023-12-18 22:29:45,591:INFO:             xgboost: 2.0.2
2023-12-18 22:29:45,591:INFO:            catboost: Not installed
2023-12-18 22:29:45,591:INFO:              kmodes: Not installed
2023-12-18 22:29:45,592:INFO:             mlxtend: Not installed
2023-12-18 22:29:45,592:INFO:       statsforecast: Not installed
2023-12-18 22:29:45,592:INFO:        tune_sklearn: Not installed
2023-12-18 22:29:45,593:INFO:                 ray: Not installed
2023-12-18 22:29:45,593:INFO:            hyperopt: Not installed
2023-12-18 22:29:45,593:INFO:              optuna: Not installed
2023-12-18 22:29:45,593:INFO:               skopt: Not installed
2023-12-18 22:29:45,594:INFO:              mlflow: Not installed
2023-12-18 22:29:45,594:INFO:              gradio: Not installed
2023-12-18 22:29:45,594:INFO:             fastapi: Not installed
2023-12-18 22:29:45,594:INFO:             uvicorn: Not installed
2023-12-18 22:29:45,595:INFO:              m2cgen: Not installed
2023-12-18 22:29:45,595:INFO:           evidently: Not installed
2023-12-18 22:29:45,595:INFO:               fugue: Not installed
2023-12-18 22:29:45,596:INFO:           streamlit: Not installed
2023-12-18 22:29:45,596:INFO:             prophet: Not installed
2023-12-18 22:29:45,596:INFO:None
2023-12-18 22:29:45,596:INFO:Set up data.
2023-12-18 22:29:45,602:INFO:Set up folding strategy.
2023-12-18 22:29:45,602:INFO:Set up train/test split.
2023-12-18 22:29:45,606:INFO:Set up index.
2023-12-18 22:29:45,607:INFO:Assigning column types.
2023-12-18 22:29:45,609:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-18 22:29:45,630:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:29:45,632:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:29:45,649:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:29:45,651:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:29:45,672:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:29:45,673:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:29:45,687:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:29:45,688:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:29:45,688:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-18 22:29:45,709:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:29:45,722:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:29:45,724:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:29:45,745:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:29:45,760:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:29:45,762:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:29:45,762:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-18 22:29:45,796:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:29:45,798:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:29:45,832:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:29:45,834:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:29:45,836:INFO:Preparing preprocessing pipeline...
2023-12-18 22:29:45,837:INFO:Set up simple imputation.
2023-12-18 22:29:45,838:INFO:Set up column name cleaning.
2023-12-18 22:29:45,855:INFO:Finished creating preprocessing pipeline.
2023-12-18 22:29:45,858:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'Hepatomegaly_N', 'Hepatomegaly_Y',
                                             'Spiders_N', 'Spiders_Y',
                                             'Edema_N', 'Edema_S', 'Edema_Y'],
                                    transformer=S...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-18 22:29:45,859:INFO:Creating final display dataframe.
2023-12-18 22:29:45,908:INFO:Setup _display_container:                     Description             Value
0                    Session id              3943
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 26)
4        Transformed data shape        (7905, 26)
5   Transformed train set shape        (6719, 26)
6    Transformed test set shape        (1186, 26)
7              Numeric features                16
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              dc26
2023-12-18 22:29:45,948:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:29:45,950:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:29:45,983:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:29:45,985:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:29:45,986:INFO:setup() successfully completed in 0.48s...............
2023-12-18 22:30:32,477:INFO:PyCaret ClassificationExperiment
2023-12-18 22:30:32,478:INFO:Logging name: clf-default-name
2023-12-18 22:30:32,478:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-18 22:30:32,478:INFO:version 3.2.0
2023-12-18 22:30:32,479:INFO:Initializing setup()
2023-12-18 22:30:32,479:INFO:self.USI: 48d7
2023-12-18 22:30:32,480:INFO:self._variable_keys: {'n_jobs_param', 'y_train', 'seed', 'html_param', '_available_plots', 'logging_param', 'USI', 'log_plots_param', 'X_train', 'is_multiclass', 'target_param', 'gpu_n_jobs_param', 'X', 'data', 'gpu_param', 'exp_name_log', 'pipeline', 'memory', '_ml_usecase', 'y_test', 'idx', 'fold_shuffle_param', 'fix_imbalance', 'y', 'fold_groups_param', 'exp_id', 'X_test', 'fold_generator'}
2023-12-18 22:30:32,480:INFO:Checking environment
2023-12-18 22:30:32,480:INFO:python_version: 3.10.9
2023-12-18 22:30:32,481:INFO:python_build: ('main', 'Mar  1 2023 18:18:15')
2023-12-18 22:30:32,481:INFO:machine: AMD64
2023-12-18 22:30:32,481:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-18 22:30:32,481:INFO:Memory: svmem(total=34190274560, available=21042896896, percent=38.5, used=13147377664, free=21042896896)
2023-12-18 22:30:32,482:INFO:Physical Core: 24
2023-12-18 22:30:32,482:INFO:Logical Core: 32
2023-12-18 22:30:32,483:INFO:Checking libraries
2023-12-18 22:30:32,483:INFO:System:
2023-12-18 22:30:32,483:INFO:    python: 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]
2023-12-18 22:30:32,484:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-18 22:30:32,484:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-18 22:30:32,484:INFO:PyCaret required dependencies:
2023-12-18 22:30:32,485:INFO:                 pip: 22.3.1
2023-12-18 22:30:32,485:INFO:          setuptools: 65.6.3
2023-12-18 22:30:32,485:INFO:             pycaret: 3.2.0
2023-12-18 22:30:32,485:INFO:             IPython: 8.10.0
2023-12-18 22:30:32,486:INFO:          ipywidgets: 7.6.5
2023-12-18 22:30:32,486:INFO:                tqdm: 4.64.1
2023-12-18 22:30:32,486:INFO:               numpy: 1.23.5
2023-12-18 22:30:32,486:INFO:              pandas: 1.5.3
2023-12-18 22:30:32,487:INFO:              jinja2: 3.1.2
2023-12-18 22:30:32,487:INFO:               scipy: 1.10.1
2023-12-18 22:30:32,487:INFO:              joblib: 1.3.2
2023-12-18 22:30:32,487:INFO:             sklearn: 1.2.1
2023-12-18 22:30:32,488:INFO:                pyod: 1.1.2
2023-12-18 22:30:32,488:INFO:            imblearn: 0.10.1
2023-12-18 22:30:32,488:INFO:   category_encoders: 2.6.3
2023-12-18 22:30:32,488:INFO:            lightgbm: 4.1.0
2023-12-18 22:30:32,489:INFO:               numba: 0.56.4
2023-12-18 22:30:32,489:INFO:            requests: 2.28.1
2023-12-18 22:30:32,489:INFO:          matplotlib: 3.6.0
2023-12-18 22:30:32,490:INFO:          scikitplot: 0.3.7
2023-12-18 22:30:32,490:INFO:         yellowbrick: 1.5
2023-12-18 22:30:32,490:INFO:              plotly: 5.9.0
2023-12-18 22:30:32,490:INFO:    plotly-resampler: Not installed
2023-12-18 22:30:32,491:INFO:             kaleido: 0.2.1
2023-12-18 22:30:32,491:INFO:           schemdraw: 0.15
2023-12-18 22:30:32,491:INFO:         statsmodels: 0.13.5
2023-12-18 22:30:32,492:INFO:              sktime: 0.21.1
2023-12-18 22:30:32,492:INFO:               tbats: 1.1.3
2023-12-18 22:30:32,492:INFO:            pmdarima: 2.0.4
2023-12-18 22:30:32,492:INFO:              psutil: 5.9.0
2023-12-18 22:30:32,493:INFO:          markupsafe: 2.1.1
2023-12-18 22:30:32,493:INFO:             pickle5: Not installed
2023-12-18 22:30:32,493:INFO:         cloudpickle: 2.0.0
2023-12-18 22:30:32,494:INFO:         deprecation: 2.1.0
2023-12-18 22:30:32,494:INFO:              xxhash: 3.4.1
2023-12-18 22:30:32,494:INFO:           wurlitzer: Not installed
2023-12-18 22:30:32,494:INFO:PyCaret optional dependencies:
2023-12-18 22:30:32,495:INFO:                shap: Not installed
2023-12-18 22:30:32,495:INFO:           interpret: Not installed
2023-12-18 22:30:32,495:INFO:                umap: Not installed
2023-12-18 22:30:32,495:INFO:     ydata_profiling: Not installed
2023-12-18 22:30:32,496:INFO:  explainerdashboard: Not installed
2023-12-18 22:30:32,496:INFO:             autoviz: Not installed
2023-12-18 22:30:32,496:INFO:           fairlearn: Not installed
2023-12-18 22:30:32,496:INFO:          deepchecks: Not installed
2023-12-18 22:30:32,496:INFO:             xgboost: 2.0.2
2023-12-18 22:30:32,497:INFO:            catboost: Not installed
2023-12-18 22:30:32,497:INFO:              kmodes: Not installed
2023-12-18 22:30:32,497:INFO:             mlxtend: Not installed
2023-12-18 22:30:32,497:INFO:       statsforecast: Not installed
2023-12-18 22:30:32,497:INFO:        tune_sklearn: Not installed
2023-12-18 22:30:32,497:INFO:                 ray: Not installed
2023-12-18 22:30:32,498:INFO:            hyperopt: Not installed
2023-12-18 22:30:32,498:INFO:              optuna: Not installed
2023-12-18 22:30:32,498:INFO:               skopt: Not installed
2023-12-18 22:30:32,498:INFO:              mlflow: Not installed
2023-12-18 22:30:32,499:INFO:              gradio: Not installed
2023-12-18 22:30:32,499:INFO:             fastapi: Not installed
2023-12-18 22:30:32,499:INFO:             uvicorn: Not installed
2023-12-18 22:30:32,499:INFO:              m2cgen: Not installed
2023-12-18 22:30:32,500:INFO:           evidently: Not installed
2023-12-18 22:30:32,500:INFO:               fugue: Not installed
2023-12-18 22:30:32,500:INFO:           streamlit: Not installed
2023-12-18 22:30:32,500:INFO:             prophet: Not installed
2023-12-18 22:30:32,500:INFO:None
2023-12-18 22:30:32,501:INFO:Set up data.
2023-12-18 22:30:32,505:INFO:Set up folding strategy.
2023-12-18 22:30:32,506:INFO:Set up train/test split.
2023-12-18 22:30:32,509:INFO:Set up index.
2023-12-18 22:30:32,510:INFO:Assigning column types.
2023-12-18 22:30:32,512:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-18 22:30:32,532:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:30:32,533:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:30:32,547:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:30:32,548:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:30:32,569:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:30:32,570:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:30:32,583:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:30:32,586:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:30:32,586:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-18 22:30:32,607:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:30:32,621:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:30:32,623:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:30:32,644:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:30:32,657:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:30:32,660:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:30:32,661:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-18 22:30:32,694:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:30:32,697:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:30:32,730:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:30:32,733:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:30:32,734:INFO:Preparing preprocessing pipeline...
2023-12-18 22:30:32,735:INFO:Set up simple imputation.
2023-12-18 22:30:32,736:INFO:Set up column name cleaning.
2023-12-18 22:30:32,750:INFO:Finished creating preprocessing pipeline.
2023-12-18 22:30:32,753:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'Hepatomegaly_N', 'Hepatomegaly_Y',
                                             'Spiders_N', 'Spiders_Y',
                                             'Edema_N', 'Edema_S', 'Edema_Y'],
                                    transformer=S...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-18 22:30:32,753:INFO:Creating final display dataframe.
2023-12-18 22:30:32,802:INFO:Setup _display_container:                     Description             Value
0                    Session id              6247
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 26)
4        Transformed data shape        (7905, 26)
5   Transformed train set shape        (6719, 26)
6    Transformed test set shape        (1186, 26)
7              Numeric features                16
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              48d7
2023-12-18 22:30:32,839:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:30:32,841:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:30:32,875:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:30:32,878:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:30:32,879:INFO:setup() successfully completed in 0.4s...............
2023-12-18 22:31:42,748:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:31:42,749:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:31:42,749:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:31:42,750:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:31:47,697:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:31:47,698:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:31:47,698:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:31:47,699:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:31:48,572:INFO:PyCaret ClassificationExperiment
2023-12-18 22:31:48,572:INFO:Logging name: clf-default-name
2023-12-18 22:31:48,573:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-18 22:31:48,573:INFO:version 3.2.0
2023-12-18 22:31:48,573:INFO:Initializing setup()
2023-12-18 22:31:48,573:INFO:self.USI: 56e4
2023-12-18 22:31:48,573:INFO:self._variable_keys: {'y', 'is_multiclass', 'idx', 'X_train', 'pipeline', 'fold_groups_param', 'gpu_param', 'target_param', 'USI', 'exp_name_log', 'gpu_n_jobs_param', 'html_param', 'fold_generator', 'y_test', 'fold_shuffle_param', 'y_train', 'data', 'log_plots_param', 'n_jobs_param', 'X_test', 'fix_imbalance', '_available_plots', 'logging_param', '_ml_usecase', 'X', 'seed', 'memory', 'exp_id'}
2023-12-18 22:31:48,574:INFO:Checking environment
2023-12-18 22:31:48,574:INFO:python_version: 3.10.9
2023-12-18 22:31:48,574:INFO:python_build: ('main', 'Mar  1 2023 18:18:15')
2023-12-18 22:31:48,574:INFO:machine: AMD64
2023-12-18 22:31:48,575:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-18 22:31:48,575:INFO:Memory: svmem(total=34190274560, available=20423761920, percent=40.3, used=13766512640, free=20423761920)
2023-12-18 22:31:48,575:INFO:Physical Core: 24
2023-12-18 22:31:48,575:INFO:Logical Core: 32
2023-12-18 22:31:48,576:INFO:Checking libraries
2023-12-18 22:31:48,576:INFO:System:
2023-12-18 22:31:48,576:INFO:    python: 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]
2023-12-18 22:31:48,577:INFO:executable: C:\Users\yjg10\anaconda3\python.exe
2023-12-18 22:31:48,577:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-18 22:31:48,577:INFO:PyCaret required dependencies:
2023-12-18 22:31:48,584:INFO:                 pip: 22.3.1
2023-12-18 22:31:48,584:INFO:          setuptools: 65.6.3
2023-12-18 22:31:48,584:INFO:             pycaret: 3.2.0
2023-12-18 22:31:48,584:INFO:             IPython: 8.10.0
2023-12-18 22:31:48,585:INFO:          ipywidgets: 7.6.5
2023-12-18 22:31:48,585:INFO:                tqdm: 4.64.1
2023-12-18 22:31:48,585:INFO:               numpy: 1.23.5
2023-12-18 22:31:48,585:INFO:              pandas: 1.5.3
2023-12-18 22:31:48,585:INFO:              jinja2: 3.1.2
2023-12-18 22:31:48,586:INFO:               scipy: 1.10.1
2023-12-18 22:31:48,586:INFO:              joblib: 1.3.2
2023-12-18 22:31:48,586:INFO:             sklearn: 1.2.1
2023-12-18 22:31:48,586:INFO:                pyod: 1.1.2
2023-12-18 22:31:48,586:INFO:            imblearn: 0.10.1
2023-12-18 22:31:48,587:INFO:   category_encoders: 2.6.3
2023-12-18 22:31:48,587:INFO:            lightgbm: 4.1.0
2023-12-18 22:31:48,587:INFO:               numba: 0.56.4
2023-12-18 22:31:48,587:INFO:            requests: 2.28.1
2023-12-18 22:31:48,587:INFO:          matplotlib: 3.6.0
2023-12-18 22:31:48,588:INFO:          scikitplot: 0.3.7
2023-12-18 22:31:48,588:INFO:         yellowbrick: 1.5
2023-12-18 22:31:48,588:INFO:              plotly: 5.9.0
2023-12-18 22:31:48,588:INFO:    plotly-resampler: Not installed
2023-12-18 22:31:48,588:INFO:             kaleido: 0.2.1
2023-12-18 22:31:48,589:INFO:           schemdraw: 0.15
2023-12-18 22:31:48,589:INFO:         statsmodels: 0.13.5
2023-12-18 22:31:48,589:INFO:              sktime: 0.21.1
2023-12-18 22:31:48,589:INFO:               tbats: 1.1.3
2023-12-18 22:31:48,589:INFO:            pmdarima: 2.0.4
2023-12-18 22:31:48,589:INFO:              psutil: 5.9.0
2023-12-18 22:31:48,590:INFO:          markupsafe: 2.1.1
2023-12-18 22:31:48,590:INFO:             pickle5: Not installed
2023-12-18 22:31:48,590:INFO:         cloudpickle: 2.0.0
2023-12-18 22:31:48,590:INFO:         deprecation: 2.1.0
2023-12-18 22:31:48,590:INFO:              xxhash: 3.4.1
2023-12-18 22:31:48,591:INFO:           wurlitzer: Not installed
2023-12-18 22:31:48,591:INFO:PyCaret optional dependencies:
2023-12-18 22:31:48,638:INFO:                shap: Not installed
2023-12-18 22:31:48,638:INFO:           interpret: Not installed
2023-12-18 22:31:48,638:INFO:                umap: Not installed
2023-12-18 22:31:48,639:INFO:     ydata_profiling: Not installed
2023-12-18 22:31:48,639:INFO:  explainerdashboard: Not installed
2023-12-18 22:31:48,640:INFO:             autoviz: Not installed
2023-12-18 22:31:48,640:INFO:           fairlearn: Not installed
2023-12-18 22:31:48,640:INFO:          deepchecks: Not installed
2023-12-18 22:31:48,640:INFO:             xgboost: 2.0.2
2023-12-18 22:31:48,641:INFO:            catboost: Not installed
2023-12-18 22:31:48,641:INFO:              kmodes: Not installed
2023-12-18 22:31:48,641:INFO:             mlxtend: Not installed
2023-12-18 22:31:48,641:INFO:       statsforecast: Not installed
2023-12-18 22:31:48,641:INFO:        tune_sklearn: Not installed
2023-12-18 22:31:48,642:INFO:                 ray: Not installed
2023-12-18 22:31:48,642:INFO:            hyperopt: Not installed
2023-12-18 22:31:48,642:INFO:              optuna: Not installed
2023-12-18 22:31:48,642:INFO:               skopt: Not installed
2023-12-18 22:31:48,642:INFO:              mlflow: Not installed
2023-12-18 22:31:48,643:INFO:              gradio: Not installed
2023-12-18 22:31:48,643:INFO:             fastapi: Not installed
2023-12-18 22:31:48,643:INFO:             uvicorn: Not installed
2023-12-18 22:31:48,643:INFO:              m2cgen: Not installed
2023-12-18 22:31:48,643:INFO:           evidently: Not installed
2023-12-18 22:31:48,644:INFO:               fugue: Not installed
2023-12-18 22:31:48,644:INFO:           streamlit: Not installed
2023-12-18 22:31:48,644:INFO:             prophet: Not installed
2023-12-18 22:31:48,644:INFO:None
2023-12-18 22:31:48,644:INFO:Set up data.
2023-12-18 22:31:48,649:INFO:Set up folding strategy.
2023-12-18 22:31:48,650:INFO:Set up train/test split.
2023-12-18 22:31:48,654:INFO:Set up index.
2023-12-18 22:31:48,654:INFO:Assigning column types.
2023-12-18 22:31:48,657:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-18 22:31:48,678:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:31:48,680:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:31:48,696:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:31:48,698:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:31:48,720:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:31:48,721:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:31:48,735:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:31:48,737:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:31:48,737:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-18 22:31:48,758:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:31:48,771:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:31:48,773:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:31:48,793:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:31:48,807:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:31:48,809:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:31:48,809:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-18 22:31:48,843:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:31:48,845:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:31:48,878:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:31:48,880:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:31:48,881:INFO:Preparing preprocessing pipeline...
2023-12-18 22:31:48,883:INFO:Set up simple imputation.
2023-12-18 22:31:48,883:INFO:Set up column name cleaning.
2023-12-18 22:31:48,905:INFO:Finished creating preprocessing pipeline.
2023-12-18 22:31:48,909:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'Hepatomegaly_N', 'Hepatomegaly_Y',
                                             'Spiders_N', 'Spiders_Y',
                                             'Edema_N', 'Edema_S', 'Edema_Y'],
                                    transformer=S...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-18 22:31:48,910:INFO:Creating final display dataframe.
2023-12-18 22:31:48,961:INFO:Setup _display_container:                     Description             Value
0                    Session id              4995
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 26)
4        Transformed data shape        (7905, 26)
5   Transformed train set shape        (6719, 26)
6    Transformed test set shape        (1186, 26)
7              Numeric features                16
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              56e4
2023-12-18 22:31:48,998:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:31:49,000:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:31:49,034:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:31:49,037:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:31:49,037:INFO:setup() successfully completed in 0.47s...............
2023-12-18 22:33:39,780:INFO:Initializing compare_models()
2023-12-18 22:33:39,780:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-12-18 22:33:39,781:INFO:Checking exceptions
2023-12-18 22:33:39,784:INFO:Preparing display monitor
2023-12-18 22:33:39,798:INFO:Initializing Logistic Regression
2023-12-18 22:33:39,798:INFO:Total runtime is 0.0 minutes
2023-12-18 22:33:39,800:INFO:SubProcess create_model() called ==================================
2023-12-18 22:33:39,800:INFO:Initializing create_model()
2023-12-18 22:33:39,800:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:33:39,801:INFO:Checking exceptions
2023-12-18 22:33:39,801:INFO:Importing libraries
2023-12-18 22:33:39,801:INFO:Copying training dataset
2023-12-18 22:33:39,804:INFO:Defining folds
2023-12-18 22:33:39,805:INFO:Declaring metric variables
2023-12-18 22:33:39,807:INFO:Importing untrained model
2023-12-18 22:33:39,809:INFO:Logistic Regression Imported successfully
2023-12-18 22:33:39,813:INFO:Starting cross validation
2023-12-18 22:33:39,814:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:33:44,614:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:33:44,620:INFO:Calculating mean and std
2023-12-18 22:33:44,621:INFO:Creating metrics dataframe
2023-12-18 22:33:44,624:INFO:Uploading results into container
2023-12-18 22:33:44,624:INFO:Uploading model into container now
2023-12-18 22:33:44,625:INFO:_master_model_container: 1
2023-12-18 22:33:44,625:INFO:_display_container: 2
2023-12-18 22:33:44,625:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=6247, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-18 22:33:44,625:INFO:create_model() successfully completed......................................
2023-12-18 22:33:44,668:INFO:SubProcess create_model() end ==================================
2023-12-18 22:33:44,669:INFO:Creating metrics dataframe
2023-12-18 22:33:44,675:INFO:Initializing K Neighbors Classifier
2023-12-18 22:33:44,676:INFO:Total runtime is 0.08129173914591471 minutes
2023-12-18 22:33:44,678:INFO:SubProcess create_model() called ==================================
2023-12-18 22:33:44,679:INFO:Initializing create_model()
2023-12-18 22:33:44,679:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:33:44,679:INFO:Checking exceptions
2023-12-18 22:33:44,680:INFO:Importing libraries
2023-12-18 22:33:44,680:INFO:Copying training dataset
2023-12-18 22:33:44,683:INFO:Defining folds
2023-12-18 22:33:44,684:INFO:Declaring metric variables
2023-12-18 22:33:44,686:INFO:Importing untrained model
2023-12-18 22:33:44,688:INFO:K Neighbors Classifier Imported successfully
2023-12-18 22:33:44,692:INFO:Starting cross validation
2023-12-18 22:33:44,693:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:33:48,086:INFO:Calculating mean and std
2023-12-18 22:33:48,088:INFO:Creating metrics dataframe
2023-12-18 22:33:48,091:INFO:Uploading results into container
2023-12-18 22:33:48,092:INFO:Uploading model into container now
2023-12-18 22:33:48,093:INFO:_master_model_container: 2
2023-12-18 22:33:48,093:INFO:_display_container: 2
2023-12-18 22:33:48,094:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-18 22:33:48,094:INFO:create_model() successfully completed......................................
2023-12-18 22:33:48,128:INFO:SubProcess create_model() end ==================================
2023-12-18 22:33:48,128:INFO:Creating metrics dataframe
2023-12-18 22:33:48,133:INFO:Initializing Naive Bayes
2023-12-18 22:33:48,134:INFO:Total runtime is 0.1389274517695109 minutes
2023-12-18 22:33:48,136:INFO:SubProcess create_model() called ==================================
2023-12-18 22:33:48,137:INFO:Initializing create_model()
2023-12-18 22:33:48,137:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:33:48,138:INFO:Checking exceptions
2023-12-18 22:33:48,138:INFO:Importing libraries
2023-12-18 22:33:48,138:INFO:Copying training dataset
2023-12-18 22:33:48,141:INFO:Defining folds
2023-12-18 22:33:48,142:INFO:Declaring metric variables
2023-12-18 22:33:48,144:INFO:Importing untrained model
2023-12-18 22:33:48,146:INFO:Naive Bayes Imported successfully
2023-12-18 22:33:48,149:INFO:Starting cross validation
2023-12-18 22:33:48,149:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:33:51,259:INFO:Calculating mean and std
2023-12-18 22:33:51,260:INFO:Creating metrics dataframe
2023-12-18 22:33:51,263:INFO:Uploading results into container
2023-12-18 22:33:51,264:INFO:Uploading model into container now
2023-12-18 22:33:51,264:INFO:_master_model_container: 3
2023-12-18 22:33:51,265:INFO:_display_container: 2
2023-12-18 22:33:51,265:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-18 22:33:51,266:INFO:create_model() successfully completed......................................
2023-12-18 22:33:51,304:INFO:SubProcess create_model() end ==================================
2023-12-18 22:33:51,304:INFO:Creating metrics dataframe
2023-12-18 22:33:51,311:INFO:Initializing Decision Tree Classifier
2023-12-18 22:33:51,312:INFO:Total runtime is 0.19189621210098268 minutes
2023-12-18 22:33:51,315:INFO:SubProcess create_model() called ==================================
2023-12-18 22:33:51,315:INFO:Initializing create_model()
2023-12-18 22:33:51,315:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:33:51,316:INFO:Checking exceptions
2023-12-18 22:33:51,316:INFO:Importing libraries
2023-12-18 22:33:51,316:INFO:Copying training dataset
2023-12-18 22:33:51,320:INFO:Defining folds
2023-12-18 22:33:51,321:INFO:Declaring metric variables
2023-12-18 22:33:51,323:INFO:Importing untrained model
2023-12-18 22:33:51,326:INFO:Decision Tree Classifier Imported successfully
2023-12-18 22:33:51,330:INFO:Starting cross validation
2023-12-18 22:33:51,331:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:33:54,564:INFO:Calculating mean and std
2023-12-18 22:33:54,565:INFO:Creating metrics dataframe
2023-12-18 22:33:54,568:INFO:Uploading results into container
2023-12-18 22:33:54,568:INFO:Uploading model into container now
2023-12-18 22:33:54,569:INFO:_master_model_container: 4
2023-12-18 22:33:54,569:INFO:_display_container: 2
2023-12-18 22:33:54,569:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=6247, splitter='best')
2023-12-18 22:33:54,570:INFO:create_model() successfully completed......................................
2023-12-18 22:33:54,603:INFO:SubProcess create_model() end ==================================
2023-12-18 22:33:54,604:INFO:Creating metrics dataframe
2023-12-18 22:33:54,610:INFO:Initializing SVM - Linear Kernel
2023-12-18 22:33:54,611:INFO:Total runtime is 0.246884286403656 minutes
2023-12-18 22:33:54,613:INFO:SubProcess create_model() called ==================================
2023-12-18 22:33:54,613:INFO:Initializing create_model()
2023-12-18 22:33:54,614:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:33:54,614:INFO:Checking exceptions
2023-12-18 22:33:54,614:INFO:Importing libraries
2023-12-18 22:33:54,615:INFO:Copying training dataset
2023-12-18 22:33:54,618:INFO:Defining folds
2023-12-18 22:33:54,618:INFO:Declaring metric variables
2023-12-18 22:33:54,620:INFO:Importing untrained model
2023-12-18 22:33:54,622:INFO:SVM - Linear Kernel Imported successfully
2023-12-18 22:33:54,626:INFO:Starting cross validation
2023-12-18 22:33:54,627:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:33:57,735:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:33:57,735:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:33:57,738:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:33:57,740:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:33:57,832:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:33:57,833:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:33:57,836:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:33:57,836:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:33:57,837:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:33:57,846:INFO:Calculating mean and std
2023-12-18 22:33:57,847:INFO:Creating metrics dataframe
2023-12-18 22:33:57,850:INFO:Uploading results into container
2023-12-18 22:33:57,850:INFO:Uploading model into container now
2023-12-18 22:33:57,851:INFO:_master_model_container: 5
2023-12-18 22:33:57,851:INFO:_display_container: 2
2023-12-18 22:33:57,852:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=6247, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-18 22:33:57,852:INFO:create_model() successfully completed......................................
2023-12-18 22:33:57,887:INFO:SubProcess create_model() end ==================================
2023-12-18 22:33:57,888:INFO:Creating metrics dataframe
2023-12-18 22:33:57,894:INFO:Initializing Ridge Classifier
2023-12-18 22:33:57,894:INFO:Total runtime is 0.30159841378529867 minutes
2023-12-18 22:33:57,896:INFO:SubProcess create_model() called ==================================
2023-12-18 22:33:57,896:INFO:Initializing create_model()
2023-12-18 22:33:57,897:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:33:57,897:INFO:Checking exceptions
2023-12-18 22:33:57,897:INFO:Importing libraries
2023-12-18 22:33:57,898:INFO:Copying training dataset
2023-12-18 22:33:57,901:INFO:Defining folds
2023-12-18 22:33:57,901:INFO:Declaring metric variables
2023-12-18 22:33:57,903:INFO:Importing untrained model
2023-12-18 22:33:57,905:INFO:Ridge Classifier Imported successfully
2023-12-18 22:33:57,909:INFO:Starting cross validation
2023-12-18 22:33:57,910:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:01,107:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:34:01,109:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:01,217:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:34:01,217:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:34:01,220:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:01,220:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:01,221:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:01,234:INFO:Calculating mean and std
2023-12-18 22:34:01,235:INFO:Creating metrics dataframe
2023-12-18 22:34:01,237:INFO:Uploading results into container
2023-12-18 22:34:01,238:INFO:Uploading model into container now
2023-12-18 22:34:01,238:INFO:_master_model_container: 6
2023-12-18 22:34:01,239:INFO:_display_container: 2
2023-12-18 22:34:01,239:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=6247, solver='auto',
                tol=0.0001)
2023-12-18 22:34:01,240:INFO:create_model() successfully completed......................................
2023-12-18 22:34:01,273:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:01,274:INFO:Creating metrics dataframe
2023-12-18 22:34:01,280:INFO:Initializing Random Forest Classifier
2023-12-18 22:34:01,280:INFO:Total runtime is 0.3580375115076701 minutes
2023-12-18 22:34:01,282:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:01,283:INFO:Initializing create_model()
2023-12-18 22:34:01,283:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:01,283:INFO:Checking exceptions
2023-12-18 22:34:01,284:INFO:Importing libraries
2023-12-18 22:34:01,284:INFO:Copying training dataset
2023-12-18 22:34:01,287:INFO:Defining folds
2023-12-18 22:34:01,288:INFO:Declaring metric variables
2023-12-18 22:34:01,290:INFO:Importing untrained model
2023-12-18 22:34:01,292:INFO:Random Forest Classifier Imported successfully
2023-12-18 22:34:01,295:INFO:Starting cross validation
2023-12-18 22:34:01,297:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:04,194:INFO:Calculating mean and std
2023-12-18 22:34:04,195:INFO:Creating metrics dataframe
2023-12-18 22:34:04,198:INFO:Uploading results into container
2023-12-18 22:34:04,198:INFO:Uploading model into container now
2023-12-18 22:34:04,199:INFO:_master_model_container: 7
2023-12-18 22:34:04,199:INFO:_display_container: 2
2023-12-18 22:34:04,200:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)
2023-12-18 22:34:04,200:INFO:create_model() successfully completed......................................
2023-12-18 22:34:04,233:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:04,234:INFO:Creating metrics dataframe
2023-12-18 22:34:04,241:INFO:Initializing Quadratic Discriminant Analysis
2023-12-18 22:34:04,241:INFO:Total runtime is 0.40738643407821656 minutes
2023-12-18 22:34:04,244:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:04,244:INFO:Initializing create_model()
2023-12-18 22:34:04,244:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:04,245:INFO:Checking exceptions
2023-12-18 22:34:04,245:INFO:Importing libraries
2023-12-18 22:34:04,245:INFO:Copying training dataset
2023-12-18 22:34:04,248:INFO:Defining folds
2023-12-18 22:34:04,249:INFO:Declaring metric variables
2023-12-18 22:34:04,251:INFO:Importing untrained model
2023-12-18 22:34:04,253:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-18 22:34:04,256:INFO:Starting cross validation
2023-12-18 22:34:04,257:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:04,291:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:34:04,291:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:34:04,298:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-12-18 22:34:04,299:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-12-18 22:34:04,299:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-12-18 22:34:04,300:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-12-18 22:34:04,320:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-12-18 22:34:04,320:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-12-18 22:34:04,320:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-12-18 22:34:04,321:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-12-18 22:34:04,322:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-12-18 22:34:04,322:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: divide by zero encountered in power
  X2 = np.dot(Xm, R * (S ** (-0.5)))


2023-12-18 22:34:04,322:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))

2023-12-18 22:34:04,322:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))
2023-12-18 22:34:04,3232023-12-18 22:34:04,323:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:951: RuntimeWarning: invalid value encountered in multiply
  X2 = np.dot(Xm, R * (S ** (-0.5)))
2023-12-18 22:34:04,323:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-12-18 22:34:04,324:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:954: RuntimeWarning: divide by zero encountered in log
  u = np.asarray([np.sum(np.log(s)) for s in self.scalings_])

2023-12-18 22:34:04,326:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 87, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-12-18 22:34:04,326:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 87, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-12-18 22:34:04,326:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 327, in _score
    return self._sign * self._score_func(y, y_pred, **self._kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 87, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_ranking.py", line 551, in roc_auc_score
    y_score = check_array(y_score, ensure_2d=False)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 921, in check_array
    _assert_all_finite(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\validation.py", line 161, in _assert_all_finite
    raise ValueError(msg_err)
ValueError: Input contains NaN.

  warnings.warn(

2023-12-18 22:34:04,328:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:04,328:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:04,328:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:04,329:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:04,335:INFO:Calculating mean and std
2023-12-18 22:34:04,336:INFO:Creating metrics dataframe
2023-12-18 22:34:04,338:INFO:Uploading results into container
2023-12-18 22:34:04,339:INFO:Uploading model into container now
2023-12-18 22:34:04,340:INFO:_master_model_container: 8
2023-12-18 22:34:04,340:INFO:_display_container: 2
2023-12-18 22:34:04,340:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-18 22:34:04,341:INFO:create_model() successfully completed......................................
2023-12-18 22:34:04,373:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:04,374:INFO:Creating metrics dataframe
2023-12-18 22:34:04,380:INFO:Initializing Ada Boost Classifier
2023-12-18 22:34:04,381:INFO:Total runtime is 0.40971161127090455 minutes
2023-12-18 22:34:04,382:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:04,383:INFO:Initializing create_model()
2023-12-18 22:34:04,383:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:04,384:INFO:Checking exceptions
2023-12-18 22:34:04,384:INFO:Importing libraries
2023-12-18 22:34:04,384:INFO:Copying training dataset
2023-12-18 22:34:04,387:INFO:Defining folds
2023-12-18 22:34:04,388:INFO:Declaring metric variables
2023-12-18 22:34:04,389:INFO:Importing untrained model
2023-12-18 22:34:04,392:INFO:Ada Boost Classifier Imported successfully
2023-12-18 22:34:04,395:INFO:Starting cross validation
2023-12-18 22:34:04,396:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:04,551:INFO:Calculating mean and std
2023-12-18 22:34:04,552:INFO:Creating metrics dataframe
2023-12-18 22:34:04,553:INFO:Uploading results into container
2023-12-18 22:34:04,554:INFO:Uploading model into container now
2023-12-18 22:34:04,555:INFO:_master_model_container: 9
2023-12-18 22:34:04,555:INFO:_display_container: 2
2023-12-18 22:34:04,555:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247)
2023-12-18 22:34:04,556:INFO:create_model() successfully completed......................................
2023-12-18 22:34:04,593:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:04,594:INFO:Creating metrics dataframe
2023-12-18 22:34:04,601:INFO:Initializing Gradient Boosting Classifier
2023-12-18 22:34:04,602:INFO:Total runtime is 0.4133906364440918 minutes
2023-12-18 22:34:04,604:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:04,604:INFO:Initializing create_model()
2023-12-18 22:34:04,605:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:04,605:INFO:Checking exceptions
2023-12-18 22:34:04,605:INFO:Importing libraries
2023-12-18 22:34:04,605:INFO:Copying training dataset
2023-12-18 22:34:04,609:INFO:Defining folds
2023-12-18 22:34:04,609:INFO:Declaring metric variables
2023-12-18 22:34:04,611:INFO:Importing untrained model
2023-12-18 22:34:04,613:INFO:Gradient Boosting Classifier Imported successfully
2023-12-18 22:34:04,617:INFO:Starting cross validation
2023-12-18 22:34:04,618:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:05,617:INFO:Calculating mean and std
2023-12-18 22:34:05,618:INFO:Creating metrics dataframe
2023-12-18 22:34:05,621:INFO:Uploading results into container
2023-12-18 22:34:05,622:INFO:Uploading model into container now
2023-12-18 22:34:05,622:INFO:_master_model_container: 10
2023-12-18 22:34:05,622:INFO:_display_container: 2
2023-12-18 22:34:05,623:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-18 22:34:05,623:INFO:create_model() successfully completed......................................
2023-12-18 22:34:05,657:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:05,657:INFO:Creating metrics dataframe
2023-12-18 22:34:05,663:INFO:Initializing Linear Discriminant Analysis
2023-12-18 22:34:05,664:INFO:Total runtime is 0.431091837088267 minutes
2023-12-18 22:34:05,666:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:05,667:INFO:Initializing create_model()
2023-12-18 22:34:05,667:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:05,667:INFO:Checking exceptions
2023-12-18 22:34:05,667:INFO:Importing libraries
2023-12-18 22:34:05,668:INFO:Copying training dataset
2023-12-18 22:34:05,671:INFO:Defining folds
2023-12-18 22:34:05,671:INFO:Declaring metric variables
2023-12-18 22:34:05,673:INFO:Importing untrained model
2023-12-18 22:34:05,675:INFO:Linear Discriminant Analysis Imported successfully
2023-12-18 22:34:05,678:INFO:Starting cross validation
2023-12-18 22:34:05,679:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:05,712:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:05,725:INFO:Calculating mean and std
2023-12-18 22:34:05,726:INFO:Creating metrics dataframe
2023-12-18 22:34:05,728:INFO:Uploading results into container
2023-12-18 22:34:05,729:INFO:Uploading model into container now
2023-12-18 22:34:05,730:INFO:_master_model_container: 11
2023-12-18 22:34:05,730:INFO:_display_container: 2
2023-12-18 22:34:05,730:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-18 22:34:05,730:INFO:create_model() successfully completed......................................
2023-12-18 22:34:05,762:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:05,763:INFO:Creating metrics dataframe
2023-12-18 22:34:05,769:INFO:Initializing Extra Trees Classifier
2023-12-18 22:34:05,770:INFO:Total runtime is 0.43286130825678504 minutes
2023-12-18 22:34:05,772:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:05,773:INFO:Initializing create_model()
2023-12-18 22:34:05,773:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:05,773:INFO:Checking exceptions
2023-12-18 22:34:05,774:INFO:Importing libraries
2023-12-18 22:34:05,774:INFO:Copying training dataset
2023-12-18 22:34:05,777:INFO:Defining folds
2023-12-18 22:34:05,778:INFO:Declaring metric variables
2023-12-18 22:34:05,780:INFO:Importing untrained model
2023-12-18 22:34:05,782:INFO:Extra Trees Classifier Imported successfully
2023-12-18 22:34:05,784:INFO:Starting cross validation
2023-12-18 22:34:05,785:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:06,098:INFO:Calculating mean and std
2023-12-18 22:34:06,099:INFO:Creating metrics dataframe
2023-12-18 22:34:06,102:INFO:Uploading results into container
2023-12-18 22:34:06,102:INFO:Uploading model into container now
2023-12-18 22:34:06,103:INFO:_master_model_container: 12
2023-12-18 22:34:06,103:INFO:_display_container: 2
2023-12-18 22:34:06,104:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=6247, verbose=0, warm_start=False)
2023-12-18 22:34:06,104:INFO:create_model() successfully completed......................................
2023-12-18 22:34:06,138:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:06,139:INFO:Creating metrics dataframe
2023-12-18 22:34:06,146:INFO:Initializing Extreme Gradient Boosting
2023-12-18 22:34:06,147:INFO:Total runtime is 0.4391441623369852 minutes
2023-12-18 22:34:06,149:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:06,150:INFO:Initializing create_model()
2023-12-18 22:34:06,150:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:06,151:INFO:Checking exceptions
2023-12-18 22:34:06,151:INFO:Importing libraries
2023-12-18 22:34:06,151:INFO:Copying training dataset
2023-12-18 22:34:06,154:INFO:Defining folds
2023-12-18 22:34:06,154:INFO:Declaring metric variables
2023-12-18 22:34:06,156:INFO:Importing untrained model
2023-12-18 22:34:06,158:INFO:Extreme Gradient Boosting Imported successfully
2023-12-18 22:34:06,162:INFO:Starting cross validation
2023-12-18 22:34:06,163:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:06,485:INFO:Calculating mean and std
2023-12-18 22:34:06,487:INFO:Creating metrics dataframe
2023-12-18 22:34:06,489:INFO:Uploading results into container
2023-12-18 22:34:06,490:INFO:Uploading model into container now
2023-12-18 22:34:06,491:INFO:_master_model_container: 13
2023-12-18 22:34:06,491:INFO:_display_container: 2
2023-12-18 22:34:06,492:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-18 22:34:06,492:INFO:create_model() successfully completed......................................
2023-12-18 22:34:06,524:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:06,525:INFO:Creating metrics dataframe
2023-12-18 22:34:06,532:INFO:Initializing Light Gradient Boosting Machine
2023-12-18 22:34:06,532:INFO:Total runtime is 0.4455575346946716 minutes
2023-12-18 22:34:06,534:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:06,535:INFO:Initializing create_model()
2023-12-18 22:34:06,535:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:06,535:INFO:Checking exceptions
2023-12-18 22:34:06,536:INFO:Importing libraries
2023-12-18 22:34:06,536:INFO:Copying training dataset
2023-12-18 22:34:06,539:INFO:Defining folds
2023-12-18 22:34:06,539:INFO:Declaring metric variables
2023-12-18 22:34:06,541:INFO:Importing untrained model
2023-12-18 22:34:06,543:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:34:06,547:INFO:Starting cross validation
2023-12-18 22:34:06,548:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:09,001:INFO:Calculating mean and std
2023-12-18 22:34:09,002:INFO:Creating metrics dataframe
2023-12-18 22:34:09,006:INFO:Uploading results into container
2023-12-18 22:34:09,006:INFO:Uploading model into container now
2023-12-18 22:34:09,007:INFO:_master_model_container: 14
2023-12-18 22:34:09,007:INFO:_display_container: 2
2023-12-18 22:34:09,008:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:34:09,008:INFO:create_model() successfully completed......................................
2023-12-18 22:34:09,046:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:09,047:INFO:Creating metrics dataframe
2023-12-18 22:34:09,055:INFO:Initializing Dummy Classifier
2023-12-18 22:34:09,056:INFO:Total runtime is 0.48762783209482824 minutes
2023-12-18 22:34:09,058:INFO:SubProcess create_model() called ==================================
2023-12-18 22:34:09,059:INFO:Initializing create_model()
2023-12-18 22:34:09,059:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F753793D00>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:09,059:INFO:Checking exceptions
2023-12-18 22:34:09,060:INFO:Importing libraries
2023-12-18 22:34:09,060:INFO:Copying training dataset
2023-12-18 22:34:09,064:INFO:Defining folds
2023-12-18 22:34:09,065:INFO:Declaring metric variables
2023-12-18 22:34:09,067:INFO:Importing untrained model
2023-12-18 22:34:09,069:INFO:Dummy Classifier Imported successfully
2023-12-18 22:34:09,074:INFO:Starting cross validation
2023-12-18 22:34:09,075:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:34:09,099:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:09,100:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:09,103:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:09,103:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:09,105:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:34:09,109:INFO:Calculating mean and std
2023-12-18 22:34:09,110:INFO:Creating metrics dataframe
2023-12-18 22:34:09,111:INFO:Uploading results into container
2023-12-18 22:34:09,112:INFO:Uploading model into container now
2023-12-18 22:34:09,113:INFO:_master_model_container: 15
2023-12-18 22:34:09,113:INFO:_display_container: 2
2023-12-18 22:34:09,113:INFO:DummyClassifier(constant=None, random_state=6247, strategy='prior')
2023-12-18 22:34:09,114:INFO:create_model() successfully completed......................................
2023-12-18 22:34:09,148:INFO:SubProcess create_model() end ==================================
2023-12-18 22:34:09,149:INFO:Creating metrics dataframe
2023-12-18 22:34:09,160:INFO:Initializing create_model()
2023-12-18 22:34:09,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:09,161:INFO:Checking exceptions
2023-12-18 22:34:09,162:INFO:Importing libraries
2023-12-18 22:34:09,163:INFO:Copying training dataset
2023-12-18 22:34:09,166:INFO:Defining folds
2023-12-18 22:34:09,167:INFO:Declaring metric variables
2023-12-18 22:34:09,167:INFO:Importing untrained model
2023-12-18 22:34:09,167:INFO:Declaring custom model
2023-12-18 22:34:09,168:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:34:09,168:INFO:Cross validation set to False
2023-12-18 22:34:09,169:INFO:Fitting Model
2023-12-18 22:34:09,184:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000565 seconds.
2023-12-18 22:34:09,185:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-18 22:34:09,186:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-18 22:34:09,186:INFO:[LightGBM] [Info] Total Bins 345
2023-12-18 22:34:09,187:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 24
2023-12-18 22:34:09,187:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-18 22:34:09,188:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-18 22:34:09,188:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-18 22:34:09,456:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:34:09,458:INFO:create_model() successfully completed......................................
2023-12-18 22:34:09,498:INFO:Initializing create_model()
2023-12-18 22:34:09,500:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:09,500:INFO:Checking exceptions
2023-12-18 22:34:09,502:INFO:Importing libraries
2023-12-18 22:34:09,502:INFO:Copying training dataset
2023-12-18 22:34:09,506:INFO:Defining folds
2023-12-18 22:34:09,506:INFO:Declaring metric variables
2023-12-18 22:34:09,507:INFO:Importing untrained model
2023-12-18 22:34:09,507:INFO:Declaring custom model
2023-12-18 22:34:09,508:INFO:Gradient Boosting Classifier Imported successfully
2023-12-18 22:34:09,509:INFO:Cross validation set to False
2023-12-18 22:34:09,509:INFO:Fitting Model
2023-12-18 22:34:10,689:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-18 22:34:10,690:INFO:create_model() successfully completed......................................
2023-12-18 22:34:10,725:INFO:Initializing create_model()
2023-12-18 22:34:10,726:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:10,726:INFO:Checking exceptions
2023-12-18 22:34:10,727:INFO:Importing libraries
2023-12-18 22:34:10,728:INFO:Copying training dataset
2023-12-18 22:34:10,731:INFO:Defining folds
2023-12-18 22:34:10,732:INFO:Declaring metric variables
2023-12-18 22:34:10,732:INFO:Importing untrained model
2023-12-18 22:34:10,732:INFO:Declaring custom model
2023-12-18 22:34:10,733:INFO:Ada Boost Classifier Imported successfully
2023-12-18 22:34:10,733:INFO:Cross validation set to False
2023-12-18 22:34:10,734:INFO:Fitting Model
2023-12-18 22:34:10,870:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247)
2023-12-18 22:34:10,871:INFO:create_model() successfully completed......................................
2023-12-18 22:34:10,908:INFO:Initializing create_model()
2023-12-18 22:34:10,909:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:10,909:INFO:Checking exceptions
2023-12-18 22:34:10,911:INFO:Importing libraries
2023-12-18 22:34:10,911:INFO:Copying training dataset
2023-12-18 22:34:10,914:INFO:Defining folds
2023-12-18 22:34:10,915:INFO:Declaring metric variables
2023-12-18 22:34:10,915:INFO:Importing untrained model
2023-12-18 22:34:10,916:INFO:Declaring custom model
2023-12-18 22:34:10,917:INFO:Extreme Gradient Boosting Imported successfully
2023-12-18 22:34:10,917:INFO:Cross validation set to False
2023-12-18 22:34:10,918:INFO:Fitting Model
2023-12-18 22:34:11,095:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-18 22:34:11,096:INFO:create_model() successfully completed......................................
2023-12-18 22:34:11,137:INFO:Initializing create_model()
2023-12-18 22:34:11,138:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:34:11,138:INFO:Checking exceptions
2023-12-18 22:34:11,139:INFO:Importing libraries
2023-12-18 22:34:11,140:INFO:Copying training dataset
2023-12-18 22:34:11,143:INFO:Defining folds
2023-12-18 22:34:11,145:INFO:Declaring metric variables
2023-12-18 22:34:11,145:INFO:Importing untrained model
2023-12-18 22:34:11,145:INFO:Declaring custom model
2023-12-18 22:34:11,146:INFO:Random Forest Classifier Imported successfully
2023-12-18 22:34:11,147:INFO:Cross validation set to False
2023-12-18 22:34:11,147:INFO:Fitting Model
2023-12-18 22:34:11,307:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)
2023-12-18 22:34:11,308:INFO:create_model() successfully completed......................................
2023-12-18 22:34:11,358:INFO:_master_model_container: 15
2023-12-18 22:34:11,359:INFO:_display_container: 2
2023-12-18 22:34:11,360:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)]
2023-12-18 22:34:11,361:INFO:compare_models() successfully completed......................................
2023-12-18 22:36:30,519:INFO:Initializing tune_model()
2023-12-18 22:36:30,520:INFO:tune_model(estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>)
2023-12-18 22:36:30,521:INFO:Checking exceptions
2023-12-18 22:37:17,504:INFO:Initializing evaluate_model()
2023-12-18 22:37:17,506:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-18 22:37:17,517:INFO:Initializing plot_model()
2023-12-18 22:37:17,518:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:17,518:INFO:Checking exceptions
2023-12-18 22:37:20,866:INFO:Initializing plot_model()
2023-12-18 22:37:20,867:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:20,867:INFO:Checking exceptions
2023-12-18 22:37:21,456:INFO:Initializing plot_model()
2023-12-18 22:37:21,457:INFO:plot_model(plot=auc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:21,457:INFO:Checking exceptions
2023-12-18 22:37:21,992:INFO:Initializing plot_model()
2023-12-18 22:37:21,993:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:21,993:INFO:Checking exceptions
2023-12-18 22:37:22,507:INFO:Initializing plot_model()
2023-12-18 22:37:22,507:INFO:plot_model(plot=feature, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:22,508:INFO:Checking exceptions
2023-12-18 22:37:22,867:INFO:Initializing plot_model()
2023-12-18 22:37:22,868:INFO:plot_model(plot=feature_all, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:22,868:INFO:Checking exceptions
2023-12-18 22:37:23,164:INFO:Initializing plot_model()
2023-12-18 22:37:23,165:INFO:plot_model(plot=boundary, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:23,165:INFO:Checking exceptions
2023-12-18 22:37:25,137:INFO:Initializing plot_model()
2023-12-18 22:37:25,138:INFO:plot_model(plot=lift, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:25,138:INFO:Checking exceptions
2023-12-18 22:37:25,451:INFO:Initializing plot_model()
2023-12-18 22:37:25,452:INFO:plot_model(plot=gain, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:25,453:INFO:Checking exceptions
2023-12-18 22:37:25,997:INFO:Initializing plot_model()
2023-12-18 22:37:25,998:INFO:plot_model(plot=tree, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:25,998:INFO:Checking exceptions
2023-12-18 22:37:26,316:INFO:Initializing plot_model()
2023-12-18 22:37:26,317:INFO:plot_model(plot=ks, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:37:26,317:INFO:Checking exceptions
2023-12-18 22:37:49,611:INFO:Initializing predict_model()
2023-12-18 22:37:49,612:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001F761426C20>)
2023-12-18 22:37:49,612:INFO:Checking exceptions
2023-12-18 22:37:49,613:INFO:Preloading libraries
2023-12-18 22:38:07,774:INFO:Initializing tune_model()
2023-12-18 22:38:07,775:INFO:tune_model(estimator=[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=6247, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=6247), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=6247, verbose=0, warm_start=False)], fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>)
2023-12-18 22:38:07,775:INFO:Checking exceptions
2023-12-18 22:38:20,267:INFO:Initializing tune_model()
2023-12-18 22:38:20,268:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>)
2023-12-18 22:38:20,268:INFO:Checking exceptions
2023-12-18 22:38:20,281:INFO:Copying training dataset
2023-12-18 22:38:20,283:INFO:Checking base model
2023-12-18 22:38:20,283:INFO:Base model : Light Gradient Boosting Machine
2023-12-18 22:38:20,286:INFO:Declaring metric variables
2023-12-18 22:38:20,288:INFO:Defining Hyperparameters
2023-12-18 22:38:20,346:INFO:Tuning with n_jobs=-1
2023-12-18 22:38:20,346:INFO:Initializing RandomizedSearchCV
2023-12-18 22:38:47,691:INFO:Initializing tune_model()
2023-12-18 22:38:47,691:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>)
2023-12-18 22:38:47,692:INFO:Checking exceptions
2023-12-18 22:38:47,704:INFO:Copying training dataset
2023-12-18 22:38:47,707:INFO:Checking base model
2023-12-18 22:38:47,707:INFO:Base model : Light Gradient Boosting Machine
2023-12-18 22:38:47,709:INFO:Declaring metric variables
2023-12-18 22:38:47,711:INFO:Defining Hyperparameters
2023-12-18 22:38:47,770:INFO:Tuning with n_jobs=-1
2023-12-18 22:38:47,772:INFO:Initializing RandomizedSearchCV
2023-12-18 22:39:19,726:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.1, 'actual_estimator__num_leaves': 60, 'actual_estimator__n_estimators': 240, 'actual_estimator__min_split_gain': 0.7, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.3, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 0, 'actual_estimator__bagging_fraction': 0.7}
2023-12-18 22:39:19,728:INFO:Hyperparameter search completed
2023-12-18 22:39:19,729:INFO:SubProcess create_model() called ==================================
2023-12-18 22:39:19,730:INFO:Initializing create_model()
2023-12-18 22:39:19,730:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F760ADC7F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.1, 'num_leaves': 60, 'n_estimators': 240, 'min_split_gain': 0.7, 'min_child_samples': 11, 'learning_rate': 0.3, 'feature_fraction': 0.4, 'bagging_freq': 0, 'bagging_fraction': 0.7})
2023-12-18 22:39:19,731:INFO:Checking exceptions
2023-12-18 22:39:19,731:INFO:Importing libraries
2023-12-18 22:39:19,731:INFO:Copying training dataset
2023-12-18 22:39:19,735:INFO:Defining folds
2023-12-18 22:39:19,736:INFO:Declaring metric variables
2023-12-18 22:39:19,739:INFO:Importing untrained model
2023-12-18 22:39:19,739:INFO:Declaring custom model
2023-12-18 22:39:19,742:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:39:19,746:INFO:Starting cross validation
2023-12-18 22:39:19,747:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:39:20,598:INFO:Calculating mean and std
2023-12-18 22:39:20,599:INFO:Creating metrics dataframe
2023-12-18 22:39:20,603:INFO:Finalizing model
2023-12-18 22:39:20,613:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-12-18 22:39:20,615:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-18 22:39:20,615:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2023-12-18 22:39:20,620:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-12-18 22:39:20,620:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-18 22:39:20,621:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2023-12-18 22:39:20,625:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001953 seconds.
2023-12-18 22:39:20,626:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-18 22:39:20,626:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-18 22:39:20,627:INFO:[LightGBM] [Info] Total Bins 347
2023-12-18 22:39:20,628:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 25
2023-12-18 22:39:20,628:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-18 22:39:20,629:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-18 22:39:20,629:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-18 22:39:20,655:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,793:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,798:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,800:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,804:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,805:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,807:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,808:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,809:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,811:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,812:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,813:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,814:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,816:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,817:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,818:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,820:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,821:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,822:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,824:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,825:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,826:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,828:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,829:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,831:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,832:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,834:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,835:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,836:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,838:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,839:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,841:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,842:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,843:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,845:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,846:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,847:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,849:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,850:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,851:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,853:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,854:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,856:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,857:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,858:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,859:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,861:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,862:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,863:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,865:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,866:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,867:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,869:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,870:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,872:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,873:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,874:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,875:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,877:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,878:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,880:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,881:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,882:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,884:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,885:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,887:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,888:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,890:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,891:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,893:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,895:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,896:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,898:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,900:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,901:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,904:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,906:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,908:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,910:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,913:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,914:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,917:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,919:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,920:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,921:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,923:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,926:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,927:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,930:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,933:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,934:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,936:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,936:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,938:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,941:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,943:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,944:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,947:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,948:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,950:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,953:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,954:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,956:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,958:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,960:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,961:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,963:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,966:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,967:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,969:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,976:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,981:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,984:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,986:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,989:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,991:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,993:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,997:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:20,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:20,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,002:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,004:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,005:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,006:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,008:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,009:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,010:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,012:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,016:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,018:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,019:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,021:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,022:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,023:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,025:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,026:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,029:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,031:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,032:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,034:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,035:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,037:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,038:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,041:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,042:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,044:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,045:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,047:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,048:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,050:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,051:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,053:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,054:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,055:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,058:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,060:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,061:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,062:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,064:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,064:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,065:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,065:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,066:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,066:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,067:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,068:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,069:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,069:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,070:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,072:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,073:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,075:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,076:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,078:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,079:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,081:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,083:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,084:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,086:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,089:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,090:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,092:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,093:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,094:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,095:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,096:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,098:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,098:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,099:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,100:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,100:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,102:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,102:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,103:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,105:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:39:21,106:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:39:21,115:INFO:Uploading results into container
2023-12-18 22:39:21,117:INFO:Uploading model into container now
2023-12-18 22:39:21,117:INFO:_master_model_container: 16
2023-12-18 22:39:21,118:INFO:_display_container: 3
2023-12-18 22:39:21,118:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:39:21,119:INFO:create_model() successfully completed......................................
2023-12-18 22:39:21,179:INFO:SubProcess create_model() end ==================================
2023-12-18 22:39:21,180:INFO:choose_better activated
2023-12-18 22:39:21,183:INFO:SubProcess create_model() called ==================================
2023-12-18 22:39:21,184:INFO:Initializing create_model()
2023-12-18 22:39:21,184:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:39:21,184:INFO:Checking exceptions
2023-12-18 22:39:21,186:INFO:Importing libraries
2023-12-18 22:39:21,186:INFO:Copying training dataset
2023-12-18 22:39:21,190:INFO:Defining folds
2023-12-18 22:39:21,191:INFO:Declaring metric variables
2023-12-18 22:39:21,192:INFO:Importing untrained model
2023-12-18 22:39:21,192:INFO:Declaring custom model
2023-12-18 22:39:21,193:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:39:21,193:INFO:Starting cross validation
2023-12-18 22:39:21,194:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:39:23,881:INFO:Calculating mean and std
2023-12-18 22:39:23,882:INFO:Creating metrics dataframe
2023-12-18 22:39:23,883:INFO:Finalizing model
2023-12-18 22:39:23,898:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.
2023-12-18 22:39:23,899:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-18 22:39:23,900:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-18 22:39:23,900:INFO:[LightGBM] [Info] Total Bins 345
2023-12-18 22:39:23,901:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 24
2023-12-18 22:39:23,901:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-18 22:39:23,902:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-18 22:39:23,902:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-18 22:39:24,189:INFO:Uploading results into container
2023-12-18 22:39:24,191:INFO:Uploading model into container now
2023-12-18 22:39:24,192:INFO:_master_model_container: 17
2023-12-18 22:39:24,192:INFO:_display_container: 4
2023-12-18 22:39:24,193:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:39:24,193:INFO:create_model() successfully completed......................................
2023-12-18 22:39:24,253:INFO:SubProcess create_model() end ==================================
2023-12-18 22:39:24,253:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=6247, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8037
2023-12-18 22:39:24,254:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8085
2023-12-18 22:39:24,255:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-18 22:39:24,255:INFO:choose_better completed
2023-12-18 22:39:24,261:INFO:_master_model_container: 17
2023-12-18 22:39:24,261:INFO:_display_container: 3
2023-12-18 22:39:24,262:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:39:24,262:INFO:tune_model() successfully completed......................................
2023-12-18 22:39:53,161:INFO:Initializing evaluate_model()
2023-12-18 22:39:53,162:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-18 22:39:53,170:INFO:Initializing plot_model()
2023-12-18 22:39:53,170:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:39:53,171:INFO:Checking exceptions
2023-12-18 22:39:53,172:INFO:Preloading libraries
2023-12-18 22:39:53,177:INFO:Copying training dataset
2023-12-18 22:39:53,177:INFO:Plot type: pipeline
2023-12-18 22:39:53,300:INFO:Visual Rendered Successfully
2023-12-18 22:39:53,356:INFO:plot_model() successfully completed......................................
2023-12-18 22:39:55,137:INFO:Initializing plot_model()
2023-12-18 22:39:55,138:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:39:55,138:INFO:Checking exceptions
2023-12-18 22:39:55,141:INFO:Preloading libraries
2023-12-18 22:39:55,145:INFO:Copying training dataset
2023-12-18 22:39:55,146:INFO:Plot type: parameter
2023-12-18 22:39:55,149:INFO:Visual Rendered Successfully
2023-12-18 22:39:55,245:INFO:plot_model() successfully completed......................................
2023-12-18 22:39:56,684:INFO:Initializing plot_model()
2023-12-18 22:39:56,685:INFO:plot_model(plot=auc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:39:56,685:INFO:Checking exceptions
2023-12-18 22:39:56,687:INFO:Preloading libraries
2023-12-18 22:39:56,691:INFO:Copying training dataset
2023-12-18 22:39:56,692:INFO:Plot type: auc
2023-12-18 22:39:56,765:INFO:Fitting Model
2023-12-18 22:39:56,766:INFO:Scoring test/hold-out set
2023-12-18 22:39:56,769:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-12-18 22:39:56,769:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-18 22:39:56,769:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2023-12-18 22:39:56,771:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-12-18 22:39:56,772:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-18 22:39:56,772:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2023-12-18 22:39:56,907:INFO:Visual Rendered Successfully
2023-12-18 22:39:56,961:INFO:plot_model() successfully completed......................................
2023-12-18 22:39:57,003:INFO:Initializing plot_model()
2023-12-18 22:39:57,004:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:39:57,004:INFO:Checking exceptions
2023-12-18 22:39:57,006:INFO:Preloading libraries
2023-12-18 22:39:57,010:INFO:Copying training dataset
2023-12-18 22:39:57,011:INFO:Plot type: parameter
2023-12-18 22:39:57,014:INFO:Visual Rendered Successfully
2023-12-18 22:39:57,117:INFO:plot_model() successfully completed......................................
2023-12-18 22:39:58,202:INFO:Initializing plot_model()
2023-12-18 22:39:58,203:INFO:plot_model(plot=auc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, system=True)
2023-12-18 22:39:58,203:INFO:Checking exceptions
2023-12-18 22:39:58,205:INFO:Preloading libraries
2023-12-18 22:39:58,209:INFO:Copying training dataset
2023-12-18 22:39:58,210:INFO:Plot type: auc
2023-12-18 22:39:58,290:INFO:Fitting Model
2023-12-18 22:39:58,291:INFO:Scoring test/hold-out set
2023-12-18 22:39:58,292:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-12-18 22:39:58,293:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-18 22:39:58,293:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2023-12-18 22:39:58,295:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-12-18 22:39:58,296:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-18 22:39:58,296:INFO:[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0
2023-12-18 22:39:58,429:INFO:Visual Rendered Successfully
2023-12-18 22:39:58,486:INFO:plot_model() successfully completed......................................
2023-12-18 22:40:12,679:INFO:Initializing predict_model()
2023-12-18 22:40:12,680:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001F719C9BEB0>)
2023-12-18 22:40:12,681:INFO:Checking exceptions
2023-12-18 22:40:12,682:INFO:Preloading libraries
2023-12-18 22:40:37,739:INFO:Initializing predict_model()
2023-12-18 22:40:37,740:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F753793550>, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=0, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=240, n_jobs=-1, num_leaves=60, objective=None,
               random_state=6247, reg_alpha=0.1, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001F761AFFF40>)
2023-12-18 22:40:37,741:INFO:Checking exceptions
2023-12-18 22:40:37,742:INFO:Preloading libraries
2023-12-18 22:40:37,744:INFO:Set up data.
2023-12-18 22:40:37,747:INFO:Set up index.
2023-12-18 22:44:56,869:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:44:56,870:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:44:56,870:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:44:56,871:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-18 22:45:11,092:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_36848\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-18 22:45:11,096:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_36848\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-18 22:45:11,143:INFO:PyCaret ClassificationExperiment
2023-12-18 22:45:11,143:INFO:Logging name: clf-default-name
2023-12-18 22:45:11,144:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-18 22:45:11,144:INFO:version 3.2.0
2023-12-18 22:45:11,144:INFO:Initializing setup()
2023-12-18 22:45:11,144:INFO:self.USI: 1289
2023-12-18 22:45:11,145:INFO:self._variable_keys: {'is_multiclass', 'fix_imbalance', '_available_plots', 'gpu_n_jobs_param', 'y_test', '_ml_usecase', 'exp_name_log', 'log_plots_param', 'html_param', 'USI', 'exp_id', 'fold_shuffle_param', 'n_jobs_param', 'X', 'gpu_param', 'memory', 'fold_groups_param', 'X_test', 'logging_param', 'y', 'X_train', 'fold_generator', 'data', 'idx', 'pipeline', 'target_param', 'y_train', 'seed'}
2023-12-18 22:45:11,145:INFO:Checking environment
2023-12-18 22:45:11,145:INFO:python_version: 3.10.9
2023-12-18 22:45:11,146:INFO:python_build: ('main', 'Mar  1 2023 18:18:15')
2023-12-18 22:45:11,146:INFO:machine: AMD64
2023-12-18 22:45:11,146:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-18 22:45:11,148:INFO:Memory: svmem(total=34190274560, available=22797746176, percent=33.3, used=11392528384, free=22797746176)
2023-12-18 22:45:11,148:INFO:Physical Core: 24
2023-12-18 22:45:11,148:INFO:Logical Core: 32
2023-12-18 22:45:11,148:INFO:Checking libraries
2023-12-18 22:45:11,149:INFO:System:
2023-12-18 22:45:11,149:INFO:    python: 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]
2023-12-18 22:45:11,149:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-18 22:45:11,149:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-18 22:45:11,149:INFO:PyCaret required dependencies:
2023-12-18 22:45:11,156:INFO:                 pip: 22.3.1
2023-12-18 22:45:11,156:INFO:          setuptools: 65.6.3
2023-12-18 22:45:11,156:INFO:             pycaret: 3.2.0
2023-12-18 22:45:11,156:INFO:             IPython: 8.10.0
2023-12-18 22:45:11,157:INFO:          ipywidgets: 7.6.5
2023-12-18 22:45:11,157:INFO:                tqdm: 4.64.1
2023-12-18 22:45:11,157:INFO:               numpy: 1.23.5
2023-12-18 22:45:11,157:INFO:              pandas: 1.5.3
2023-12-18 22:45:11,158:INFO:              jinja2: 3.1.2
2023-12-18 22:45:11,158:INFO:               scipy: 1.10.1
2023-12-18 22:45:11,158:INFO:              joblib: 1.3.2
2023-12-18 22:45:11,158:INFO:             sklearn: 1.2.1
2023-12-18 22:45:11,159:INFO:                pyod: 1.1.2
2023-12-18 22:45:11,159:INFO:            imblearn: 0.10.1
2023-12-18 22:45:11,159:INFO:   category_encoders: 2.6.3
2023-12-18 22:45:11,160:INFO:            lightgbm: 4.1.0
2023-12-18 22:45:11,160:INFO:               numba: 0.56.4
2023-12-18 22:45:11,160:INFO:            requests: 2.28.1
2023-12-18 22:45:11,160:INFO:          matplotlib: 3.6.0
2023-12-18 22:45:11,161:INFO:          scikitplot: 0.3.7
2023-12-18 22:45:11,161:INFO:         yellowbrick: 1.5
2023-12-18 22:45:11,161:INFO:              plotly: 5.9.0
2023-12-18 22:45:11,161:INFO:    plotly-resampler: Not installed
2023-12-18 22:45:11,162:INFO:             kaleido: 0.2.1
2023-12-18 22:45:11,162:INFO:           schemdraw: 0.15
2023-12-18 22:45:11,162:INFO:         statsmodels: 0.13.5
2023-12-18 22:45:11,162:INFO:              sktime: 0.21.1
2023-12-18 22:45:11,162:INFO:               tbats: 1.1.3
2023-12-18 22:45:11,163:INFO:            pmdarima: 2.0.4
2023-12-18 22:45:11,163:INFO:              psutil: 5.9.0
2023-12-18 22:45:11,163:INFO:          markupsafe: 2.1.1
2023-12-18 22:45:11,163:INFO:             pickle5: Not installed
2023-12-18 22:45:11,164:INFO:         cloudpickle: 2.0.0
2023-12-18 22:45:11,164:INFO:         deprecation: 2.1.0
2023-12-18 22:45:11,164:INFO:              xxhash: 3.4.1
2023-12-18 22:45:11,164:INFO:           wurlitzer: Not installed
2023-12-18 22:45:11,164:INFO:PyCaret optional dependencies:
2023-12-18 22:45:11,215:INFO:                shap: Not installed
2023-12-18 22:45:11,215:INFO:           interpret: Not installed
2023-12-18 22:45:11,215:INFO:                umap: Not installed
2023-12-18 22:45:11,216:INFO:     ydata_profiling: Not installed
2023-12-18 22:45:11,216:INFO:  explainerdashboard: Not installed
2023-12-18 22:45:11,216:INFO:             autoviz: Not installed
2023-12-18 22:45:11,216:INFO:           fairlearn: Not installed
2023-12-18 22:45:11,216:INFO:          deepchecks: Not installed
2023-12-18 22:45:11,217:INFO:             xgboost: 2.0.2
2023-12-18 22:45:11,217:INFO:            catboost: Not installed
2023-12-18 22:45:11,217:INFO:              kmodes: Not installed
2023-12-18 22:45:11,217:INFO:             mlxtend: Not installed
2023-12-18 22:45:11,218:INFO:       statsforecast: Not installed
2023-12-18 22:45:11,218:INFO:        tune_sklearn: Not installed
2023-12-18 22:45:11,218:INFO:                 ray: Not installed
2023-12-18 22:45:11,218:INFO:            hyperopt: Not installed
2023-12-18 22:45:11,219:INFO:              optuna: Not installed
2023-12-18 22:45:11,219:INFO:               skopt: Not installed
2023-12-18 22:45:11,219:INFO:              mlflow: Not installed
2023-12-18 22:45:11,219:INFO:              gradio: Not installed
2023-12-18 22:45:11,219:INFO:             fastapi: Not installed
2023-12-18 22:45:11,220:INFO:             uvicorn: Not installed
2023-12-18 22:45:11,220:INFO:              m2cgen: Not installed
2023-12-18 22:45:11,220:INFO:           evidently: Not installed
2023-12-18 22:45:11,220:INFO:               fugue: Not installed
2023-12-18 22:45:11,220:INFO:           streamlit: Not installed
2023-12-18 22:45:11,221:INFO:             prophet: Not installed
2023-12-18 22:45:11,221:INFO:None
2023-12-18 22:45:11,221:INFO:Set up data.
2023-12-18 22:45:11,227:INFO:Set up folding strategy.
2023-12-18 22:45:11,227:INFO:Set up train/test split.
2023-12-18 22:45:11,231:INFO:Set up index.
2023-12-18 22:45:11,232:INFO:Assigning column types.
2023-12-18 22:45:11,234:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-18 22:45:11,254:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:45:11,256:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:45:11,272:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:45:11,275:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:45:11,295:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:45:11,297:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:45:11,310:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:45:11,312:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:45:11,312:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-18 22:45:11,333:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:45:11,347:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:45:11,349:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:45:11,370:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:45:11,384:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:45:11,386:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:45:11,387:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-18 22:45:11,422:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:45:11,424:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:45:11,458:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:45:11,460:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:45:11,462:INFO:Preparing preprocessing pipeline...
2023-12-18 22:45:11,464:INFO:Set up simple imputation.
2023-12-18 22:45:11,464:INFO:Set up column name cleaning.
2023-12-18 22:45:11,480:INFO:Finished creating preprocessing pipeline.
2023-12-18 22:45:11,484:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'He...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-18 22:45:11,485:INFO:Creating final display dataframe.
2023-12-18 22:45:11,531:INFO:Setup _display_container:                     Description             Value
0                    Session id              8281
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 31)
4        Transformed data shape        (7905, 31)
5   Transformed train set shape        (6719, 31)
6    Transformed test set shape        (1186, 31)
7              Numeric features                30
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              1289
2023-12-18 22:45:11,568:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:45:11,570:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:45:11,603:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:45:11,606:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:45:11,606:INFO:setup() successfully completed in 0.47s...............
2023-12-18 22:45:11,633:INFO:Initializing compare_models()
2023-12-18 22:45:11,633:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-12-18 22:45:11,634:INFO:Checking exceptions
2023-12-18 22:45:11,637:INFO:Preparing display monitor
2023-12-18 22:45:11,653:INFO:Initializing Logistic Regression
2023-12-18 22:45:11,654:INFO:Total runtime is 1.6637643178304036e-05 minutes
2023-12-18 22:45:11,656:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:11,657:INFO:Initializing create_model()
2023-12-18 22:45:11,657:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:11,657:INFO:Checking exceptions
2023-12-18 22:45:11,658:INFO:Importing libraries
2023-12-18 22:45:11,658:INFO:Copying training dataset
2023-12-18 22:45:11,662:INFO:Defining folds
2023-12-18 22:45:11,663:INFO:Declaring metric variables
2023-12-18 22:45:11,665:INFO:Importing untrained model
2023-12-18 22:45:11,667:INFO:Logistic Regression Imported successfully
2023-12-18 22:45:11,671:INFO:Starting cross validation
2023-12-18 22:45:11,671:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:15,816:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:15,839:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:15,861:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:15,862:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:15,867:INFO:Calculating mean and std
2023-12-18 22:45:15,868:INFO:Creating metrics dataframe
2023-12-18 22:45:15,871:INFO:Uploading results into container
2023-12-18 22:45:15,872:INFO:Uploading model into container now
2023-12-18 22:45:15,872:INFO:_master_model_container: 1
2023-12-18 22:45:15,872:INFO:_display_container: 2
2023-12-18 22:45:15,873:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8281, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-18 22:45:15,873:INFO:create_model() successfully completed......................................
2023-12-18 22:45:15,924:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:15,925:INFO:Creating metrics dataframe
2023-12-18 22:45:15,930:INFO:Initializing K Neighbors Classifier
2023-12-18 22:45:15,931:INFO:Total runtime is 0.07131084203720092 minutes
2023-12-18 22:45:15,933:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:15,934:INFO:Initializing create_model()
2023-12-18 22:45:15,934:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:15,934:INFO:Checking exceptions
2023-12-18 22:45:15,934:INFO:Importing libraries
2023-12-18 22:45:15,935:INFO:Copying training dataset
2023-12-18 22:45:15,938:INFO:Defining folds
2023-12-18 22:45:15,939:INFO:Declaring metric variables
2023-12-18 22:45:15,941:INFO:Importing untrained model
2023-12-18 22:45:15,943:INFO:K Neighbors Classifier Imported successfully
2023-12-18 22:45:15,948:INFO:Starting cross validation
2023-12-18 22:45:15,949:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:18,723:INFO:Calculating mean and std
2023-12-18 22:45:18,724:INFO:Creating metrics dataframe
2023-12-18 22:45:18,727:INFO:Uploading results into container
2023-12-18 22:45:18,728:INFO:Uploading model into container now
2023-12-18 22:45:18,728:INFO:_master_model_container: 2
2023-12-18 22:45:18,728:INFO:_display_container: 2
2023-12-18 22:45:18,729:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-18 22:45:18,729:INFO:create_model() successfully completed......................................
2023-12-18 22:45:18,778:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:18,779:INFO:Creating metrics dataframe
2023-12-18 22:45:18,784:INFO:Initializing Naive Bayes
2023-12-18 22:45:18,785:INFO:Total runtime is 0.11888058185577392 minutes
2023-12-18 22:45:18,787:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:18,787:INFO:Initializing create_model()
2023-12-18 22:45:18,788:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:18,788:INFO:Checking exceptions
2023-12-18 22:45:18,788:INFO:Importing libraries
2023-12-18 22:45:18,788:INFO:Copying training dataset
2023-12-18 22:45:18,791:INFO:Defining folds
2023-12-18 22:45:18,792:INFO:Declaring metric variables
2023-12-18 22:45:18,793:INFO:Importing untrained model
2023-12-18 22:45:18,795:INFO:Naive Bayes Imported successfully
2023-12-18 22:45:18,799:INFO:Starting cross validation
2023-12-18 22:45:18,800:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:21,665:INFO:Calculating mean and std
2023-12-18 22:45:21,668:INFO:Creating metrics dataframe
2023-12-18 22:45:21,670:INFO:Uploading results into container
2023-12-18 22:45:21,670:INFO:Uploading model into container now
2023-12-18 22:45:21,671:INFO:_master_model_container: 3
2023-12-18 22:45:21,671:INFO:_display_container: 2
2023-12-18 22:45:21,671:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-18 22:45:21,672:INFO:create_model() successfully completed......................................
2023-12-18 22:45:21,721:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:21,722:INFO:Creating metrics dataframe
2023-12-18 22:45:21,728:INFO:Initializing Decision Tree Classifier
2023-12-18 22:45:21,729:INFO:Total runtime is 0.16792590618133546 minutes
2023-12-18 22:45:21,730:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:21,731:INFO:Initializing create_model()
2023-12-18 22:45:21,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:21,731:INFO:Checking exceptions
2023-12-18 22:45:21,732:INFO:Importing libraries
2023-12-18 22:45:21,732:INFO:Copying training dataset
2023-12-18 22:45:21,735:INFO:Defining folds
2023-12-18 22:45:21,736:INFO:Declaring metric variables
2023-12-18 22:45:21,738:INFO:Importing untrained model
2023-12-18 22:45:21,740:INFO:Decision Tree Classifier Imported successfully
2023-12-18 22:45:21,743:INFO:Starting cross validation
2023-12-18 22:45:21,745:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:24,574:INFO:Calculating mean and std
2023-12-18 22:45:24,575:INFO:Creating metrics dataframe
2023-12-18 22:45:24,579:INFO:Uploading results into container
2023-12-18 22:45:24,579:INFO:Uploading model into container now
2023-12-18 22:45:24,580:INFO:_master_model_container: 4
2023-12-18 22:45:24,580:INFO:_display_container: 2
2023-12-18 22:45:24,581:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8281, splitter='best')
2023-12-18 22:45:24,581:INFO:create_model() successfully completed......................................
2023-12-18 22:45:24,628:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:24,629:INFO:Creating metrics dataframe
2023-12-18 22:45:24,635:INFO:Initializing SVM - Linear Kernel
2023-12-18 22:45:24,636:INFO:Total runtime is 0.21638629039128623 minutes
2023-12-18 22:45:24,638:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:24,638:INFO:Initializing create_model()
2023-12-18 22:45:24,639:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:24,639:INFO:Checking exceptions
2023-12-18 22:45:24,639:INFO:Importing libraries
2023-12-18 22:45:24,639:INFO:Copying training dataset
2023-12-18 22:45:24,643:INFO:Defining folds
2023-12-18 22:45:24,644:INFO:Declaring metric variables
2023-12-18 22:45:24,646:INFO:Importing untrained model
2023-12-18 22:45:24,648:INFO:SVM - Linear Kernel Imported successfully
2023-12-18 22:45:24,651:INFO:Starting cross validation
2023-12-18 22:45:24,652:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:27,210:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:45:27,289:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:45:27,289:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:45:27,292:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:27,309:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:45:27,313:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:27,319:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:45:27,322:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:27,339:INFO:Calculating mean and std
2023-12-18 22:45:27,340:INFO:Creating metrics dataframe
2023-12-18 22:45:27,342:INFO:Uploading results into container
2023-12-18 22:45:27,343:INFO:Uploading model into container now
2023-12-18 22:45:27,344:INFO:_master_model_container: 5
2023-12-18 22:45:27,344:INFO:_display_container: 2
2023-12-18 22:45:27,345:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8281, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-18 22:45:27,345:INFO:create_model() successfully completed......................................
2023-12-18 22:45:27,393:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:27,393:INFO:Creating metrics dataframe
2023-12-18 22:45:27,399:INFO:Initializing Ridge Classifier
2023-12-18 22:45:27,400:INFO:Total runtime is 0.2624646822611491 minutes
2023-12-18 22:45:27,402:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:27,402:INFO:Initializing create_model()
2023-12-18 22:45:27,403:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:27,403:INFO:Checking exceptions
2023-12-18 22:45:27,403:INFO:Importing libraries
2023-12-18 22:45:27,404:INFO:Copying training dataset
2023-12-18 22:45:27,407:INFO:Defining folds
2023-12-18 22:45:27,408:INFO:Declaring metric variables
2023-12-18 22:45:27,410:INFO:Importing untrained model
2023-12-18 22:45:27,412:INFO:Ridge Classifier Imported successfully
2023-12-18 22:45:27,415:INFO:Starting cross validation
2023-12-18 22:45:27,416:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:30,233:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:45:30,235:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:30,241:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:45:30,243:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:30,257:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:45:30,259:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:30,264:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:45:30,265:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:45:30,267:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:30,268:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:30,280:INFO:Calculating mean and std
2023-12-18 22:45:30,281:INFO:Creating metrics dataframe
2023-12-18 22:45:30,284:INFO:Uploading results into container
2023-12-18 22:45:30,284:INFO:Uploading model into container now
2023-12-18 22:45:30,285:INFO:_master_model_container: 6
2023-12-18 22:45:30,285:INFO:_display_container: 2
2023-12-18 22:45:30,286:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=8281, solver='auto',
                tol=0.0001)
2023-12-18 22:45:30,286:INFO:create_model() successfully completed......................................
2023-12-18 22:45:30,335:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:30,336:INFO:Creating metrics dataframe
2023-12-18 22:45:30,342:INFO:Initializing Random Forest Classifier
2023-12-18 22:45:30,343:INFO:Total runtime is 0.311502738793691 minutes
2023-12-18 22:45:30,345:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:30,345:INFO:Initializing create_model()
2023-12-18 22:45:30,346:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:30,346:INFO:Checking exceptions
2023-12-18 22:45:30,346:INFO:Importing libraries
2023-12-18 22:45:30,346:INFO:Copying training dataset
2023-12-18 22:45:30,350:INFO:Defining folds
2023-12-18 22:45:30,350:INFO:Declaring metric variables
2023-12-18 22:45:30,352:INFO:Importing untrained model
2023-12-18 22:45:30,354:INFO:Random Forest Classifier Imported successfully
2023-12-18 22:45:30,358:INFO:Starting cross validation
2023-12-18 22:45:30,358:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:33,039:INFO:Calculating mean and std
2023-12-18 22:45:33,040:INFO:Creating metrics dataframe
2023-12-18 22:45:33,043:INFO:Uploading results into container
2023-12-18 22:45:33,044:INFO:Uploading model into container now
2023-12-18 22:45:33,044:INFO:_master_model_container: 7
2023-12-18 22:45:33,045:INFO:_display_container: 2
2023-12-18 22:45:33,045:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8281, verbose=0, warm_start=False)
2023-12-18 22:45:33,046:INFO:create_model() successfully completed......................................
2023-12-18 22:45:33,095:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:33,095:INFO:Creating metrics dataframe
2023-12-18 22:45:33,102:INFO:Initializing Quadratic Discriminant Analysis
2023-12-18 22:45:33,103:INFO:Total runtime is 0.3575053572654724 minutes
2023-12-18 22:45:33,105:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:33,105:INFO:Initializing create_model()
2023-12-18 22:45:33,106:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:33,106:INFO:Checking exceptions
2023-12-18 22:45:33,106:INFO:Importing libraries
2023-12-18 22:45:33,107:INFO:Copying training dataset
2023-12-18 22:45:33,109:INFO:Defining folds
2023-12-18 22:45:33,110:INFO:Declaring metric variables
2023-12-18 22:45:33,111:INFO:Importing untrained model
2023-12-18 22:45:33,115:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-18 22:45:33,118:INFO:Starting cross validation
2023-12-18 22:45:33,120:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:33,137:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:45:33,138:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:45:33,138:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:45:33,163:INFO:Calculating mean and std
2023-12-18 22:45:33,164:INFO:Creating metrics dataframe
2023-12-18 22:45:33,166:INFO:Uploading results into container
2023-12-18 22:45:33,166:INFO:Uploading model into container now
2023-12-18 22:45:33,167:INFO:_master_model_container: 8
2023-12-18 22:45:33,167:INFO:_display_container: 2
2023-12-18 22:45:33,167:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-18 22:45:33,168:INFO:create_model() successfully completed......................................
2023-12-18 22:45:33,214:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:33,214:INFO:Creating metrics dataframe
2023-12-18 22:45:33,221:INFO:Initializing Ada Boost Classifier
2023-12-18 22:45:33,221:INFO:Total runtime is 0.359481143951416 minutes
2023-12-18 22:45:33,223:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:33,223:INFO:Initializing create_model()
2023-12-18 22:45:33,224:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:33,224:INFO:Checking exceptions
2023-12-18 22:45:33,224:INFO:Importing libraries
2023-12-18 22:45:33,225:INFO:Copying training dataset
2023-12-18 22:45:33,228:INFO:Defining folds
2023-12-18 22:45:33,228:INFO:Declaring metric variables
2023-12-18 22:45:33,230:INFO:Importing untrained model
2023-12-18 22:45:33,232:INFO:Ada Boost Classifier Imported successfully
2023-12-18 22:45:33,236:INFO:Starting cross validation
2023-12-18 22:45:33,236:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:33,443:INFO:Calculating mean and std
2023-12-18 22:45:33,444:INFO:Creating metrics dataframe
2023-12-18 22:45:33,446:INFO:Uploading results into container
2023-12-18 22:45:33,447:INFO:Uploading model into container now
2023-12-18 22:45:33,448:INFO:_master_model_container: 9
2023-12-18 22:45:33,448:INFO:_display_container: 2
2023-12-18 22:45:33,449:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8281)
2023-12-18 22:45:33,449:INFO:create_model() successfully completed......................................
2023-12-18 22:45:33,498:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:33,499:INFO:Creating metrics dataframe
2023-12-18 22:45:33,505:INFO:Initializing Gradient Boosting Classifier
2023-12-18 22:45:33,506:INFO:Total runtime is 0.3642173647880554 minutes
2023-12-18 22:45:33,508:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:33,509:INFO:Initializing create_model()
2023-12-18 22:45:33,509:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:33,509:INFO:Checking exceptions
2023-12-18 22:45:33,509:INFO:Importing libraries
2023-12-18 22:45:33,510:INFO:Copying training dataset
2023-12-18 22:45:33,513:INFO:Defining folds
2023-12-18 22:45:33,513:INFO:Declaring metric variables
2023-12-18 22:45:33,515:INFO:Importing untrained model
2023-12-18 22:45:33,517:INFO:Gradient Boosting Classifier Imported successfully
2023-12-18 22:45:33,521:INFO:Starting cross validation
2023-12-18 22:45:33,522:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:35,479:INFO:Calculating mean and std
2023-12-18 22:45:35,480:INFO:Creating metrics dataframe
2023-12-18 22:45:35,483:INFO:Uploading results into container
2023-12-18 22:45:35,483:INFO:Uploading model into container now
2023-12-18 22:45:35,484:INFO:_master_model_container: 10
2023-12-18 22:45:35,484:INFO:_display_container: 2
2023-12-18 22:45:35,485:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8281, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-18 22:45:35,485:INFO:create_model() successfully completed......................................
2023-12-18 22:45:35,533:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:35,533:INFO:Creating metrics dataframe
2023-12-18 22:45:35,540:INFO:Initializing Linear Discriminant Analysis
2023-12-18 22:45:35,541:INFO:Total runtime is 0.39813527663548787 minutes
2023-12-18 22:45:35,543:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:35,544:INFO:Initializing create_model()
2023-12-18 22:45:35,544:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:35,544:INFO:Checking exceptions
2023-12-18 22:45:35,545:INFO:Importing libraries
2023-12-18 22:45:35,545:INFO:Copying training dataset
2023-12-18 22:45:35,548:INFO:Defining folds
2023-12-18 22:45:35,549:INFO:Declaring metric variables
2023-12-18 22:45:35,551:INFO:Importing untrained model
2023-12-18 22:45:35,553:INFO:Linear Discriminant Analysis Imported successfully
2023-12-18 22:45:35,557:INFO:Starting cross validation
2023-12-18 22:45:35,558:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:35,592:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:35,592:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:35,593:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:35,603:INFO:Calculating mean and std
2023-12-18 22:45:35,605:INFO:Creating metrics dataframe
2023-12-18 22:45:35,607:INFO:Uploading results into container
2023-12-18 22:45:35,607:INFO:Uploading model into container now
2023-12-18 22:45:35,608:INFO:_master_model_container: 11
2023-12-18 22:45:35,608:INFO:_display_container: 2
2023-12-18 22:45:35,609:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-18 22:45:35,609:INFO:create_model() successfully completed......................................
2023-12-18 22:45:35,657:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:35,658:INFO:Creating metrics dataframe
2023-12-18 22:45:35,665:INFO:Initializing Extra Trees Classifier
2023-12-18 22:45:35,666:INFO:Total runtime is 0.4002276301383972 minutes
2023-12-18 22:45:35,668:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:35,668:INFO:Initializing create_model()
2023-12-18 22:45:35,669:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:35,669:INFO:Checking exceptions
2023-12-18 22:45:35,669:INFO:Importing libraries
2023-12-18 22:45:35,670:INFO:Copying training dataset
2023-12-18 22:45:35,673:INFO:Defining folds
2023-12-18 22:45:35,673:INFO:Declaring metric variables
2023-12-18 22:45:35,675:INFO:Importing untrained model
2023-12-18 22:45:35,677:INFO:Extra Trees Classifier Imported successfully
2023-12-18 22:45:35,681:INFO:Starting cross validation
2023-12-18 22:45:35,682:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:35,913:INFO:Calculating mean and std
2023-12-18 22:45:35,914:INFO:Creating metrics dataframe
2023-12-18 22:45:35,916:INFO:Uploading results into container
2023-12-18 22:45:35,917:INFO:Uploading model into container now
2023-12-18 22:45:35,917:INFO:_master_model_container: 12
2023-12-18 22:45:35,918:INFO:_display_container: 2
2023-12-18 22:45:35,918:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8281, verbose=0, warm_start=False)
2023-12-18 22:45:35,918:INFO:create_model() successfully completed......................................
2023-12-18 22:45:35,967:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:35,968:INFO:Creating metrics dataframe
2023-12-18 22:45:35,974:INFO:Initializing Extreme Gradient Boosting
2023-12-18 22:45:35,975:INFO:Total runtime is 0.4053767601648966 minutes
2023-12-18 22:45:35,978:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:35,978:INFO:Initializing create_model()
2023-12-18 22:45:35,979:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:35,979:INFO:Checking exceptions
2023-12-18 22:45:35,979:INFO:Importing libraries
2023-12-18 22:45:35,979:INFO:Copying training dataset
2023-12-18 22:45:35,983:INFO:Defining folds
2023-12-18 22:45:35,983:INFO:Declaring metric variables
2023-12-18 22:45:35,985:INFO:Importing untrained model
2023-12-18 22:45:35,987:INFO:Extreme Gradient Boosting Imported successfully
2023-12-18 22:45:35,991:INFO:Starting cross validation
2023-12-18 22:45:35,992:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:36,351:INFO:Calculating mean and std
2023-12-18 22:45:36,352:INFO:Creating metrics dataframe
2023-12-18 22:45:36,355:INFO:Uploading results into container
2023-12-18 22:45:36,356:INFO:Uploading model into container now
2023-12-18 22:45:36,357:INFO:_master_model_container: 13
2023-12-18 22:45:36,357:INFO:_display_container: 2
2023-12-18 22:45:36,358:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-18 22:45:36,358:INFO:create_model() successfully completed......................................
2023-12-18 22:45:36,406:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:36,407:INFO:Creating metrics dataframe
2023-12-18 22:45:36,413:INFO:Initializing Light Gradient Boosting Machine
2023-12-18 22:45:36,414:INFO:Total runtime is 0.4126945455869038 minutes
2023-12-18 22:45:36,416:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:36,417:INFO:Initializing create_model()
2023-12-18 22:45:36,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:36,417:INFO:Checking exceptions
2023-12-18 22:45:36,418:INFO:Importing libraries
2023-12-18 22:45:36,418:INFO:Copying training dataset
2023-12-18 22:45:36,421:INFO:Defining folds
2023-12-18 22:45:36,421:INFO:Declaring metric variables
2023-12-18 22:45:36,423:INFO:Importing untrained model
2023-12-18 22:45:36,425:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:45:36,429:INFO:Starting cross validation
2023-12-18 22:45:36,430:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:39,151:INFO:Calculating mean and std
2023-12-18 22:45:39,153:INFO:Creating metrics dataframe
2023-12-18 22:45:39,156:INFO:Uploading results into container
2023-12-18 22:45:39,156:INFO:Uploading model into container now
2023-12-18 22:45:39,157:INFO:_master_model_container: 14
2023-12-18 22:45:39,157:INFO:_display_container: 2
2023-12-18 22:45:39,158:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:45:39,158:INFO:create_model() successfully completed......................................
2023-12-18 22:45:39,213:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:39,213:INFO:Creating metrics dataframe
2023-12-18 22:45:39,222:INFO:Initializing Dummy Classifier
2023-12-18 22:45:39,223:INFO:Total runtime is 0.45950669844945263 minutes
2023-12-18 22:45:39,226:INFO:SubProcess create_model() called ==================================
2023-12-18 22:45:39,227:INFO:Initializing create_model()
2023-12-18 22:45:39,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352FFC040>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:39,227:INFO:Checking exceptions
2023-12-18 22:45:39,228:INFO:Importing libraries
2023-12-18 22:45:39,228:INFO:Copying training dataset
2023-12-18 22:45:39,232:INFO:Defining folds
2023-12-18 22:45:39,233:INFO:Declaring metric variables
2023-12-18 22:45:39,235:INFO:Importing untrained model
2023-12-18 22:45:39,237:INFO:Dummy Classifier Imported successfully
2023-12-18 22:45:39,242:INFO:Starting cross validation
2023-12-18 22:45:39,243:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:45:39,259:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:39,260:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:39,261:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:39,261:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:39,262:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:45:39,274:INFO:Calculating mean and std
2023-12-18 22:45:39,275:INFO:Creating metrics dataframe
2023-12-18 22:45:39,277:INFO:Uploading results into container
2023-12-18 22:45:39,277:INFO:Uploading model into container now
2023-12-18 22:45:39,278:INFO:_master_model_container: 15
2023-12-18 22:45:39,278:INFO:_display_container: 2
2023-12-18 22:45:39,278:INFO:DummyClassifier(constant=None, random_state=8281, strategy='prior')
2023-12-18 22:45:39,279:INFO:create_model() successfully completed......................................
2023-12-18 22:45:39,325:INFO:SubProcess create_model() end ==================================
2023-12-18 22:45:39,326:INFO:Creating metrics dataframe
2023-12-18 22:45:39,337:INFO:Initializing create_model()
2023-12-18 22:45:39,338:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:39,339:INFO:Checking exceptions
2023-12-18 22:45:39,340:INFO:Importing libraries
2023-12-18 22:45:39,340:INFO:Copying training dataset
2023-12-18 22:45:39,343:INFO:Defining folds
2023-12-18 22:45:39,344:INFO:Declaring metric variables
2023-12-18 22:45:39,344:INFO:Importing untrained model
2023-12-18 22:45:39,345:INFO:Declaring custom model
2023-12-18 22:45:39,345:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:45:39,346:INFO:Cross validation set to False
2023-12-18 22:45:39,346:INFO:Fitting Model
2023-12-18 22:45:39,361:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000434 seconds.
2023-12-18 22:45:39,362:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-18 22:45:39,362:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-18 22:45:39,363:INFO:[LightGBM] [Info] Total Bins 1710
2023-12-18 22:45:39,363:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 30
2023-12-18 22:45:39,364:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-18 22:45:39,364:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-18 22:45:39,366:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-18 22:45:39,676:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:45:39,677:INFO:create_model() successfully completed......................................
2023-12-18 22:45:39,734:INFO:Initializing create_model()
2023-12-18 22:45:39,735:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8281, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:39,735:INFO:Checking exceptions
2023-12-18 22:45:39,736:INFO:Importing libraries
2023-12-18 22:45:39,737:INFO:Copying training dataset
2023-12-18 22:45:39,741:INFO:Defining folds
2023-12-18 22:45:39,742:INFO:Declaring metric variables
2023-12-18 22:45:39,743:INFO:Importing untrained model
2023-12-18 22:45:39,743:INFO:Declaring custom model
2023-12-18 22:45:39,744:INFO:Gradient Boosting Classifier Imported successfully
2023-12-18 22:45:39,744:INFO:Cross validation set to False
2023-12-18 22:45:39,745:INFO:Fitting Model
2023-12-18 22:45:42,097:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8281, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-18 22:45:42,098:INFO:create_model() successfully completed......................................
2023-12-18 22:45:42,149:INFO:Initializing create_model()
2023-12-18 22:45:42,149:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8281, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:42,149:INFO:Checking exceptions
2023-12-18 22:45:42,150:INFO:Importing libraries
2023-12-18 22:45:42,151:INFO:Copying training dataset
2023-12-18 22:45:42,154:INFO:Defining folds
2023-12-18 22:45:42,154:INFO:Declaring metric variables
2023-12-18 22:45:42,155:INFO:Importing untrained model
2023-12-18 22:45:42,155:INFO:Declaring custom model
2023-12-18 22:45:42,155:INFO:Random Forest Classifier Imported successfully
2023-12-18 22:45:42,156:INFO:Cross validation set to False
2023-12-18 22:45:42,156:INFO:Fitting Model
2023-12-18 22:45:42,302:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8281, verbose=0, warm_start=False)
2023-12-18 22:45:42,303:INFO:create_model() successfully completed......................................
2023-12-18 22:45:42,353:INFO:Initializing create_model()
2023-12-18 22:45:42,354:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:42,354:INFO:Checking exceptions
2023-12-18 22:45:42,355:INFO:Importing libraries
2023-12-18 22:45:42,356:INFO:Copying training dataset
2023-12-18 22:45:42,360:INFO:Defining folds
2023-12-18 22:45:42,361:INFO:Declaring metric variables
2023-12-18 22:45:42,362:INFO:Importing untrained model
2023-12-18 22:45:42,362:INFO:Declaring custom model
2023-12-18 22:45:42,363:INFO:Extreme Gradient Boosting Imported successfully
2023-12-18 22:45:42,364:INFO:Cross validation set to False
2023-12-18 22:45:42,364:INFO:Fitting Model
2023-12-18 22:45:42,586:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-18 22:45:42,587:INFO:create_model() successfully completed......................................
2023-12-18 22:45:42,646:INFO:Initializing create_model()
2023-12-18 22:45:42,647:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8281, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:45:42,647:INFO:Checking exceptions
2023-12-18 22:45:42,649:INFO:Importing libraries
2023-12-18 22:45:42,650:INFO:Copying training dataset
2023-12-18 22:45:42,654:INFO:Defining folds
2023-12-18 22:45:42,656:INFO:Declaring metric variables
2023-12-18 22:45:42,656:INFO:Importing untrained model
2023-12-18 22:45:42,656:INFO:Declaring custom model
2023-12-18 22:45:42,657:INFO:Extra Trees Classifier Imported successfully
2023-12-18 22:45:42,658:INFO:Cross validation set to False
2023-12-18 22:45:42,658:INFO:Fitting Model
2023-12-18 22:45:42,770:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8281, verbose=0, warm_start=False)
2023-12-18 22:45:42,771:INFO:create_model() successfully completed......................................
2023-12-18 22:45:42,832:INFO:_master_model_container: 15
2023-12-18 22:45:42,833:INFO:_display_container: 2
2023-12-18 22:45:42,834:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8281, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8281, verbose=0, warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8281, verbose=0, warm_start=False)]
2023-12-18 22:45:42,835:INFO:compare_models() successfully completed......................................
2023-12-18 22:45:42,879:INFO:Initializing tune_model()
2023-12-18 22:45:42,880:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>)
2023-12-18 22:45:42,880:INFO:Checking exceptions
2023-12-18 22:45:42,891:INFO:Copying training dataset
2023-12-18 22:45:42,894:INFO:Checking base model
2023-12-18 22:45:42,894:INFO:Base model : Light Gradient Boosting Machine
2023-12-18 22:45:42,896:INFO:Declaring metric variables
2023-12-18 22:45:42,898:INFO:Defining Hyperparameters
2023-12-18 22:45:42,954:INFO:Tuning with n_jobs=-1
2023-12-18 22:45:42,955:INFO:Initializing RandomizedSearchCV
2023-12-18 22:46:03,267:INFO:best_params: {'actual_estimator__reg_lambda': 0.1, 'actual_estimator__reg_alpha': 0.1, 'actual_estimator__num_leaves': 50, 'actual_estimator__n_estimators': 190, 'actual_estimator__min_split_gain': 0.5, 'actual_estimator__min_child_samples': 11, 'actual_estimator__learning_rate': 0.15, 'actual_estimator__feature_fraction': 0.7, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.8}
2023-12-18 22:46:03,269:INFO:Hyperparameter search completed
2023-12-18 22:46:03,270:INFO:SubProcess create_model() called ==================================
2023-12-18 22:46:03,271:INFO:Initializing create_model()
2023-12-18 22:46:03,271:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B12EB90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.1, 'reg_alpha': 0.1, 'num_leaves': 50, 'n_estimators': 190, 'min_split_gain': 0.5, 'min_child_samples': 11, 'learning_rate': 0.15, 'feature_fraction': 0.7, 'bagging_freq': 1, 'bagging_fraction': 0.8})
2023-12-18 22:46:03,272:INFO:Checking exceptions
2023-12-18 22:46:03,272:INFO:Importing libraries
2023-12-18 22:46:03,272:INFO:Copying training dataset
2023-12-18 22:46:03,276:INFO:Defining folds
2023-12-18 22:46:03,277:INFO:Declaring metric variables
2023-12-18 22:46:03,280:INFO:Importing untrained model
2023-12-18 22:46:03,280:INFO:Declaring custom model
2023-12-18 22:46:03,283:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:46:03,288:INFO:Starting cross validation
2023-12-18 22:46:03,289:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:46:05,429:INFO:Calculating mean and std
2023-12-18 22:46:05,430:INFO:Creating metrics dataframe
2023-12-18 22:46:05,434:INFO:Finalizing model
2023-12-18 22:46:05,443:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2023-12-18 22:46:05,443:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-18 22:46:05,443:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2023-12-18 22:46:05,447:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2023-12-18 22:46:05,449:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-18 22:46:05,449:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2023-12-18 22:46:05,451:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.
2023-12-18 22:46:05,452:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-18 22:46:05,452:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-18 22:46:05,452:INFO:[LightGBM] [Info] Total Bins 1710
2023-12-18 22:46:05,453:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 30
2023-12-18 22:46:05,453:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-18 22:46:05,454:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-18 22:46:05,454:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-18 22:46:05,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,682:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,703:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,758:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,767:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,769:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,771:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,772:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,774:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,779:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,781:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,787:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,807:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:46:05,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,832:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:46:05,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,863:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:46:05,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,871:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:46:05,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,883:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-18 22:46:05,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-18 22:46:05,937:INFO:Uploading results into container
2023-12-18 22:46:05,938:INFO:Uploading model into container now
2023-12-18 22:46:05,939:INFO:_master_model_container: 16
2023-12-18 22:46:05,939:INFO:_display_container: 3
2023-12-18 22:46:05,940:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=190, n_jobs=-1, num_leaves=50, objective=None,
               random_state=8281, reg_alpha=0.1, reg_lambda=0.1, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:46:05,940:INFO:create_model() successfully completed......................................
2023-12-18 22:46:05,996:INFO:SubProcess create_model() end ==================================
2023-12-18 22:46:05,997:INFO:choose_better activated
2023-12-18 22:46:06,000:INFO:SubProcess create_model() called ==================================
2023-12-18 22:46:06,001:INFO:Initializing create_model()
2023-12-18 22:46:06,002:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:46:06,002:INFO:Checking exceptions
2023-12-18 22:46:06,003:INFO:Importing libraries
2023-12-18 22:46:06,004:INFO:Copying training dataset
2023-12-18 22:46:06,008:INFO:Defining folds
2023-12-18 22:46:06,008:INFO:Declaring metric variables
2023-12-18 22:46:06,009:INFO:Importing untrained model
2023-12-18 22:46:06,009:INFO:Declaring custom model
2023-12-18 22:46:06,010:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:46:06,010:INFO:Starting cross validation
2023-12-18 22:46:06,011:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:46:08,855:INFO:Calculating mean and std
2023-12-18 22:46:08,857:INFO:Creating metrics dataframe
2023-12-18 22:46:08,859:INFO:Finalizing model
2023-12-18 22:46:08,874:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001040 seconds.
2023-12-18 22:46:08,875:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-18 22:46:08,875:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-18 22:46:08,876:INFO:[LightGBM] [Info] Total Bins 1710
2023-12-18 22:46:08,876:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 30
2023-12-18 22:46:08,877:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-18 22:46:08,877:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-18 22:46:08,877:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-18 22:46:09,230:INFO:Uploading results into container
2023-12-18 22:46:09,231:INFO:Uploading model into container now
2023-12-18 22:46:09,231:INFO:_master_model_container: 17
2023-12-18 22:46:09,231:INFO:_display_container: 4
2023-12-18 22:46:09,232:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:46:09,232:INFO:create_model() successfully completed......................................
2023-12-18 22:46:09,286:INFO:SubProcess create_model() end ==================================
2023-12-18 22:46:09,289:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8281, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8277
2023-12-18 22:46:09,290:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=190, n_jobs=-1, num_leaves=50, objective=None,
               random_state=8281, reg_alpha=0.1, reg_lambda=0.1, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8309
2023-12-18 22:46:09,291:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=190, n_jobs=-1, num_leaves=50, objective=None,
               random_state=8281, reg_alpha=0.1, reg_lambda=0.1, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-18 22:46:09,291:INFO:choose_better completed
2023-12-18 22:46:09,296:INFO:_master_model_container: 17
2023-12-18 22:46:09,298:INFO:_display_container: 3
2023-12-18 22:46:09,298:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=190, n_jobs=-1, num_leaves=50, objective=None,
               random_state=8281, reg_alpha=0.1, reg_lambda=0.1, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:46:09,299:INFO:tune_model() successfully completed......................................
2023-12-18 22:46:09,394:INFO:Initializing evaluate_model()
2023-12-18 22:46:09,395:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=190, n_jobs=-1, num_leaves=50, objective=None,
               random_state=8281, reg_alpha=0.1, reg_lambda=0.1, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-18 22:46:09,404:INFO:Initializing plot_model()
2023-12-18 22:46:09,405:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=190, n_jobs=-1, num_leaves=50, objective=None,
               random_state=8281, reg_alpha=0.1, reg_lambda=0.1, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, system=True)
2023-12-18 22:46:09,405:INFO:Checking exceptions
2023-12-18 22:46:09,407:INFO:Preloading libraries
2023-12-18 22:46:09,431:INFO:Copying training dataset
2023-12-18 22:46:09,432:INFO:Plot type: pipeline
2023-12-18 22:46:09,550:INFO:Visual Rendered Successfully
2023-12-18 22:46:09,599:INFO:plot_model() successfully completed......................................
2023-12-18 22:46:09,636:INFO:Initializing predict_model()
2023-12-18 22:46:09,636:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=190, n_jobs=-1, num_leaves=50, objective=None,
               random_state=8281, reg_alpha=0.1, reg_lambda=0.1, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002730EF52EF0>)
2023-12-18 22:46:09,637:INFO:Checking exceptions
2023-12-18 22:46:09,637:INFO:Preloading libraries
2023-12-18 22:46:09,639:INFO:Set up data.
2023-12-18 22:46:09,643:INFO:Set up index.
2023-12-18 22:46:25,817:INFO:Initializing predict_model()
2023-12-18 22:46:25,818:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730EC47B20>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=11, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=190, n_jobs=-1, num_leaves=50, objective=None,
               random_state=8281, reg_alpha=0.1, reg_lambda=0.1, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002735349D900>)
2023-12-18 22:46:25,818:INFO:Checking exceptions
2023-12-18 22:46:25,819:INFO:Preloading libraries
2023-12-18 22:46:25,821:INFO:Set up data.
2023-12-18 22:46:25,825:INFO:Set up index.
2023-12-18 22:48:00,015:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_36848\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-18 22:48:00,019:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_36848\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-18 22:48:00,071:INFO:PyCaret ClassificationExperiment
2023-12-18 22:48:00,071:INFO:Logging name: clf-default-name
2023-12-18 22:48:00,071:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-18 22:48:00,072:INFO:version 3.2.0
2023-12-18 22:48:00,072:INFO:Initializing setup()
2023-12-18 22:48:00,072:INFO:self.USI: 5742
2023-12-18 22:48:00,072:INFO:self._variable_keys: {'is_multiclass', 'fix_imbalance', '_available_plots', 'gpu_n_jobs_param', 'y_test', '_ml_usecase', 'exp_name_log', 'log_plots_param', 'html_param', 'USI', 'exp_id', 'fold_shuffle_param', 'n_jobs_param', 'X', 'gpu_param', 'memory', 'fold_groups_param', 'X_test', 'logging_param', 'y', 'X_train', 'fold_generator', 'data', 'idx', 'pipeline', 'target_param', 'y_train', 'seed'}
2023-12-18 22:48:00,073:INFO:Checking environment
2023-12-18 22:48:00,073:INFO:python_version: 3.10.9
2023-12-18 22:48:00,073:INFO:python_build: ('main', 'Mar  1 2023 18:18:15')
2023-12-18 22:48:00,073:INFO:machine: AMD64
2023-12-18 22:48:00,074:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-18 22:48:00,074:INFO:Memory: svmem(total=34190274560, available=17991983104, percent=47.4, used=16198291456, free=17991983104)
2023-12-18 22:48:00,074:INFO:Physical Core: 24
2023-12-18 22:48:00,074:INFO:Logical Core: 32
2023-12-18 22:48:00,075:INFO:Checking libraries
2023-12-18 22:48:00,075:INFO:System:
2023-12-18 22:48:00,075:INFO:    python: 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]
2023-12-18 22:48:00,076:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-18 22:48:00,076:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-18 22:48:00,076:INFO:PyCaret required dependencies:
2023-12-18 22:48:00,076:INFO:                 pip: 22.3.1
2023-12-18 22:48:00,077:INFO:          setuptools: 65.6.3
2023-12-18 22:48:00,077:INFO:             pycaret: 3.2.0
2023-12-18 22:48:00,077:INFO:             IPython: 8.10.0
2023-12-18 22:48:00,077:INFO:          ipywidgets: 7.6.5
2023-12-18 22:48:00,078:INFO:                tqdm: 4.64.1
2023-12-18 22:48:00,078:INFO:               numpy: 1.23.5
2023-12-18 22:48:00,078:INFO:              pandas: 1.5.3
2023-12-18 22:48:00,078:INFO:              jinja2: 3.1.2
2023-12-18 22:48:00,079:INFO:               scipy: 1.10.1
2023-12-18 22:48:00,079:INFO:              joblib: 1.3.2
2023-12-18 22:48:00,079:INFO:             sklearn: 1.2.1
2023-12-18 22:48:00,079:INFO:                pyod: 1.1.2
2023-12-18 22:48:00,080:INFO:            imblearn: 0.10.1
2023-12-18 22:48:00,080:INFO:   category_encoders: 2.6.3
2023-12-18 22:48:00,080:INFO:            lightgbm: 4.1.0
2023-12-18 22:48:00,080:INFO:               numba: 0.56.4
2023-12-18 22:48:00,081:INFO:            requests: 2.28.1
2023-12-18 22:48:00,081:INFO:          matplotlib: 3.6.0
2023-12-18 22:48:00,081:INFO:          scikitplot: 0.3.7
2023-12-18 22:48:00,081:INFO:         yellowbrick: 1.5
2023-12-18 22:48:00,081:INFO:              plotly: 5.9.0
2023-12-18 22:48:00,082:INFO:    plotly-resampler: Not installed
2023-12-18 22:48:00,082:INFO:             kaleido: 0.2.1
2023-12-18 22:48:00,082:INFO:           schemdraw: 0.15
2023-12-18 22:48:00,082:INFO:         statsmodels: 0.13.5
2023-12-18 22:48:00,083:INFO:              sktime: 0.21.1
2023-12-18 22:48:00,083:INFO:               tbats: 1.1.3
2023-12-18 22:48:00,083:INFO:            pmdarima: 2.0.4
2023-12-18 22:48:00,083:INFO:              psutil: 5.9.0
2023-12-18 22:48:00,084:INFO:          markupsafe: 2.1.1
2023-12-18 22:48:00,084:INFO:             pickle5: Not installed
2023-12-18 22:48:00,084:INFO:         cloudpickle: 2.0.0
2023-12-18 22:48:00,085:INFO:         deprecation: 2.1.0
2023-12-18 22:48:00,085:INFO:              xxhash: 3.4.1
2023-12-18 22:48:00,085:INFO:           wurlitzer: Not installed
2023-12-18 22:48:00,085:INFO:PyCaret optional dependencies:
2023-12-18 22:48:00,085:INFO:                shap: Not installed
2023-12-18 22:48:00,086:INFO:           interpret: Not installed
2023-12-18 22:48:00,086:INFO:                umap: Not installed
2023-12-18 22:48:00,086:INFO:     ydata_profiling: Not installed
2023-12-18 22:48:00,086:INFO:  explainerdashboard: Not installed
2023-12-18 22:48:00,087:INFO:             autoviz: Not installed
2023-12-18 22:48:00,087:INFO:           fairlearn: Not installed
2023-12-18 22:48:00,087:INFO:          deepchecks: Not installed
2023-12-18 22:48:00,087:INFO:             xgboost: 2.0.2
2023-12-18 22:48:00,088:INFO:            catboost: Not installed
2023-12-18 22:48:00,088:INFO:              kmodes: Not installed
2023-12-18 22:48:00,088:INFO:             mlxtend: Not installed
2023-12-18 22:48:00,088:INFO:       statsforecast: Not installed
2023-12-18 22:48:00,088:INFO:        tune_sklearn: Not installed
2023-12-18 22:48:00,089:INFO:                 ray: Not installed
2023-12-18 22:48:00,089:INFO:            hyperopt: Not installed
2023-12-18 22:48:00,089:INFO:              optuna: Not installed
2023-12-18 22:48:00,089:INFO:               skopt: Not installed
2023-12-18 22:48:00,089:INFO:              mlflow: Not installed
2023-12-18 22:48:00,090:INFO:              gradio: Not installed
2023-12-18 22:48:00,090:INFO:             fastapi: Not installed
2023-12-18 22:48:00,090:INFO:             uvicorn: Not installed
2023-12-18 22:48:00,090:INFO:              m2cgen: Not installed
2023-12-18 22:48:00,091:INFO:           evidently: Not installed
2023-12-18 22:48:00,091:INFO:               fugue: Not installed
2023-12-18 22:48:00,091:INFO:           streamlit: Not installed
2023-12-18 22:48:00,091:INFO:             prophet: Not installed
2023-12-18 22:48:00,091:INFO:None
2023-12-18 22:48:00,092:INFO:Set up data.
2023-12-18 22:48:00,096:INFO:Set up folding strategy.
2023-12-18 22:48:00,097:INFO:Set up train/test split.
2023-12-18 22:48:00,100:INFO:Set up index.
2023-12-18 22:48:00,101:INFO:Assigning column types.
2023-12-18 22:48:00,103:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-18 22:48:00,123:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:48:00,124:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:48:00,138:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:48:00,140:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:48:00,162:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-18 22:48:00,163:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:48:00,177:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:48:00,179:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:48:00,180:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-18 22:48:00,201:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:48:00,215:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:48:00,218:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:48:00,239:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-18 22:48:00,253:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:48:00,255:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:48:00,256:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-18 22:48:00,289:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:48:00,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:48:00,325:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:48:00,328:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:48:00,329:INFO:Preparing preprocessing pipeline...
2023-12-18 22:48:00,330:INFO:Set up simple imputation.
2023-12-18 22:48:00,331:INFO:Set up column name cleaning.
2023-12-18 22:48:00,344:INFO:Finished creating preprocessing pipeline.
2023-12-18 22:48:00,347:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'He...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-18 22:48:00,348:INFO:Creating final display dataframe.
2023-12-18 22:48:00,396:INFO:Setup _display_container:                     Description             Value
0                    Session id              5096
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 31)
4        Transformed data shape        (7905, 31)
5   Transformed train set shape        (6719, 31)
6    Transformed test set shape        (1186, 31)
7              Numeric features                30
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              5742
2023-12-18 22:48:00,433:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:48:00,435:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:48:00,469:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-18 22:48:00,471:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-18 22:48:00,472:INFO:setup() successfully completed in 0.4s...............
2023-12-18 22:48:00,492:INFO:Initializing compare_models()
2023-12-18 22:48:00,494:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-12-18 22:48:00,494:INFO:Checking exceptions
2023-12-18 22:48:00,497:INFO:Preparing display monitor
2023-12-18 22:48:00,513:INFO:Initializing Logistic Regression
2023-12-18 22:48:00,514:INFO:Total runtime is 1.6673405965169272e-05 minutes
2023-12-18 22:48:00,516:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:00,517:INFO:Initializing create_model()
2023-12-18 22:48:00,517:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:00,517:INFO:Checking exceptions
2023-12-18 22:48:00,518:INFO:Importing libraries
2023-12-18 22:48:00,518:INFO:Copying training dataset
2023-12-18 22:48:00,521:INFO:Defining folds
2023-12-18 22:48:00,522:INFO:Declaring metric variables
2023-12-18 22:48:00,524:INFO:Importing untrained model
2023-12-18 22:48:00,526:INFO:Logistic Regression Imported successfully
2023-12-18 22:48:00,530:INFO:Starting cross validation
2023-12-18 22:48:00,531:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:00,620:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:00,627:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:00,628:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:00,629:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:00,641:INFO:Calculating mean and std
2023-12-18 22:48:00,642:INFO:Creating metrics dataframe
2023-12-18 22:48:00,644:INFO:Uploading results into container
2023-12-18 22:48:00,645:INFO:Uploading model into container now
2023-12-18 22:48:00,645:INFO:_master_model_container: 1
2023-12-18 22:48:00,645:INFO:_display_container: 2
2023-12-18 22:48:00,646:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=5096, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-18 22:48:00,646:INFO:create_model() successfully completed......................................
2023-12-18 22:48:00,712:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:00,713:INFO:Creating metrics dataframe
2023-12-18 22:48:00,718:INFO:Initializing K Neighbors Classifier
2023-12-18 22:48:00,719:INFO:Total runtime is 0.0034392873446146646 minutes
2023-12-18 22:48:00,721:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:00,721:INFO:Initializing create_model()
2023-12-18 22:48:00,722:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:00,722:INFO:Checking exceptions
2023-12-18 22:48:00,723:INFO:Importing libraries
2023-12-18 22:48:00,723:INFO:Copying training dataset
2023-12-18 22:48:00,727:INFO:Defining folds
2023-12-18 22:48:00,728:INFO:Declaring metric variables
2023-12-18 22:48:00,730:INFO:Importing untrained model
2023-12-18 22:48:00,732:INFO:K Neighbors Classifier Imported successfully
2023-12-18 22:48:00,735:INFO:Starting cross validation
2023-12-18 22:48:00,736:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:01,045:INFO:Calculating mean and std
2023-12-18 22:48:01,046:INFO:Creating metrics dataframe
2023-12-18 22:48:01,050:INFO:Uploading results into container
2023-12-18 22:48:01,051:INFO:Uploading model into container now
2023-12-18 22:48:01,052:INFO:_master_model_container: 2
2023-12-18 22:48:01,052:INFO:_display_container: 2
2023-12-18 22:48:01,053:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-18 22:48:01,053:INFO:create_model() successfully completed......................................
2023-12-18 22:48:01,154:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:01,155:INFO:Creating metrics dataframe
2023-12-18 22:48:01,160:INFO:Initializing Naive Bayes
2023-12-18 22:48:01,161:INFO:Total runtime is 0.010805368423461914 minutes
2023-12-18 22:48:01,163:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:01,164:INFO:Initializing create_model()
2023-12-18 22:48:01,164:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:01,164:INFO:Checking exceptions
2023-12-18 22:48:01,165:INFO:Importing libraries
2023-12-18 22:48:01,165:INFO:Copying training dataset
2023-12-18 22:48:01,168:INFO:Defining folds
2023-12-18 22:48:01,169:INFO:Declaring metric variables
2023-12-18 22:48:01,171:INFO:Importing untrained model
2023-12-18 22:48:01,173:INFO:Naive Bayes Imported successfully
2023-12-18 22:48:01,177:INFO:Starting cross validation
2023-12-18 22:48:01,178:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:01,217:INFO:Calculating mean and std
2023-12-18 22:48:01,218:INFO:Creating metrics dataframe
2023-12-18 22:48:01,220:INFO:Uploading results into container
2023-12-18 22:48:01,221:INFO:Uploading model into container now
2023-12-18 22:48:01,221:INFO:_master_model_container: 3
2023-12-18 22:48:01,222:INFO:_display_container: 2
2023-12-18 22:48:01,222:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-18 22:48:01,222:INFO:create_model() successfully completed......................................
2023-12-18 22:48:01,288:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:01,289:INFO:Creating metrics dataframe
2023-12-18 22:48:01,293:INFO:Initializing Decision Tree Classifier
2023-12-18 22:48:01,295:INFO:Total runtime is 0.013039195537567138 minutes
2023-12-18 22:48:01,297:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:01,297:INFO:Initializing create_model()
2023-12-18 22:48:01,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:01,298:INFO:Checking exceptions
2023-12-18 22:48:01,298:INFO:Importing libraries
2023-12-18 22:48:01,298:INFO:Copying training dataset
2023-12-18 22:48:01,302:INFO:Defining folds
2023-12-18 22:48:01,302:INFO:Declaring metric variables
2023-12-18 22:48:01,304:INFO:Importing untrained model
2023-12-18 22:48:01,306:INFO:Decision Tree Classifier Imported successfully
2023-12-18 22:48:01,310:INFO:Starting cross validation
2023-12-18 22:48:01,312:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:01,373:INFO:Calculating mean and std
2023-12-18 22:48:01,374:INFO:Creating metrics dataframe
2023-12-18 22:48:01,376:INFO:Uploading results into container
2023-12-18 22:48:01,376:INFO:Uploading model into container now
2023-12-18 22:48:01,377:INFO:_master_model_container: 4
2023-12-18 22:48:01,377:INFO:_display_container: 2
2023-12-18 22:48:01,378:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=5096, splitter='best')
2023-12-18 22:48:01,378:INFO:create_model() successfully completed......................................
2023-12-18 22:48:01,445:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:01,446:INFO:Creating metrics dataframe
2023-12-18 22:48:01,451:INFO:Initializing SVM - Linear Kernel
2023-12-18 22:48:01,452:INFO:Total runtime is 0.01565797726313273 minutes
2023-12-18 22:48:01,454:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:01,454:INFO:Initializing create_model()
2023-12-18 22:48:01,455:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:01,455:INFO:Checking exceptions
2023-12-18 22:48:01,455:INFO:Importing libraries
2023-12-18 22:48:01,455:INFO:Copying training dataset
2023-12-18 22:48:01,459:INFO:Defining folds
2023-12-18 22:48:01,459:INFO:Declaring metric variables
2023-12-18 22:48:01,461:INFO:Importing untrained model
2023-12-18 22:48:01,463:INFO:SVM - Linear Kernel Imported successfully
2023-12-18 22:48:01,467:INFO:Starting cross validation
2023-12-18 22:48:01,468:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:01,518:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:48:01,536:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:48:01,536:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:48:01,538:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:01,539:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:01,539:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-18 22:48:01,541:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:01,544:INFO:Calculating mean and std
2023-12-18 22:48:01,545:INFO:Creating metrics dataframe
2023-12-18 22:48:01,547:INFO:Uploading results into container
2023-12-18 22:48:01,548:INFO:Uploading model into container now
2023-12-18 22:48:01,548:INFO:_master_model_container: 5
2023-12-18 22:48:01,548:INFO:_display_container: 2
2023-12-18 22:48:01,549:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=5096, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-18 22:48:01,549:INFO:create_model() successfully completed......................................
2023-12-18 22:48:01,612:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:01,613:INFO:Creating metrics dataframe
2023-12-18 22:48:01,618:INFO:Initializing Ridge Classifier
2023-12-18 22:48:01,619:INFO:Total runtime is 0.018441800276438394 minutes
2023-12-18 22:48:01,621:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:01,621:INFO:Initializing create_model()
2023-12-18 22:48:01,622:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:01,622:INFO:Checking exceptions
2023-12-18 22:48:01,622:INFO:Importing libraries
2023-12-18 22:48:01,623:INFO:Copying training dataset
2023-12-18 22:48:01,626:INFO:Defining folds
2023-12-18 22:48:01,627:INFO:Declaring metric variables
2023-12-18 22:48:01,629:INFO:Importing untrained model
2023-12-18 22:48:01,631:INFO:Ridge Classifier Imported successfully
2023-12-18 22:48:01,634:INFO:Starting cross validation
2023-12-18 22:48:01,636:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:01,654:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:48:01,655:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:48:01,655:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:48:01,655:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:48:01,656:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:01,657:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:01,658:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-18 22:48:01,658:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:01,660:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:01,669:INFO:Calculating mean and std
2023-12-18 22:48:01,670:INFO:Creating metrics dataframe
2023-12-18 22:48:01,672:INFO:Uploading results into container
2023-12-18 22:48:01,673:INFO:Uploading model into container now
2023-12-18 22:48:01,674:INFO:_master_model_container: 6
2023-12-18 22:48:01,674:INFO:_display_container: 2
2023-12-18 22:48:01,674:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=5096, solver='auto',
                tol=0.0001)
2023-12-18 22:48:01,675:INFO:create_model() successfully completed......................................
2023-12-18 22:48:01,741:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:01,742:INFO:Creating metrics dataframe
2023-12-18 22:48:01,748:INFO:Initializing Random Forest Classifier
2023-12-18 22:48:01,749:INFO:Total runtime is 0.02060035467147827 minutes
2023-12-18 22:48:01,751:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:01,752:INFO:Initializing create_model()
2023-12-18 22:48:01,753:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:01,753:INFO:Checking exceptions
2023-12-18 22:48:01,753:INFO:Importing libraries
2023-12-18 22:48:01,754:INFO:Copying training dataset
2023-12-18 22:48:01,757:INFO:Defining folds
2023-12-18 22:48:01,758:INFO:Declaring metric variables
2023-12-18 22:48:01,760:INFO:Importing untrained model
2023-12-18 22:48:01,762:INFO:Random Forest Classifier Imported successfully
2023-12-18 22:48:01,765:INFO:Starting cross validation
2023-12-18 22:48:01,767:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:02,055:INFO:Calculating mean and std
2023-12-18 22:48:02,056:INFO:Creating metrics dataframe
2023-12-18 22:48:02,058:INFO:Uploading results into container
2023-12-18 22:48:02,059:INFO:Uploading model into container now
2023-12-18 22:48:02,060:INFO:_master_model_container: 7
2023-12-18 22:48:02,060:INFO:_display_container: 2
2023-12-18 22:48:02,061:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False)
2023-12-18 22:48:02,061:INFO:create_model() successfully completed......................................
2023-12-18 22:48:02,130:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:02,131:INFO:Creating metrics dataframe
2023-12-18 22:48:02,137:INFO:Initializing Quadratic Discriminant Analysis
2023-12-18 22:48:02,138:INFO:Total runtime is 0.027082788944244384 minutes
2023-12-18 22:48:02,140:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:02,141:INFO:Initializing create_model()
2023-12-18 22:48:02,141:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:02,142:INFO:Checking exceptions
2023-12-18 22:48:02,142:INFO:Importing libraries
2023-12-18 22:48:02,142:INFO:Copying training dataset
2023-12-18 22:48:02,146:INFO:Defining folds
2023-12-18 22:48:02,146:INFO:Declaring metric variables
2023-12-18 22:48:02,148:INFO:Importing untrained model
2023-12-18 22:48:02,150:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-18 22:48:02,154:INFO:Starting cross validation
2023-12-18 22:48:02,155:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:02,171:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:48:02,172:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:48:02,173:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:48:02,173:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:48:02,174:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-18 22:48:02,197:INFO:Calculating mean and std
2023-12-18 22:48:02,198:INFO:Creating metrics dataframe
2023-12-18 22:48:02,200:INFO:Uploading results into container
2023-12-18 22:48:02,201:INFO:Uploading model into container now
2023-12-18 22:48:02,201:INFO:_master_model_container: 8
2023-12-18 22:48:02,202:INFO:_display_container: 2
2023-12-18 22:48:02,202:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-18 22:48:02,202:INFO:create_model() successfully completed......................................
2023-12-18 22:48:02,265:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:02,266:INFO:Creating metrics dataframe
2023-12-18 22:48:02,272:INFO:Initializing Ada Boost Classifier
2023-12-18 22:48:02,272:INFO:Total runtime is 0.02931906779607137 minutes
2023-12-18 22:48:02,274:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:02,275:INFO:Initializing create_model()
2023-12-18 22:48:02,275:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:02,275:INFO:Checking exceptions
2023-12-18 22:48:02,276:INFO:Importing libraries
2023-12-18 22:48:02,276:INFO:Copying training dataset
2023-12-18 22:48:02,279:INFO:Defining folds
2023-12-18 22:48:02,279:INFO:Declaring metric variables
2023-12-18 22:48:02,281:INFO:Importing untrained model
2023-12-18 22:48:02,283:INFO:Ada Boost Classifier Imported successfully
2023-12-18 22:48:02,287:INFO:Starting cross validation
2023-12-18 22:48:02,288:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:02,492:INFO:Calculating mean and std
2023-12-18 22:48:02,493:INFO:Creating metrics dataframe
2023-12-18 22:48:02,496:INFO:Uploading results into container
2023-12-18 22:48:02,497:INFO:Uploading model into container now
2023-12-18 22:48:02,497:INFO:_master_model_container: 9
2023-12-18 22:48:02,498:INFO:_display_container: 2
2023-12-18 22:48:02,498:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=5096)
2023-12-18 22:48:02,498:INFO:create_model() successfully completed......................................
2023-12-18 22:48:02,563:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:02,564:INFO:Creating metrics dataframe
2023-12-18 22:48:02,570:INFO:Initializing Gradient Boosting Classifier
2023-12-18 22:48:02,571:INFO:Total runtime is 0.03430740435918172 minutes
2023-12-18 22:48:02,573:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:02,573:INFO:Initializing create_model()
2023-12-18 22:48:02,573:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:02,574:INFO:Checking exceptions
2023-12-18 22:48:02,574:INFO:Importing libraries
2023-12-18 22:48:02,574:INFO:Copying training dataset
2023-12-18 22:48:02,577:INFO:Defining folds
2023-12-18 22:48:02,578:INFO:Declaring metric variables
2023-12-18 22:48:02,579:INFO:Importing untrained model
2023-12-18 22:48:02,582:INFO:Gradient Boosting Classifier Imported successfully
2023-12-18 22:48:02,586:INFO:Starting cross validation
2023-12-18 22:48:02,587:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:04,579:INFO:Calculating mean and std
2023-12-18 22:48:04,580:INFO:Creating metrics dataframe
2023-12-18 22:48:04,583:INFO:Uploading results into container
2023-12-18 22:48:04,583:INFO:Uploading model into container now
2023-12-18 22:48:04,584:INFO:_master_model_container: 10
2023-12-18 22:48:04,584:INFO:_display_container: 2
2023-12-18 22:48:04,584:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5096, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-18 22:48:04,585:INFO:create_model() successfully completed......................................
2023-12-18 22:48:04,648:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:04,649:INFO:Creating metrics dataframe
2023-12-18 22:48:04,655:INFO:Initializing Linear Discriminant Analysis
2023-12-18 22:48:04,656:INFO:Total runtime is 0.06905458768208822 minutes
2023-12-18 22:48:04,658:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:04,659:INFO:Initializing create_model()
2023-12-18 22:48:04,659:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:04,659:INFO:Checking exceptions
2023-12-18 22:48:04,660:INFO:Importing libraries
2023-12-18 22:48:04,660:INFO:Copying training dataset
2023-12-18 22:48:04,663:INFO:Defining folds
2023-12-18 22:48:04,663:INFO:Declaring metric variables
2023-12-18 22:48:04,665:INFO:Importing untrained model
2023-12-18 22:48:04,667:INFO:Linear Discriminant Analysis Imported successfully
2023-12-18 22:48:04,672:INFO:Starting cross validation
2023-12-18 22:48:04,673:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:04,703:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:04,703:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:04,707:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:04,719:INFO:Calculating mean and std
2023-12-18 22:48:04,720:INFO:Creating metrics dataframe
2023-12-18 22:48:04,722:INFO:Uploading results into container
2023-12-18 22:48:04,723:INFO:Uploading model into container now
2023-12-18 22:48:04,723:INFO:_master_model_container: 11
2023-12-18 22:48:04,724:INFO:_display_container: 2
2023-12-18 22:48:04,724:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-18 22:48:04,724:INFO:create_model() successfully completed......................................
2023-12-18 22:48:04,787:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:04,788:INFO:Creating metrics dataframe
2023-12-18 22:48:04,794:INFO:Initializing Extra Trees Classifier
2023-12-18 22:48:04,795:INFO:Total runtime is 0.07136422793070475 minutes
2023-12-18 22:48:04,797:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:04,797:INFO:Initializing create_model()
2023-12-18 22:48:04,797:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:04,798:INFO:Checking exceptions
2023-12-18 22:48:04,798:INFO:Importing libraries
2023-12-18 22:48:04,798:INFO:Copying training dataset
2023-12-18 22:48:04,802:INFO:Defining folds
2023-12-18 22:48:04,802:INFO:Declaring metric variables
2023-12-18 22:48:04,804:INFO:Importing untrained model
2023-12-18 22:48:04,807:INFO:Extra Trees Classifier Imported successfully
2023-12-18 22:48:04,811:INFO:Starting cross validation
2023-12-18 22:48:04,812:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:05,046:INFO:Calculating mean and std
2023-12-18 22:48:05,047:INFO:Creating metrics dataframe
2023-12-18 22:48:05,049:INFO:Uploading results into container
2023-12-18 22:48:05,049:INFO:Uploading model into container now
2023-12-18 22:48:05,050:INFO:_master_model_container: 12
2023-12-18 22:48:05,050:INFO:_display_container: 2
2023-12-18 22:48:05,051:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=5096, verbose=0, warm_start=False)
2023-12-18 22:48:05,051:INFO:create_model() successfully completed......................................
2023-12-18 22:48:05,113:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:05,114:INFO:Creating metrics dataframe
2023-12-18 22:48:05,121:INFO:Initializing Extreme Gradient Boosting
2023-12-18 22:48:05,121:INFO:Total runtime is 0.07680660883585612 minutes
2023-12-18 22:48:05,123:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:05,124:INFO:Initializing create_model()
2023-12-18 22:48:05,124:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:05,125:INFO:Checking exceptions
2023-12-18 22:48:05,125:INFO:Importing libraries
2023-12-18 22:48:05,125:INFO:Copying training dataset
2023-12-18 22:48:05,129:INFO:Defining folds
2023-12-18 22:48:05,129:INFO:Declaring metric variables
2023-12-18 22:48:05,131:INFO:Importing untrained model
2023-12-18 22:48:05,134:INFO:Extreme Gradient Boosting Imported successfully
2023-12-18 22:48:05,139:INFO:Starting cross validation
2023-12-18 22:48:05,140:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:05,963:INFO:Calculating mean and std
2023-12-18 22:48:05,965:INFO:Creating metrics dataframe
2023-12-18 22:48:05,969:INFO:Uploading results into container
2023-12-18 22:48:05,969:INFO:Uploading model into container now
2023-12-18 22:48:05,970:INFO:_master_model_container: 13
2023-12-18 22:48:05,970:INFO:_display_container: 2
2023-12-18 22:48:05,971:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-18 22:48:05,971:INFO:create_model() successfully completed......................................
2023-12-18 22:48:06,067:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:06,068:INFO:Creating metrics dataframe
2023-12-18 22:48:06,075:INFO:Initializing Light Gradient Boosting Machine
2023-12-18 22:48:06,075:INFO:Total runtime is 0.09271403153737386 minutes
2023-12-18 22:48:06,078:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:06,079:INFO:Initializing create_model()
2023-12-18 22:48:06,079:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:06,081:INFO:Checking exceptions
2023-12-18 22:48:06,081:INFO:Importing libraries
2023-12-18 22:48:06,081:INFO:Copying training dataset
2023-12-18 22:48:06,085:INFO:Defining folds
2023-12-18 22:48:06,086:INFO:Declaring metric variables
2023-12-18 22:48:06,088:INFO:Importing untrained model
2023-12-18 22:48:06,090:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:48:06,095:INFO:Starting cross validation
2023-12-18 22:48:06,096:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:08,985:INFO:Calculating mean and std
2023-12-18 22:48:08,987:INFO:Creating metrics dataframe
2023-12-18 22:48:08,990:INFO:Uploading results into container
2023-12-18 22:48:08,991:INFO:Uploading model into container now
2023-12-18 22:48:08,992:INFO:_master_model_container: 14
2023-12-18 22:48:08,992:INFO:_display_container: 2
2023-12-18 22:48:08,993:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5096, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:48:08,993:INFO:create_model() successfully completed......................................
2023-12-18 22:48:09,095:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:09,096:INFO:Creating metrics dataframe
2023-12-18 22:48:09,102:INFO:Initializing Dummy Classifier
2023-12-18 22:48:09,103:INFO:Total runtime is 0.14317367871602377 minutes
2023-12-18 22:48:09,105:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:09,105:INFO:Initializing create_model()
2023-12-18 22:48:09,106:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002730B1A4280>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:09,106:INFO:Checking exceptions
2023-12-18 22:48:09,106:INFO:Importing libraries
2023-12-18 22:48:09,107:INFO:Copying training dataset
2023-12-18 22:48:09,110:INFO:Defining folds
2023-12-18 22:48:09,111:INFO:Declaring metric variables
2023-12-18 22:48:09,113:INFO:Importing untrained model
2023-12-18 22:48:09,116:INFO:Dummy Classifier Imported successfully
2023-12-18 22:48:09,119:INFO:Starting cross validation
2023-12-18 22:48:09,120:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:09,138:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:09,138:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:09,141:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:09,157:INFO:Calculating mean and std
2023-12-18 22:48:09,158:INFO:Creating metrics dataframe
2023-12-18 22:48:09,160:INFO:Uploading results into container
2023-12-18 22:48:09,161:INFO:Uploading model into container now
2023-12-18 22:48:09,161:INFO:_master_model_container: 15
2023-12-18 22:48:09,161:INFO:_display_container: 2
2023-12-18 22:48:09,161:INFO:DummyClassifier(constant=None, random_state=5096, strategy='prior')
2023-12-18 22:48:09,162:INFO:create_model() successfully completed......................................
2023-12-18 22:48:09,224:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:09,225:INFO:Creating metrics dataframe
2023-12-18 22:48:09,236:INFO:Initializing create_model()
2023-12-18 22:48:09,237:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:09,237:INFO:Checking exceptions
2023-12-18 22:48:09,238:INFO:Importing libraries
2023-12-18 22:48:09,239:INFO:Copying training dataset
2023-12-18 22:48:09,242:INFO:Defining folds
2023-12-18 22:48:09,242:INFO:Declaring metric variables
2023-12-18 22:48:09,243:INFO:Importing untrained model
2023-12-18 22:48:09,243:INFO:Declaring custom model
2023-12-18 22:48:09,243:INFO:Random Forest Classifier Imported successfully
2023-12-18 22:48:09,244:INFO:Cross validation set to False
2023-12-18 22:48:09,244:INFO:Fitting Model
2023-12-18 22:48:09,392:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False)
2023-12-18 22:48:09,393:INFO:create_model() successfully completed......................................
2023-12-18 22:48:09,458:INFO:Initializing create_model()
2023-12-18 22:48:09,459:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5096, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:09,460:INFO:Checking exceptions
2023-12-18 22:48:09,462:INFO:Importing libraries
2023-12-18 22:48:09,462:INFO:Copying training dataset
2023-12-18 22:48:09,466:INFO:Defining folds
2023-12-18 22:48:09,466:INFO:Declaring metric variables
2023-12-18 22:48:09,467:INFO:Importing untrained model
2023-12-18 22:48:09,467:INFO:Declaring custom model
2023-12-18 22:48:09,468:INFO:Gradient Boosting Classifier Imported successfully
2023-12-18 22:48:09,468:INFO:Cross validation set to False
2023-12-18 22:48:09,469:INFO:Fitting Model
2023-12-18 22:48:11,818:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5096, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-18 22:48:11,818:INFO:create_model() successfully completed......................................
2023-12-18 22:48:11,885:INFO:Initializing create_model()
2023-12-18 22:48:11,886:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5096, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:11,887:INFO:Checking exceptions
2023-12-18 22:48:11,888:INFO:Importing libraries
2023-12-18 22:48:11,888:INFO:Copying training dataset
2023-12-18 22:48:11,891:INFO:Defining folds
2023-12-18 22:48:11,893:INFO:Declaring metric variables
2023-12-18 22:48:11,893:INFO:Importing untrained model
2023-12-18 22:48:11,893:INFO:Declaring custom model
2023-12-18 22:48:11,894:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-18 22:48:11,895:INFO:Cross validation set to False
2023-12-18 22:48:11,895:INFO:Fitting Model
2023-12-18 22:48:11,906:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000727 seconds.
2023-12-18 22:48:11,906:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-18 22:48:11,907:INFO:[LightGBM] [Info] Total Bins 1707
2023-12-18 22:48:11,907:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 30
2023-12-18 22:48:11,908:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-18 22:48:11,908:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-18 22:48:11,909:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-18 22:48:12,195:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5096, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-18 22:48:12,196:INFO:create_model() successfully completed......................................
2023-12-18 22:48:12,304:INFO:Initializing create_model()
2023-12-18 22:48:12,304:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:12,305:INFO:Checking exceptions
2023-12-18 22:48:12,306:INFO:Importing libraries
2023-12-18 22:48:12,307:INFO:Copying training dataset
2023-12-18 22:48:12,311:INFO:Defining folds
2023-12-18 22:48:12,311:INFO:Declaring metric variables
2023-12-18 22:48:12,312:INFO:Importing untrained model
2023-12-18 22:48:12,312:INFO:Declaring custom model
2023-12-18 22:48:12,313:INFO:Extreme Gradient Boosting Imported successfully
2023-12-18 22:48:12,314:INFO:Cross validation set to False
2023-12-18 22:48:12,314:INFO:Fitting Model
2023-12-18 22:48:12,533:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-18 22:48:12,534:INFO:create_model() successfully completed......................................
2023-12-18 22:48:12,643:INFO:Initializing create_model()
2023-12-18 22:48:12,644:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=5096, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:12,644:INFO:Checking exceptions
2023-12-18 22:48:12,646:INFO:Importing libraries
2023-12-18 22:48:12,646:INFO:Copying training dataset
2023-12-18 22:48:12,649:INFO:Defining folds
2023-12-18 22:48:12,650:INFO:Declaring metric variables
2023-12-18 22:48:12,650:INFO:Importing untrained model
2023-12-18 22:48:12,650:INFO:Declaring custom model
2023-12-18 22:48:12,651:INFO:Extra Trees Classifier Imported successfully
2023-12-18 22:48:12,652:INFO:Cross validation set to False
2023-12-18 22:48:12,652:INFO:Fitting Model
2023-12-18 22:48:12,762:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=5096, verbose=0, warm_start=False)
2023-12-18 22:48:12,763:INFO:create_model() successfully completed......................................
2023-12-18 22:48:12,840:INFO:_master_model_container: 15
2023-12-18 22:48:12,841:INFO:_display_container: 2
2023-12-18 22:48:12,842:INFO:[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=5096, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=5096, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=5096, verbose=0, warm_start=False)]
2023-12-18 22:48:12,842:INFO:compare_models() successfully completed......................................
2023-12-18 22:48:12,885:INFO:Initializing tune_model()
2023-12-18 22:48:12,886:INFO:tune_model(estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>)
2023-12-18 22:48:12,886:INFO:Checking exceptions
2023-12-18 22:48:12,897:INFO:Copying training dataset
2023-12-18 22:48:12,900:INFO:Checking base model
2023-12-18 22:48:12,901:INFO:Base model : Random Forest Classifier
2023-12-18 22:48:12,903:INFO:Declaring metric variables
2023-12-18 22:48:12,906:INFO:Defining Hyperparameters
2023-12-18 22:48:12,972:INFO:Tuning with n_jobs=-1
2023-12-18 22:48:12,973:INFO:Initializing RandomizedSearchCV
2023-12-18 22:48:14,737:INFO:best_params: {'actual_estimator__n_estimators': 130, 'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 6, 'actual_estimator__min_impurity_decrease': 0.005, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 7, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': True}
2023-12-18 22:48:14,739:INFO:Hyperparameter search completed
2023-12-18 22:48:14,739:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:14,740:INFO:Initializing create_model()
2023-12-18 22:48:14,741:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000027352ED7A90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 130, 'min_samples_split': 7, 'min_samples_leaf': 6, 'min_impurity_decrease': 0.005, 'max_features': 1.0, 'max_depth': 7, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': True})
2023-12-18 22:48:14,741:INFO:Checking exceptions
2023-12-18 22:48:14,742:INFO:Importing libraries
2023-12-18 22:48:14,742:INFO:Copying training dataset
2023-12-18 22:48:14,746:INFO:Defining folds
2023-12-18 22:48:14,746:INFO:Declaring metric variables
2023-12-18 22:48:14,748:INFO:Importing untrained model
2023-12-18 22:48:14,749:INFO:Declaring custom model
2023-12-18 22:48:14,751:INFO:Random Forest Classifier Imported successfully
2023-12-18 22:48:14,754:INFO:Starting cross validation
2023-12-18 22:48:14,755:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:15,147:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:15,163:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:15,164:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:15,165:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-18 22:48:15,173:INFO:Calculating mean and std
2023-12-18 22:48:15,174:INFO:Creating metrics dataframe
2023-12-18 22:48:15,177:INFO:Finalizing model
2023-12-18 22:48:15,381:INFO:Uploading results into container
2023-12-18 22:48:15,382:INFO:Uploading model into container now
2023-12-18 22:48:15,383:INFO:_master_model_container: 16
2023-12-18 22:48:15,383:INFO:_display_container: 3
2023-12-18 22:48:15,383:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=7, max_features=1.0,
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.005, min_samples_leaf=6,
                       min_samples_split=7, min_weight_fraction_leaf=0.0,
                       n_estimators=130, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False)
2023-12-18 22:48:15,384:INFO:create_model() successfully completed......................................
2023-12-18 22:48:15,445:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:15,446:INFO:choose_better activated
2023-12-18 22:48:15,449:INFO:SubProcess create_model() called ==================================
2023-12-18 22:48:15,449:INFO:Initializing create_model()
2023-12-18 22:48:15,449:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-18 22:48:15,450:INFO:Checking exceptions
2023-12-18 22:48:15,451:INFO:Importing libraries
2023-12-18 22:48:15,451:INFO:Copying training dataset
2023-12-18 22:48:15,455:INFO:Defining folds
2023-12-18 22:48:15,455:INFO:Declaring metric variables
2023-12-18 22:48:15,456:INFO:Importing untrained model
2023-12-18 22:48:15,456:INFO:Declaring custom model
2023-12-18 22:48:15,456:INFO:Random Forest Classifier Imported successfully
2023-12-18 22:48:15,457:INFO:Starting cross validation
2023-12-18 22:48:15,458:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-18 22:48:15,733:INFO:Calculating mean and std
2023-12-18 22:48:15,733:INFO:Creating metrics dataframe
2023-12-18 22:48:15,735:INFO:Finalizing model
2023-12-18 22:48:15,890:INFO:Uploading results into container
2023-12-18 22:48:15,891:INFO:Uploading model into container now
2023-12-18 22:48:15,892:INFO:_master_model_container: 17
2023-12-18 22:48:15,892:INFO:_display_container: 4
2023-12-18 22:48:15,893:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False)
2023-12-18 22:48:15,893:INFO:create_model() successfully completed......................................
2023-12-18 22:48:15,955:INFO:SubProcess create_model() end ==================================
2023-12-18 22:48:15,956:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False) result for Accuracy is 0.8291
2023-12-18 22:48:15,957:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=7, max_features=1.0,
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.005, min_samples_leaf=6,
                       min_samples_split=7, min_weight_fraction_leaf=0.0,
                       n_estimators=130, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False) result for Accuracy is 0.8004
2023-12-18 22:48:15,957:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False) is best model
2023-12-18 22:48:15,958:INFO:choose_better completed
2023-12-18 22:48:15,958:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-12-18 22:48:15,962:INFO:_master_model_container: 17
2023-12-18 22:48:15,964:INFO:_display_container: 3
2023-12-18 22:48:15,964:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False)
2023-12-18 22:48:15,965:INFO:tune_model() successfully completed......................................
2023-12-18 22:48:16,077:INFO:Initializing evaluate_model()
2023-12-18 22:48:16,077:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-18 22:48:16,086:INFO:Initializing plot_model()
2023-12-18 22:48:16,087:INFO:plot_model(plot=pipeline, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, system=True)
2023-12-18 22:48:16,087:INFO:Checking exceptions
2023-12-18 22:48:16,109:INFO:Preloading libraries
2023-12-18 22:48:16,130:INFO:Copying training dataset
2023-12-18 22:48:16,131:INFO:Plot type: pipeline
2023-12-18 22:48:16,173:INFO:Visual Rendered Successfully
2023-12-18 22:48:16,239:INFO:plot_model() successfully completed......................................
2023-12-18 22:48:16,270:INFO:Initializing predict_model()
2023-12-18 22:48:16,271:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000002731FAD1E10>)
2023-12-18 22:48:16,271:INFO:Checking exceptions
2023-12-18 22:48:16,271:INFO:Preloading libraries
2023-12-18 22:48:16,273:INFO:Set up data.
2023-12-18 22:48:16,277:INFO:Set up index.
2023-12-18 23:19:55,149:INFO:Initializing plot_model()
2023-12-18 23:19:55,150:INFO:plot_model(plot=confusion_matrix, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, system=True)
2023-12-18 23:19:55,151:INFO:Checking exceptions
2023-12-18 23:19:55,183:INFO:Preloading libraries
2023-12-18 23:19:55,197:INFO:Copying training dataset
2023-12-18 23:19:55,198:INFO:Plot type: confusion_matrix
2023-12-18 23:19:55,246:INFO:Fitting Model
2023-12-18 23:19:55,247:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\base.py:420: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2023-12-18 23:19:55,248:INFO:Scoring test/hold-out set
2023-12-18 23:19:55,365:INFO:Visual Rendered Successfully
2023-12-18 23:19:55,439:INFO:plot_model() successfully completed......................................
2023-12-18 23:19:55,887:INFO:Initializing plot_model()
2023-12-18 23:19:55,888:INFO:plot_model(plot=auc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, system=True)
2023-12-18 23:19:55,888:INFO:Checking exceptions
2023-12-18 23:19:55,913:INFO:Preloading libraries
2023-12-18 23:19:55,926:INFO:Copying training dataset
2023-12-18 23:19:55,927:INFO:Plot type: auc
2023-12-18 23:19:55,971:INFO:Fitting Model
2023-12-18 23:19:55,972:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\base.py:420: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2023-12-18 23:19:55,973:INFO:Scoring test/hold-out set
2023-12-18 23:19:56,128:INFO:Visual Rendered Successfully
2023-12-18 23:19:56,194:INFO:plot_model() successfully completed......................................
2023-12-18 23:19:56,969:INFO:Initializing plot_model()
2023-12-18 23:19:56,970:INFO:plot_model(plot=parameter, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, system=True)
2023-12-18 23:19:56,970:INFO:Checking exceptions
2023-12-18 23:19:56,991:INFO:Preloading libraries
2023-12-18 23:19:57,004:INFO:Copying training dataset
2023-12-18 23:19:57,005:INFO:Plot type: parameter
2023-12-18 23:19:57,007:INFO:Visual Rendered Successfully
2023-12-18 23:19:57,074:INFO:plot_model() successfully completed......................................
2023-12-18 23:19:57,380:INFO:Initializing plot_model()
2023-12-18 23:19:57,381:INFO:plot_model(plot=auc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=5096, verbose=0, warm_start=False), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.classification.oop.ClassificationExperiment object at 0x000002730F046B30>, system=True)
2023-12-18 23:19:57,381:INFO:Checking exceptions
2023-12-18 23:19:57,410:INFO:Preloading libraries
2023-12-18 23:19:57,425:INFO:Copying training dataset
2023-12-18 23:19:57,426:INFO:Plot type: auc
2023-12-18 23:19:57,472:INFO:Fitting Model
2023-12-18 23:19:57,472:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\base.py:420: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names
  warnings.warn(

2023-12-18 23:19:57,473:INFO:Scoring test/hold-out set
2023-12-18 23:19:57,619:INFO:Visual Rendered Successfully
2023-12-18 23:19:57,684:INFO:plot_model() successfully completed......................................
2023-12-19 16:35:39,301:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-19 16:35:39,518:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-19 16:35:39,518:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-19 16:35:39,518:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-19 16:40:11,085:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-19 16:40:11,086:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-19 16:40:11,086:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-19 16:40:11,086:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-19 16:49:31,685:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,686:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,687:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,688:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,688:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,689:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,689:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,690:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,691:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,691:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:49:31,696:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,696:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,697:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,698:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,698:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,699:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,699:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,700:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,700:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:31,701:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:49:34,629:INFO:PyCaret ClassificationExperiment
2023-12-19 16:49:34,629:INFO:Logging name: clf-default-name
2023-12-19 16:49:34,629:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-19 16:49:34,629:INFO:version 3.2.0
2023-12-19 16:49:34,629:INFO:Initializing setup()
2023-12-19 16:49:34,629:INFO:self.USI: d223
2023-12-19 16:49:34,629:INFO:self._variable_keys: {'y_train', 'log_plots_param', 'data', 'fix_imbalance', 'exp_name_log', 'memory', 'fold_shuffle_param', 'y_test', 'is_multiclass', 'idx', 'gpu_param', 'seed', 'pipeline', 'X_train', '_ml_usecase', 'html_param', 'logging_param', 'target_param', 'gpu_n_jobs_param', 'n_jobs_param', '_available_plots', 'X_test', 'fold_groups_param', 'fold_generator', 'USI', 'y', 'X', 'exp_id'}
2023-12-19 16:49:34,629:INFO:Checking environment
2023-12-19 16:49:34,629:INFO:python_version: 3.11.5
2023-12-19 16:49:34,629:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-19 16:49:34,629:INFO:machine: AMD64
2023-12-19 16:49:34,629:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-19 16:49:34,629:INFO:Memory: svmem(total=16718413824, available=6253629440, percent=62.6, used=10464784384, free=6253629440)
2023-12-19 16:49:34,629:INFO:Physical Core: 12
2023-12-19 16:49:34,629:INFO:Logical Core: 16
2023-12-19 16:49:34,629:INFO:Checking libraries
2023-12-19 16:49:34,629:INFO:System:
2023-12-19 16:49:34,629:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-19 16:49:34,629:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-19 16:49:34,629:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-19 16:49:34,629:INFO:PyCaret required dependencies:
2023-12-19 16:49:35,299:INFO:                 pip: 23.2.1
2023-12-19 16:49:35,299:INFO:          setuptools: 68.0.0
2023-12-19 16:49:35,300:INFO:             pycaret: 3.2.0
2023-12-19 16:49:35,300:INFO:             IPython: 8.15.0
2023-12-19 16:49:35,300:INFO:          ipywidgets: 8.0.4
2023-12-19 16:49:35,300:INFO:                tqdm: 4.65.0
2023-12-19 16:49:35,300:INFO:               numpy: 1.24.3
2023-12-19 16:49:35,300:INFO:              pandas: 1.5.3
2023-12-19 16:49:35,300:INFO:              jinja2: 3.1.2
2023-12-19 16:49:35,300:INFO:               scipy: 1.10.1
2023-12-19 16:49:35,300:INFO:              joblib: 1.2.0
2023-12-19 16:49:35,300:INFO:             sklearn: 1.2.1
2023-12-19 16:49:35,300:INFO:                pyod: 1.1.2
2023-12-19 16:49:35,300:INFO:            imblearn: 0.11.0
2023-12-19 16:49:35,300:INFO:   category_encoders: 2.6.3
2023-12-19 16:49:35,300:INFO:            lightgbm: 4.1.0
2023-12-19 16:49:35,300:INFO:               numba: 0.57.1
2023-12-19 16:49:35,300:INFO:            requests: 2.31.0
2023-12-19 16:49:35,300:INFO:          matplotlib: 3.6.0
2023-12-19 16:49:35,300:INFO:          scikitplot: 0.3.7
2023-12-19 16:49:35,300:INFO:         yellowbrick: 1.5
2023-12-19 16:49:35,300:INFO:              plotly: 5.9.0
2023-12-19 16:49:35,300:INFO:    plotly-resampler: Not installed
2023-12-19 16:49:35,300:INFO:             kaleido: 0.2.1
2023-12-19 16:49:35,300:INFO:           schemdraw: 0.15
2023-12-19 16:49:35,300:INFO:         statsmodels: 0.14.0
2023-12-19 16:49:35,300:INFO:              sktime: 0.21.1
2023-12-19 16:49:35,300:INFO:               tbats: 1.1.3
2023-12-19 16:49:35,300:INFO:            pmdarima: 2.0.4
2023-12-19 16:49:35,300:INFO:              psutil: 5.9.0
2023-12-19 16:49:35,300:INFO:          markupsafe: 2.1.1
2023-12-19 16:49:35,300:INFO:             pickle5: Not installed
2023-12-19 16:49:35,300:INFO:         cloudpickle: 2.2.1
2023-12-19 16:49:35,300:INFO:         deprecation: 2.1.0
2023-12-19 16:49:35,300:INFO:              xxhash: 2.0.2
2023-12-19 16:49:35,300:INFO:           wurlitzer: Not installed
2023-12-19 16:49:35,300:INFO:PyCaret optional dependencies:
2023-12-19 16:49:35,368:INFO:                shap: Not installed
2023-12-19 16:49:35,368:INFO:           interpret: Not installed
2023-12-19 16:49:35,368:INFO:                umap: Not installed
2023-12-19 16:49:35,368:INFO:     ydata_profiling: Not installed
2023-12-19 16:49:35,368:INFO:  explainerdashboard: Not installed
2023-12-19 16:49:35,368:INFO:             autoviz: Not installed
2023-12-19 16:49:35,369:INFO:           fairlearn: Not installed
2023-12-19 16:49:35,369:INFO:          deepchecks: Not installed
2023-12-19 16:49:35,369:INFO:             xgboost: 2.0.2
2023-12-19 16:49:35,369:INFO:            catboost: Not installed
2023-12-19 16:49:35,369:INFO:              kmodes: Not installed
2023-12-19 16:49:35,369:INFO:             mlxtend: Not installed
2023-12-19 16:49:35,369:INFO:       statsforecast: Not installed
2023-12-19 16:49:35,369:INFO:        tune_sklearn: Not installed
2023-12-19 16:49:35,369:INFO:                 ray: Not installed
2023-12-19 16:49:35,369:INFO:            hyperopt: Not installed
2023-12-19 16:49:35,369:INFO:              optuna: Not installed
2023-12-19 16:49:35,369:INFO:               skopt: Not installed
2023-12-19 16:49:35,369:INFO:              mlflow: Not installed
2023-12-19 16:49:35,369:INFO:              gradio: Not installed
2023-12-19 16:49:35,369:INFO:             fastapi: Not installed
2023-12-19 16:49:35,369:INFO:             uvicorn: Not installed
2023-12-19 16:49:35,369:INFO:              m2cgen: Not installed
2023-12-19 16:49:35,369:INFO:           evidently: Not installed
2023-12-19 16:49:35,369:INFO:               fugue: Not installed
2023-12-19 16:49:35,369:INFO:           streamlit: Not installed
2023-12-19 16:49:35,369:INFO:             prophet: Not installed
2023-12-19 16:49:35,369:INFO:None
2023-12-19 16:49:35,369:INFO:Set up data.
2023-12-19 16:49:35,378:INFO:Set up folding strategy.
2023-12-19 16:49:35,378:INFO:Set up train/test split.
2023-12-19 16:49:35,384:INFO:Set up index.
2023-12-19 16:49:35,384:INFO:Assigning column types.
2023-12-19 16:49:35,386:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-19 16:49:35,412:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-19 16:49:35,414:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-19 16:49:35,436:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:49:35,437:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:49:35,462:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-19 16:49:35,463:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-19 16:49:35,479:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:49:35,481:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:49:35,481:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-19 16:49:35,508:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-19 16:49:35,526:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:49:35,528:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:49:35,553:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-19 16:49:35,569:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:49:35,571:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:49:35,571:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-19 16:49:35,612:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:49:35,614:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:49:35,666:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:49:35,667:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:49:35,670:INFO:Preparing preprocessing pipeline...
2023-12-19 16:49:35,671:INFO:Set up simple imputation.
2023-12-19 16:49:35,672:INFO:Set up column name cleaning.
2023-12-19 16:49:35,695:INFO:Finished creating preprocessing pipeline.
2023-12-19 16:49:35,700:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'Hepatomegaly_N', 'Hepatomegaly_Y',
                                             'Spiders_N', 'Spiders_Y',
                                             'Edema_N', 'Edema_S', 'Edema_Y',
                                             'Age_cat_2', 'Age_cat...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-19 16:49:35,700:INFO:Creating final display dataframe.
2023-12-19 16:49:35,762:INFO:Setup _display_container:                     Description             Value
0                    Session id              8806
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 42)
4        Transformed data shape        (7905, 42)
5   Transformed train set shape        (6719, 42)
6    Transformed test set shape        (1186, 42)
7              Numeric features                41
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              d223
2023-12-19 16:49:35,809:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:49:35,811:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:49:35,852:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:49:35,853:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:49:35,854:INFO:setup() successfully completed in 1.37s...............
2023-12-19 16:49:40,038:INFO:Initializing compare_models()
2023-12-19 16:49:40,039:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-19 16:49:40,039:INFO:Checking exceptions
2023-12-19 16:49:40,042:INFO:Preparing display monitor
2023-12-19 16:49:40,062:INFO:Initializing Logistic Regression
2023-12-19 16:49:40,062:INFO:Total runtime is 0.0 minutes
2023-12-19 16:49:40,066:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:40,066:INFO:Initializing create_model()
2023-12-19 16:49:40,066:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:40,067:INFO:Checking exceptions
2023-12-19 16:49:40,067:INFO:Importing libraries
2023-12-19 16:49:40,067:INFO:Copying training dataset
2023-12-19 16:49:40,071:INFO:Defining folds
2023-12-19 16:49:40,071:INFO:Declaring metric variables
2023-12-19 16:49:40,074:INFO:Importing untrained model
2023-12-19 16:49:40,076:INFO:Logistic Regression Imported successfully
2023-12-19 16:49:40,083:INFO:Starting cross validation
2023-12-19 16:49:40,084:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:43,809:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:43,848:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:43,874:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:43,898:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:43,945:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:43,949:INFO:Calculating mean and std
2023-12-19 16:49:43,950:INFO:Creating metrics dataframe
2023-12-19 16:49:43,952:INFO:Uploading results into container
2023-12-19 16:49:43,953:INFO:Uploading model into container now
2023-12-19 16:49:43,953:INFO:_master_model_container: 1
2023-12-19 16:49:43,953:INFO:_display_container: 2
2023-12-19 16:49:43,954:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8806, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-19 16:49:43,954:INFO:create_model() successfully completed......................................
2023-12-19 16:49:44,036:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:44,036:INFO:Creating metrics dataframe
2023-12-19 16:49:44,046:INFO:Initializing K Neighbors Classifier
2023-12-19 16:49:44,046:INFO:Total runtime is 0.06639645099639893 minutes
2023-12-19 16:49:44,048:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:44,048:INFO:Initializing create_model()
2023-12-19 16:49:44,048:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:44,048:INFO:Checking exceptions
2023-12-19 16:49:44,048:INFO:Importing libraries
2023-12-19 16:49:44,049:INFO:Copying training dataset
2023-12-19 16:49:44,055:INFO:Defining folds
2023-12-19 16:49:44,055:INFO:Declaring metric variables
2023-12-19 16:49:44,058:INFO:Importing untrained model
2023-12-19 16:49:44,060:INFO:K Neighbors Classifier Imported successfully
2023-12-19 16:49:44,065:INFO:Starting cross validation
2023-12-19 16:49:44,065:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:47,012:INFO:Calculating mean and std
2023-12-19 16:49:47,014:INFO:Creating metrics dataframe
2023-12-19 16:49:47,016:INFO:Uploading results into container
2023-12-19 16:49:47,017:INFO:Uploading model into container now
2023-12-19 16:49:47,018:INFO:_master_model_container: 2
2023-12-19 16:49:47,018:INFO:_display_container: 2
2023-12-19 16:49:47,018:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-19 16:49:47,018:INFO:create_model() successfully completed......................................
2023-12-19 16:49:47,089:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:47,089:INFO:Creating metrics dataframe
2023-12-19 16:49:47,099:INFO:Initializing Naive Bayes
2023-12-19 16:49:47,099:INFO:Total runtime is 0.11727702617645264 minutes
2023-12-19 16:49:47,102:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:47,102:INFO:Initializing create_model()
2023-12-19 16:49:47,102:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:47,102:INFO:Checking exceptions
2023-12-19 16:49:47,103:INFO:Importing libraries
2023-12-19 16:49:47,103:INFO:Copying training dataset
2023-12-19 16:49:47,108:INFO:Defining folds
2023-12-19 16:49:47,108:INFO:Declaring metric variables
2023-12-19 16:49:47,111:INFO:Importing untrained model
2023-12-19 16:49:47,114:INFO:Naive Bayes Imported successfully
2023-12-19 16:49:47,120:INFO:Starting cross validation
2023-12-19 16:49:47,121:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:49,905:INFO:Calculating mean and std
2023-12-19 16:49:49,906:INFO:Creating metrics dataframe
2023-12-19 16:49:49,908:INFO:Uploading results into container
2023-12-19 16:49:49,909:INFO:Uploading model into container now
2023-12-19 16:49:49,909:INFO:_master_model_container: 3
2023-12-19 16:49:49,909:INFO:_display_container: 2
2023-12-19 16:49:49,909:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-19 16:49:49,910:INFO:create_model() successfully completed......................................
2023-12-19 16:49:49,976:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:49,977:INFO:Creating metrics dataframe
2023-12-19 16:49:49,987:INFO:Initializing Decision Tree Classifier
2023-12-19 16:49:49,987:INFO:Total runtime is 0.16541630824406942 minutes
2023-12-19 16:49:49,992:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:49,992:INFO:Initializing create_model()
2023-12-19 16:49:49,992:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:49,992:INFO:Checking exceptions
2023-12-19 16:49:49,992:INFO:Importing libraries
2023-12-19 16:49:49,992:INFO:Copying training dataset
2023-12-19 16:49:49,996:INFO:Defining folds
2023-12-19 16:49:49,996:INFO:Declaring metric variables
2023-12-19 16:49:49,998:INFO:Importing untrained model
2023-12-19 16:49:50,000:INFO:Decision Tree Classifier Imported successfully
2023-12-19 16:49:50,004:INFO:Starting cross validation
2023-12-19 16:49:50,005:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:52,276:INFO:Calculating mean and std
2023-12-19 16:49:52,277:INFO:Creating metrics dataframe
2023-12-19 16:49:52,279:INFO:Uploading results into container
2023-12-19 16:49:52,280:INFO:Uploading model into container now
2023-12-19 16:49:52,280:INFO:_master_model_container: 4
2023-12-19 16:49:52,280:INFO:_display_container: 2
2023-12-19 16:49:52,280:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8806, splitter='best')
2023-12-19 16:49:52,281:INFO:create_model() successfully completed......................................
2023-12-19 16:49:52,354:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:52,354:INFO:Creating metrics dataframe
2023-12-19 16:49:52,363:INFO:Initializing SVM - Linear Kernel
2023-12-19 16:49:52,363:INFO:Total runtime is 0.20501543680826823 minutes
2023-12-19 16:49:52,366:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:52,367:INFO:Initializing create_model()
2023-12-19 16:49:52,367:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:52,367:INFO:Checking exceptions
2023-12-19 16:49:52,367:INFO:Importing libraries
2023-12-19 16:49:52,367:INFO:Copying training dataset
2023-12-19 16:49:52,373:INFO:Defining folds
2023-12-19 16:49:52,373:INFO:Declaring metric variables
2023-12-19 16:49:52,376:INFO:Importing untrained model
2023-12-19 16:49:52,378:INFO:SVM - Linear Kernel Imported successfully
2023-12-19 16:49:52,383:INFO:Starting cross validation
2023-12-19 16:49:52,384:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:52,530:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:49:52,534:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,536:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:49:52,539:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,544:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:49:52,546:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,547:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:49:52,550:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,551:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:49:52,553:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,557:INFO:Calculating mean and std
2023-12-19 16:49:52,557:INFO:Creating metrics dataframe
2023-12-19 16:49:52,559:INFO:Uploading results into container
2023-12-19 16:49:52,560:INFO:Uploading model into container now
2023-12-19 16:49:52,560:INFO:_master_model_container: 5
2023-12-19 16:49:52,560:INFO:_display_container: 2
2023-12-19 16:49:52,561:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8806, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-19 16:49:52,561:INFO:create_model() successfully completed......................................
2023-12-19 16:49:52,625:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:52,626:INFO:Creating metrics dataframe
2023-12-19 16:49:52,632:INFO:Initializing Ridge Classifier
2023-12-19 16:49:52,632:INFO:Total runtime is 0.2094938278198242 minutes
2023-12-19 16:49:52,634:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:52,635:INFO:Initializing create_model()
2023-12-19 16:49:52,635:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:52,635:INFO:Checking exceptions
2023-12-19 16:49:52,635:INFO:Importing libraries
2023-12-19 16:49:52,635:INFO:Copying training dataset
2023-12-19 16:49:52,640:INFO:Defining folds
2023-12-19 16:49:52,640:INFO:Declaring metric variables
2023-12-19 16:49:52,642:INFO:Importing untrained model
2023-12-19 16:49:52,644:INFO:Ridge Classifier Imported successfully
2023-12-19 16:49:52,647:INFO:Starting cross validation
2023-12-19 16:49:52,648:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:52,692:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:49:52,694:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:49:52,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,696:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:49:52,697:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,698:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:49:52,699:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,701:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,705:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:49:52,708:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:52,711:INFO:Calculating mean and std
2023-12-19 16:49:52,711:INFO:Creating metrics dataframe
2023-12-19 16:49:52,713:INFO:Uploading results into container
2023-12-19 16:49:52,714:INFO:Uploading model into container now
2023-12-19 16:49:52,714:INFO:_master_model_container: 6
2023-12-19 16:49:52,714:INFO:_display_container: 2
2023-12-19 16:49:52,714:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=8806, solver='auto',
                tol=0.0001)
2023-12-19 16:49:52,714:INFO:create_model() successfully completed......................................
2023-12-19 16:49:52,779:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:52,779:INFO:Creating metrics dataframe
2023-12-19 16:49:52,786:INFO:Initializing Random Forest Classifier
2023-12-19 16:49:52,786:INFO:Total runtime is 0.21205713748931884 minutes
2023-12-19 16:49:52,788:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:52,789:INFO:Initializing create_model()
2023-12-19 16:49:52,789:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:52,789:INFO:Checking exceptions
2023-12-19 16:49:52,789:INFO:Importing libraries
2023-12-19 16:49:52,789:INFO:Copying training dataset
2023-12-19 16:49:52,797:INFO:Defining folds
2023-12-19 16:49:52,797:INFO:Declaring metric variables
2023-12-19 16:49:52,801:INFO:Importing untrained model
2023-12-19 16:49:52,806:INFO:Random Forest Classifier Imported successfully
2023-12-19 16:49:52,812:INFO:Starting cross validation
2023-12-19 16:49:52,813:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:53,210:INFO:Calculating mean and std
2023-12-19 16:49:53,211:INFO:Creating metrics dataframe
2023-12-19 16:49:53,213:INFO:Uploading results into container
2023-12-19 16:49:53,214:INFO:Uploading model into container now
2023-12-19 16:49:53,214:INFO:_master_model_container: 7
2023-12-19 16:49:53,214:INFO:_display_container: 2
2023-12-19 16:49:53,214:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8806, verbose=0, warm_start=False)
2023-12-19 16:49:53,214:INFO:create_model() successfully completed......................................
2023-12-19 16:49:53,285:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:53,286:INFO:Creating metrics dataframe
2023-12-19 16:49:53,294:INFO:Initializing Quadratic Discriminant Analysis
2023-12-19 16:49:53,294:INFO:Total runtime is 0.22052400906880695 minutes
2023-12-19 16:49:53,297:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:53,297:INFO:Initializing create_model()
2023-12-19 16:49:53,297:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:53,297:INFO:Checking exceptions
2023-12-19 16:49:53,297:INFO:Importing libraries
2023-12-19 16:49:53,298:INFO:Copying training dataset
2023-12-19 16:49:53,302:INFO:Defining folds
2023-12-19 16:49:53,302:INFO:Declaring metric variables
2023-12-19 16:49:53,304:INFO:Importing untrained model
2023-12-19 16:49:53,306:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-19 16:49:53,310:INFO:Starting cross validation
2023-12-19 16:49:53,310:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:53,370:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:49:53,371:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:49:53,371:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:49:53,372:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:49:53,372:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:49:53,396:INFO:Calculating mean and std
2023-12-19 16:49:53,397:INFO:Creating metrics dataframe
2023-12-19 16:49:53,399:INFO:Uploading results into container
2023-12-19 16:49:53,399:INFO:Uploading model into container now
2023-12-19 16:49:53,400:INFO:_master_model_container: 8
2023-12-19 16:49:53,400:INFO:_display_container: 2
2023-12-19 16:49:53,400:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-19 16:49:53,400:INFO:create_model() successfully completed......................................
2023-12-19 16:49:53,468:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:53,468:INFO:Creating metrics dataframe
2023-12-19 16:49:53,477:INFO:Initializing Ada Boost Classifier
2023-12-19 16:49:53,477:INFO:Total runtime is 0.22358093261718748 minutes
2023-12-19 16:49:53,479:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:53,479:INFO:Initializing create_model()
2023-12-19 16:49:53,479:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:53,479:INFO:Checking exceptions
2023-12-19 16:49:53,479:INFO:Importing libraries
2023-12-19 16:49:53,479:INFO:Copying training dataset
2023-12-19 16:49:53,483:INFO:Defining folds
2023-12-19 16:49:53,483:INFO:Declaring metric variables
2023-12-19 16:49:53,485:INFO:Importing untrained model
2023-12-19 16:49:53,487:INFO:Ada Boost Classifier Imported successfully
2023-12-19 16:49:53,491:INFO:Starting cross validation
2023-12-19 16:49:53,493:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:53,763:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:53,770:INFO:Calculating mean and std
2023-12-19 16:49:53,771:INFO:Creating metrics dataframe
2023-12-19 16:49:53,773:INFO:Uploading results into container
2023-12-19 16:49:53,773:INFO:Uploading model into container now
2023-12-19 16:49:53,773:INFO:_master_model_container: 9
2023-12-19 16:49:53,773:INFO:_display_container: 2
2023-12-19 16:49:53,774:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8806)
2023-12-19 16:49:53,774:INFO:create_model() successfully completed......................................
2023-12-19 16:49:53,838:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:53,838:INFO:Creating metrics dataframe
2023-12-19 16:49:53,846:INFO:Initializing Gradient Boosting Classifier
2023-12-19 16:49:53,846:INFO:Total runtime is 0.22973137696584064 minutes
2023-12-19 16:49:53,848:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:53,848:INFO:Initializing create_model()
2023-12-19 16:49:53,848:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:53,848:INFO:Checking exceptions
2023-12-19 16:49:53,848:INFO:Importing libraries
2023-12-19 16:49:53,848:INFO:Copying training dataset
2023-12-19 16:49:53,852:INFO:Defining folds
2023-12-19 16:49:53,852:INFO:Declaring metric variables
2023-12-19 16:49:53,854:INFO:Importing untrained model
2023-12-19 16:49:53,857:INFO:Gradient Boosting Classifier Imported successfully
2023-12-19 16:49:53,861:INFO:Starting cross validation
2023-12-19 16:49:53,862:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:55,642:INFO:Calculating mean and std
2023-12-19 16:49:55,643:INFO:Creating metrics dataframe
2023-12-19 16:49:55,645:INFO:Uploading results into container
2023-12-19 16:49:55,645:INFO:Uploading model into container now
2023-12-19 16:49:55,646:INFO:_master_model_container: 10
2023-12-19 16:49:55,646:INFO:_display_container: 2
2023-12-19 16:49:55,646:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8806, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-19 16:49:55,646:INFO:create_model() successfully completed......................................
2023-12-19 16:49:55,714:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:55,714:INFO:Creating metrics dataframe
2023-12-19 16:49:55,723:INFO:Initializing Linear Discriminant Analysis
2023-12-19 16:49:55,724:INFO:Total runtime is 0.26101553042729697 minutes
2023-12-19 16:49:55,726:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:55,726:INFO:Initializing create_model()
2023-12-19 16:49:55,726:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:55,726:INFO:Checking exceptions
2023-12-19 16:49:55,726:INFO:Importing libraries
2023-12-19 16:49:55,726:INFO:Copying training dataset
2023-12-19 16:49:55,730:INFO:Defining folds
2023-12-19 16:49:55,730:INFO:Declaring metric variables
2023-12-19 16:49:55,733:INFO:Importing untrained model
2023-12-19 16:49:55,735:INFO:Linear Discriminant Analysis Imported successfully
2023-12-19 16:49:55,739:INFO:Starting cross validation
2023-12-19 16:49:55,740:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:55,810:INFO:Calculating mean and std
2023-12-19 16:49:55,811:INFO:Creating metrics dataframe
2023-12-19 16:49:55,813:INFO:Uploading results into container
2023-12-19 16:49:55,813:INFO:Uploading model into container now
2023-12-19 16:49:55,814:INFO:_master_model_container: 11
2023-12-19 16:49:55,814:INFO:_display_container: 2
2023-12-19 16:49:55,814:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-19 16:49:55,814:INFO:create_model() successfully completed......................................
2023-12-19 16:49:55,880:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:55,881:INFO:Creating metrics dataframe
2023-12-19 16:49:55,893:INFO:Initializing Extra Trees Classifier
2023-12-19 16:49:55,894:INFO:Total runtime is 0.26386049191157024 minutes
2023-12-19 16:49:55,897:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:55,897:INFO:Initializing create_model()
2023-12-19 16:49:55,897:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:55,897:INFO:Checking exceptions
2023-12-19 16:49:55,897:INFO:Importing libraries
2023-12-19 16:49:55,897:INFO:Copying training dataset
2023-12-19 16:49:55,901:INFO:Defining folds
2023-12-19 16:49:55,901:INFO:Declaring metric variables
2023-12-19 16:49:55,904:INFO:Importing untrained model
2023-12-19 16:49:55,906:INFO:Extra Trees Classifier Imported successfully
2023-12-19 16:49:55,909:INFO:Starting cross validation
2023-12-19 16:49:55,910:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:56,330:INFO:Calculating mean and std
2023-12-19 16:49:56,331:INFO:Creating metrics dataframe
2023-12-19 16:49:56,333:INFO:Uploading results into container
2023-12-19 16:49:56,334:INFO:Uploading model into container now
2023-12-19 16:49:56,334:INFO:_master_model_container: 12
2023-12-19 16:49:56,334:INFO:_display_container: 2
2023-12-19 16:49:56,335:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8806, verbose=0, warm_start=False)
2023-12-19 16:49:56,335:INFO:create_model() successfully completed......................................
2023-12-19 16:49:56,407:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:56,407:INFO:Creating metrics dataframe
2023-12-19 16:49:56,417:INFO:Initializing Extreme Gradient Boosting
2023-12-19 16:49:56,417:INFO:Total runtime is 0.27257609367370605 minutes
2023-12-19 16:49:56,419:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:56,419:INFO:Initializing create_model()
2023-12-19 16:49:56,420:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:56,420:INFO:Checking exceptions
2023-12-19 16:49:56,420:INFO:Importing libraries
2023-12-19 16:49:56,420:INFO:Copying training dataset
2023-12-19 16:49:56,425:INFO:Defining folds
2023-12-19 16:49:56,425:INFO:Declaring metric variables
2023-12-19 16:49:56,427:INFO:Importing untrained model
2023-12-19 16:49:56,430:INFO:Extreme Gradient Boosting Imported successfully
2023-12-19 16:49:56,434:INFO:Starting cross validation
2023-12-19 16:49:56,435:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:56,806:INFO:Calculating mean and std
2023-12-19 16:49:56,806:INFO:Creating metrics dataframe
2023-12-19 16:49:56,808:INFO:Uploading results into container
2023-12-19 16:49:56,809:INFO:Uploading model into container now
2023-12-19 16:49:56,809:INFO:_master_model_container: 13
2023-12-19 16:49:56,809:INFO:_display_container: 2
2023-12-19 16:49:56,810:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-19 16:49:56,810:INFO:create_model() successfully completed......................................
2023-12-19 16:49:56,876:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:56,877:INFO:Creating metrics dataframe
2023-12-19 16:49:56,889:INFO:Initializing Light Gradient Boosting Machine
2023-12-19 16:49:56,890:INFO:Total runtime is 0.2804721633593241 minutes
2023-12-19 16:49:56,892:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:56,893:INFO:Initializing create_model()
2023-12-19 16:49:56,893:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:56,893:INFO:Checking exceptions
2023-12-19 16:49:56,893:INFO:Importing libraries
2023-12-19 16:49:56,893:INFO:Copying training dataset
2023-12-19 16:49:56,897:INFO:Defining folds
2023-12-19 16:49:56,898:INFO:Declaring metric variables
2023-12-19 16:49:56,901:INFO:Importing untrained model
2023-12-19 16:49:56,903:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-19 16:49:56,907:INFO:Starting cross validation
2023-12-19 16:49:56,909:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:59,605:INFO:Calculating mean and std
2023-12-19 16:49:59,607:INFO:Creating metrics dataframe
2023-12-19 16:49:59,609:INFO:Uploading results into container
2023-12-19 16:49:59,610:INFO:Uploading model into container now
2023-12-19 16:49:59,610:INFO:_master_model_container: 14
2023-12-19 16:49:59,610:INFO:_display_container: 2
2023-12-19 16:49:59,611:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:49:59,611:INFO:create_model() successfully completed......................................
2023-12-19 16:49:59,693:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:59,693:INFO:Creating metrics dataframe
2023-12-19 16:49:59,701:INFO:Initializing Dummy Classifier
2023-12-19 16:49:59,701:INFO:Total runtime is 0.3273186246554057 minutes
2023-12-19 16:49:59,703:INFO:SubProcess create_model() called ==================================
2023-12-19 16:49:59,703:INFO:Initializing create_model()
2023-12-19 16:49:59,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF26AD50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:59,704:INFO:Checking exceptions
2023-12-19 16:49:59,704:INFO:Importing libraries
2023-12-19 16:49:59,704:INFO:Copying training dataset
2023-12-19 16:49:59,708:INFO:Defining folds
2023-12-19 16:49:59,708:INFO:Declaring metric variables
2023-12-19 16:49:59,710:INFO:Importing untrained model
2023-12-19 16:49:59,712:INFO:Dummy Classifier Imported successfully
2023-12-19 16:49:59,716:INFO:Starting cross validation
2023-12-19 16:49:59,717:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:49:59,755:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:59,766:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:59,767:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:59,768:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:59,768:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:49:59,774:INFO:Calculating mean and std
2023-12-19 16:49:59,775:INFO:Creating metrics dataframe
2023-12-19 16:49:59,778:INFO:Uploading results into container
2023-12-19 16:49:59,779:INFO:Uploading model into container now
2023-12-19 16:49:59,779:INFO:_master_model_container: 15
2023-12-19 16:49:59,779:INFO:_display_container: 2
2023-12-19 16:49:59,780:INFO:DummyClassifier(constant=None, random_state=8806, strategy='prior')
2023-12-19 16:49:59,780:INFO:create_model() successfully completed......................................
2023-12-19 16:49:59,848:INFO:SubProcess create_model() end ==================================
2023-12-19 16:49:59,848:INFO:Creating metrics dataframe
2023-12-19 16:49:59,876:INFO:Initializing create_model()
2023-12-19 16:49:59,876:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:49:59,876:INFO:Checking exceptions
2023-12-19 16:49:59,877:INFO:Importing libraries
2023-12-19 16:49:59,877:INFO:Copying training dataset
2023-12-19 16:49:59,881:INFO:Defining folds
2023-12-19 16:49:59,881:INFO:Declaring metric variables
2023-12-19 16:49:59,881:INFO:Importing untrained model
2023-12-19 16:49:59,881:INFO:Declaring custom model
2023-12-19 16:49:59,881:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-19 16:49:59,881:INFO:Cross validation set to False
2023-12-19 16:49:59,881:INFO:Fitting Model
2023-12-19 16:49:59,920:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.
2023-12-19 16:49:59,920:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-19 16:49:59,920:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-19 16:49:59,920:INFO:[LightGBM] [Info] Total Bins 317
2023-12-19 16:49:59,921:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 38
2023-12-19 16:49:59,921:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-19 16:49:59,921:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-19 16:49:59,922:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-19 16:50:00,117:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:50:00,117:INFO:create_model() successfully completed......................................
2023-12-19 16:50:00,201:INFO:Initializing create_model()
2023-12-19 16:50:00,201:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8806, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:50:00,201:INFO:Checking exceptions
2023-12-19 16:50:00,203:INFO:Importing libraries
2023-12-19 16:50:00,203:INFO:Copying training dataset
2023-12-19 16:50:00,210:INFO:Defining folds
2023-12-19 16:50:00,210:INFO:Declaring metric variables
2023-12-19 16:50:00,210:INFO:Importing untrained model
2023-12-19 16:50:00,210:INFO:Declaring custom model
2023-12-19 16:50:00,210:INFO:Gradient Boosting Classifier Imported successfully
2023-12-19 16:50:00,211:INFO:Cross validation set to False
2023-12-19 16:50:00,211:INFO:Fitting Model
2023-12-19 16:50:02,022:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8806, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-19 16:50:02,023:INFO:create_model() successfully completed......................................
2023-12-19 16:50:02,108:INFO:Initializing create_model()
2023-12-19 16:50:02,109:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:50:02,109:INFO:Checking exceptions
2023-12-19 16:50:02,110:INFO:Importing libraries
2023-12-19 16:50:02,110:INFO:Copying training dataset
2023-12-19 16:50:02,116:INFO:Defining folds
2023-12-19 16:50:02,116:INFO:Declaring metric variables
2023-12-19 16:50:02,116:INFO:Importing untrained model
2023-12-19 16:50:02,116:INFO:Declaring custom model
2023-12-19 16:50:02,118:INFO:Extreme Gradient Boosting Imported successfully
2023-12-19 16:50:02,118:INFO:Cross validation set to False
2023-12-19 16:50:02,118:INFO:Fitting Model
2023-12-19 16:50:02,330:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-19 16:50:02,330:INFO:create_model() successfully completed......................................
2023-12-19 16:50:02,413:INFO:Initializing create_model()
2023-12-19 16:50:02,413:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8806), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:50:02,413:INFO:Checking exceptions
2023-12-19 16:50:02,415:INFO:Importing libraries
2023-12-19 16:50:02,415:INFO:Copying training dataset
2023-12-19 16:50:02,421:INFO:Defining folds
2023-12-19 16:50:02,421:INFO:Declaring metric variables
2023-12-19 16:50:02,422:INFO:Importing untrained model
2023-12-19 16:50:02,422:INFO:Declaring custom model
2023-12-19 16:50:02,422:INFO:Ada Boost Classifier Imported successfully
2023-12-19 16:50:02,423:INFO:Cross validation set to False
2023-12-19 16:50:02,423:INFO:Fitting Model
2023-12-19 16:50:02,657:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8806)
2023-12-19 16:50:02,657:INFO:create_model() successfully completed......................................
2023-12-19 16:50:02,759:INFO:Initializing create_model()
2023-12-19 16:50:02,759:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8806, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:50:02,759:INFO:Checking exceptions
2023-12-19 16:50:02,761:INFO:Importing libraries
2023-12-19 16:50:02,761:INFO:Copying training dataset
2023-12-19 16:50:02,762:INFO:Defining folds
2023-12-19 16:50:02,762:INFO:Declaring metric variables
2023-12-19 16:50:02,762:INFO:Importing untrained model
2023-12-19 16:50:02,762:INFO:Declaring custom model
2023-12-19 16:50:02,762:INFO:Logistic Regression Imported successfully
2023-12-19 16:50:02,762:INFO:Cross validation set to False
2023-12-19 16:50:02,762:INFO:Fitting Model
2023-12-19 16:50:03,132:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8806, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-19 16:50:03,132:INFO:create_model() successfully completed......................................
2023-12-19 16:50:03,225:INFO:_master_model_container: 15
2023-12-19 16:50:03,225:INFO:_display_container: 2
2023-12-19 16:50:03,227:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8806, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8806), LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8806, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)]
2023-12-19 16:50:03,227:INFO:compare_models() successfully completed......................................
2023-12-19 16:50:11,278:INFO:Initializing tune_model()
2023-12-19 16:50:11,278:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-19 16:50:11,278:INFO:Checking exceptions
2023-12-19 16:50:11,293:INFO:Copying training dataset
2023-12-19 16:50:11,297:INFO:Checking base model
2023-12-19 16:50:11,298:INFO:Base model : Light Gradient Boosting Machine
2023-12-19 16:50:11,300:INFO:Declaring metric variables
2023-12-19 16:50:11,303:INFO:Defining Hyperparameters
2023-12-19 16:50:11,385:INFO:Tuning with n_jobs=-1
2023-12-19 16:50:11,385:INFO:Initializing RandomizedSearchCV
2023-12-19 16:50:46,649:INFO:best_params: {'actual_estimator__reg_lambda': 4, 'actual_estimator__reg_alpha': 0.15, 'actual_estimator__num_leaves': 150, 'actual_estimator__n_estimators': 230, 'actual_estimator__min_split_gain': 0.4, 'actual_estimator__min_child_samples': 71, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.8, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.7}
2023-12-19 16:50:46,650:INFO:Hyperparameter search completed
2023-12-19 16:50:46,651:INFO:SubProcess create_model() called ==================================
2023-12-19 16:50:46,651:INFO:Initializing create_model()
2023-12-19 16:50:46,652:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DB9C28D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 4, 'reg_alpha': 0.15, 'num_leaves': 150, 'n_estimators': 230, 'min_split_gain': 0.4, 'min_child_samples': 71, 'learning_rate': 0.1, 'feature_fraction': 0.8, 'bagging_freq': 2, 'bagging_fraction': 0.7})
2023-12-19 16:50:46,652:INFO:Checking exceptions
2023-12-19 16:50:46,652:INFO:Importing libraries
2023-12-19 16:50:46,652:INFO:Copying training dataset
2023-12-19 16:50:46,664:INFO:Defining folds
2023-12-19 16:50:46,664:INFO:Declaring metric variables
2023-12-19 16:50:46,665:INFO:Importing untrained model
2023-12-19 16:50:46,665:INFO:Declaring custom model
2023-12-19 16:50:46,672:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-19 16:50:46,679:INFO:Starting cross validation
2023-12-19 16:50:46,680:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:50:49,822:INFO:Calculating mean and std
2023-12-19 16:50:49,824:INFO:Creating metrics dataframe
2023-12-19 16:50:49,829:INFO:Finalizing model
2023-12-19 16:50:49,855:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2023-12-19 16:50:49,855:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-19 16:50:49,855:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-12-19 16:50:49,863:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2023-12-19 16:50:49,863:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-19 16:50:49,863:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-12-19 16:50:49,863:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001723 seconds.
2023-12-19 16:50:49,863:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-19 16:50:49,863:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-19 16:50:49,863:INFO:[LightGBM] [Info] Total Bins 315
2023-12-19 16:50:49,863:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 37
2023-12-19 16:50:49,870:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-19 16:50:49,871:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-19 16:50:49,871:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-19 16:50:49,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,909:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:49,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,060:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,062:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,093:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,110:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,121:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,126:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,148:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,160:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,174:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,198:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,204:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,208:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,210:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,214:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,216:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,220:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,224:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,231:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,234:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,237:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,239:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,242:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,245:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,257:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,259:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,260:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,262:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,264:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,271:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,272:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,273:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,275:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,276:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,280:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,281:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,291:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,292:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,295:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,296:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,306:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,308:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,310:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,313:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,322:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,324:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,329:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,331:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,336:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,338:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,340:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,345:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,370:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,377:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,388:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,391:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,393:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,395:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,410:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,411:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,422:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,426:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,427:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,429:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,436:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,439:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,441:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,444:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,445:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,471:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,472:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,474:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,476:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,478:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,486:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,487:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,490:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,492:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,494:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,506:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,507:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,509:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,510:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,513:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,520:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,529:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,536:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,537:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,544:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,560:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,561:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,563:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,569:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,572:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,576:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,578:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,586:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,594:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,596:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,603:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,606:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,609:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,610:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,613:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,622:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,627:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,629:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,638:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,643:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,646:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,653:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,659:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,662:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,670:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,673:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,679:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,680:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,690:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,693:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,696:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,698:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,706:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,714:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,718:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,722:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,723:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,724:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,728:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,731:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,733:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,737:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,740:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,742:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,744:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,748:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,750:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,751:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,756:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,761:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,762:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:50:50,827:INFO:Uploading results into container
2023-12-19 16:50:50,828:INFO:Uploading model into container now
2023-12-19 16:50:50,829:INFO:_master_model_container: 16
2023-12-19 16:50:50,829:INFO:_display_container: 3
2023-12-19 16:50:50,830:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=71, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=230, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8806, reg_alpha=0.15, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:50:50,830:INFO:create_model() successfully completed......................................
2023-12-19 16:50:50,940:INFO:SubProcess create_model() end ==================================
2023-12-19 16:50:50,940:INFO:choose_better activated
2023-12-19 16:50:50,943:INFO:SubProcess create_model() called ==================================
2023-12-19 16:50:50,943:INFO:Initializing create_model()
2023-12-19 16:50:50,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:50:50,943:INFO:Checking exceptions
2023-12-19 16:50:50,945:INFO:Importing libraries
2023-12-19 16:50:50,945:INFO:Copying training dataset
2023-12-19 16:50:50,950:INFO:Defining folds
2023-12-19 16:50:50,950:INFO:Declaring metric variables
2023-12-19 16:50:50,950:INFO:Importing untrained model
2023-12-19 16:50:50,950:INFO:Declaring custom model
2023-12-19 16:50:50,950:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-19 16:50:50,951:INFO:Starting cross validation
2023-12-19 16:50:50,951:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:50:53,864:INFO:Calculating mean and std
2023-12-19 16:50:53,864:INFO:Creating metrics dataframe
2023-12-19 16:50:53,867:INFO:Finalizing model
2023-12-19 16:50:53,901:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002110 seconds.
2023-12-19 16:50:53,902:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-19 16:50:53,902:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-19 16:50:53,902:INFO:[LightGBM] [Info] Total Bins 317
2023-12-19 16:50:53,902:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 38
2023-12-19 16:50:53,903:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-19 16:50:53,903:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-19 16:50:53,903:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-19 16:50:54,371:INFO:Uploading results into container
2023-12-19 16:50:54,372:INFO:Uploading model into container now
2023-12-19 16:50:54,373:INFO:_master_model_container: 17
2023-12-19 16:50:54,373:INFO:_display_container: 4
2023-12-19 16:50:54,373:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:50:54,374:INFO:create_model() successfully completed......................................
2023-12-19 16:50:54,489:INFO:SubProcess create_model() end ==================================
2023-12-19 16:50:54,489:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8806, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8079
2023-12-19 16:50:54,490:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=71, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=230, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8806, reg_alpha=0.15, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.812
2023-12-19 16:50:54,490:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=71, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=230, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8806, reg_alpha=0.15, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-19 16:50:54,490:INFO:choose_better completed
2023-12-19 16:50:54,495:INFO:_master_model_container: 17
2023-12-19 16:50:54,495:INFO:_display_container: 3
2023-12-19 16:50:54,495:INFO:LGBMClassifier(bagging_fraction=0.7, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=71, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=230, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8806, reg_alpha=0.15, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:50:54,496:INFO:tune_model() successfully completed......................................
2023-12-19 16:51:02,830:INFO:Initializing evaluate_model()
2023-12-19 16:51:02,830:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=71, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=230, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8806, reg_alpha=0.15, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-19 16:51:02,845:INFO:Initializing plot_model()
2023-12-19 16:51:02,845:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=71, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=230, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8806, reg_alpha=0.15, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-19 16:51:02,846:INFO:Checking exceptions
2023-12-19 16:51:02,849:INFO:Preloading libraries
2023-12-19 16:51:02,899:INFO:Copying training dataset
2023-12-19 16:51:02,899:INFO:Plot type: pipeline
2023-12-19 16:51:03,039:INFO:Visual Rendered Successfully
2023-12-19 16:51:03,113:INFO:plot_model() successfully completed......................................
2023-12-19 16:51:05,178:INFO:Initializing plot_model()
2023-12-19 16:51:05,178:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC063950>, estimator=LGBMClassifier(bagging_fraction=0.7, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.8,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=71, min_child_weight=0.001, min_split_gain=0.4,
               n_estimators=230, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8806, reg_alpha=0.15, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-19 16:51:05,178:INFO:Checking exceptions
2023-12-19 16:51:05,178:INFO:Preloading libraries
2023-12-19 16:51:05,262:INFO:Copying training dataset
2023-12-19 16:51:05,262:INFO:Plot type: auc
2023-12-19 16:51:05,378:INFO:Fitting Model
2023-12-19 16:51:05,378:INFO:Scoring test/hold-out set
2023-12-19 16:51:05,378:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2023-12-19 16:51:05,378:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-19 16:51:05,378:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-12-19 16:51:05,386:INFO:[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8
2023-12-19 16:51:05,386:INFO:[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7
2023-12-19 16:51:05,386:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-12-19 16:51:05,545:INFO:Visual Rendered Successfully
2023-12-19 16:51:05,622:INFO:plot_model() successfully completed......................................
2023-12-19 16:52:32,349:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,350:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,350:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,350:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,351:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,352:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,352:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,354:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,354:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,355:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-19 16:52:32,356:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,356:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,360:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,360:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,360:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,361:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,361:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,361:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,361:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,361:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22612\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-19 16:52:32,511:INFO:PyCaret ClassificationExperiment
2023-12-19 16:52:32,511:INFO:Logging name: clf-default-name
2023-12-19 16:52:32,511:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-19 16:52:32,511:INFO:version 3.2.0
2023-12-19 16:52:32,511:INFO:Initializing setup()
2023-12-19 16:52:32,511:INFO:self.USI: ad80
2023-12-19 16:52:32,511:INFO:self._variable_keys: {'y_train', 'log_plots_param', 'data', 'fix_imbalance', 'exp_name_log', 'memory', 'fold_shuffle_param', 'y_test', 'is_multiclass', 'idx', 'gpu_param', 'seed', 'pipeline', 'X_train', '_ml_usecase', 'html_param', 'logging_param', 'target_param', 'gpu_n_jobs_param', 'n_jobs_param', '_available_plots', 'X_test', 'fold_groups_param', 'fold_generator', 'USI', 'y', 'X', 'exp_id'}
2023-12-19 16:52:32,511:INFO:Checking environment
2023-12-19 16:52:32,511:INFO:python_version: 3.11.5
2023-12-19 16:52:32,511:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-19 16:52:32,511:INFO:machine: AMD64
2023-12-19 16:52:32,511:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-19 16:52:32,511:INFO:Memory: svmem(total=16718413824, available=3279642624, percent=80.4, used=13438771200, free=3279642624)
2023-12-19 16:52:32,511:INFO:Physical Core: 12
2023-12-19 16:52:32,511:INFO:Logical Core: 16
2023-12-19 16:52:32,511:INFO:Checking libraries
2023-12-19 16:52:32,526:INFO:System:
2023-12-19 16:52:32,526:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-19 16:52:32,526:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-19 16:52:32,526:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-19 16:52:32,526:INFO:PyCaret required dependencies:
2023-12-19 16:52:32,526:INFO:                 pip: 23.2.1
2023-12-19 16:52:32,526:INFO:          setuptools: 68.0.0
2023-12-19 16:52:32,527:INFO:             pycaret: 3.2.0
2023-12-19 16:52:32,527:INFO:             IPython: 8.15.0
2023-12-19 16:52:32,527:INFO:          ipywidgets: 8.0.4
2023-12-19 16:52:32,527:INFO:                tqdm: 4.65.0
2023-12-19 16:52:32,527:INFO:               numpy: 1.24.3
2023-12-19 16:52:32,527:INFO:              pandas: 1.5.3
2023-12-19 16:52:32,527:INFO:              jinja2: 3.1.2
2023-12-19 16:52:32,527:INFO:               scipy: 1.10.1
2023-12-19 16:52:32,528:INFO:              joblib: 1.2.0
2023-12-19 16:52:32,528:INFO:             sklearn: 1.2.1
2023-12-19 16:52:32,528:INFO:                pyod: 1.1.2
2023-12-19 16:52:32,528:INFO:            imblearn: 0.11.0
2023-12-19 16:52:32,528:INFO:   category_encoders: 2.6.3
2023-12-19 16:52:32,528:INFO:            lightgbm: 4.1.0
2023-12-19 16:52:32,528:INFO:               numba: 0.57.1
2023-12-19 16:52:32,528:INFO:            requests: 2.31.0
2023-12-19 16:52:32,528:INFO:          matplotlib: 3.6.0
2023-12-19 16:52:32,528:INFO:          scikitplot: 0.3.7
2023-12-19 16:52:32,528:INFO:         yellowbrick: 1.5
2023-12-19 16:52:32,528:INFO:              plotly: 5.9.0
2023-12-19 16:52:32,528:INFO:    plotly-resampler: Not installed
2023-12-19 16:52:32,528:INFO:             kaleido: 0.2.1
2023-12-19 16:52:32,528:INFO:           schemdraw: 0.15
2023-12-19 16:52:32,528:INFO:         statsmodels: 0.14.0
2023-12-19 16:52:32,528:INFO:              sktime: 0.21.1
2023-12-19 16:52:32,528:INFO:               tbats: 1.1.3
2023-12-19 16:52:32,528:INFO:            pmdarima: 2.0.4
2023-12-19 16:52:32,528:INFO:              psutil: 5.9.0
2023-12-19 16:52:32,528:INFO:          markupsafe: 2.1.1
2023-12-19 16:52:32,528:INFO:             pickle5: Not installed
2023-12-19 16:52:32,528:INFO:         cloudpickle: 2.2.1
2023-12-19 16:52:32,528:INFO:         deprecation: 2.1.0
2023-12-19 16:52:32,528:INFO:              xxhash: 2.0.2
2023-12-19 16:52:32,528:INFO:           wurlitzer: Not installed
2023-12-19 16:52:32,528:INFO:PyCaret optional dependencies:
2023-12-19 16:52:32,528:INFO:                shap: Not installed
2023-12-19 16:52:32,528:INFO:           interpret: Not installed
2023-12-19 16:52:32,528:INFO:                umap: Not installed
2023-12-19 16:52:32,528:INFO:     ydata_profiling: Not installed
2023-12-19 16:52:32,528:INFO:  explainerdashboard: Not installed
2023-12-19 16:52:32,528:INFO:             autoviz: Not installed
2023-12-19 16:52:32,528:INFO:           fairlearn: Not installed
2023-12-19 16:52:32,528:INFO:          deepchecks: Not installed
2023-12-19 16:52:32,528:INFO:             xgboost: 2.0.2
2023-12-19 16:52:32,528:INFO:            catboost: Not installed
2023-12-19 16:52:32,528:INFO:              kmodes: Not installed
2023-12-19 16:52:32,528:INFO:             mlxtend: Not installed
2023-12-19 16:52:32,528:INFO:       statsforecast: Not installed
2023-12-19 16:52:32,528:INFO:        tune_sklearn: Not installed
2023-12-19 16:52:32,528:INFO:                 ray: Not installed
2023-12-19 16:52:32,528:INFO:            hyperopt: Not installed
2023-12-19 16:52:32,528:INFO:              optuna: Not installed
2023-12-19 16:52:32,528:INFO:               skopt: Not installed
2023-12-19 16:52:32,528:INFO:              mlflow: Not installed
2023-12-19 16:52:32,528:INFO:              gradio: Not installed
2023-12-19 16:52:32,528:INFO:             fastapi: Not installed
2023-12-19 16:52:32,528:INFO:             uvicorn: Not installed
2023-12-19 16:52:32,528:INFO:              m2cgen: Not installed
2023-12-19 16:52:32,528:INFO:           evidently: Not installed
2023-12-19 16:52:32,528:INFO:               fugue: Not installed
2023-12-19 16:52:32,528:INFO:           streamlit: Not installed
2023-12-19 16:52:32,528:INFO:             prophet: Not installed
2023-12-19 16:52:32,528:INFO:None
2023-12-19 16:52:32,528:INFO:Set up data.
2023-12-19 16:52:32,544:INFO:Set up folding strategy.
2023-12-19 16:52:32,544:INFO:Set up train/test split.
2023-12-19 16:52:32,544:INFO:Set up index.
2023-12-19 16:52:32,544:INFO:Assigning column types.
2023-12-19 16:52:32,544:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-19 16:52:32,578:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-19 16:52:32,578:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-19 16:52:32,594:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:52:32,594:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:52:32,611:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-19 16:52:32,611:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-19 16:52:32,640:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:52:32,642:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:52:32,642:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-19 16:52:32,668:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-19 16:52:32,677:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:52:32,677:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:52:32,711:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-19 16:52:32,727:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:52:32,729:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:52:32,729:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-19 16:52:32,761:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:52:32,761:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:52:32,811:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:52:32,811:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:52:32,811:INFO:Preparing preprocessing pipeline...
2023-12-19 16:52:32,811:INFO:Set up simple imputation.
2023-12-19 16:52:32,811:INFO:Set up column name cleaning.
2023-12-19 16:52:32,828:INFO:Finished creating preprocessing pipeline.
2023-12-19 16:52:32,844:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'He...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-19 16:52:32,844:INFO:Creating final display dataframe.
2023-12-19 16:52:32,910:INFO:Setup _display_container:                     Description             Value
0                    Session id              7326
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 51)
4        Transformed data shape        (7905, 51)
5   Transformed train set shape        (6719, 51)
6    Transformed test set shape        (1186, 51)
7              Numeric features                50
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              ad80
2023-12-19 16:52:32,945:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:52:32,945:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:52:32,994:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-19 16:52:32,994:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-19 16:52:32,994:INFO:setup() successfully completed in 0.6s...............
2023-12-19 16:52:33,013:INFO:Initializing compare_models()
2023-12-19 16:52:33,013:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-19 16:52:33,013:INFO:Checking exceptions
2023-12-19 16:52:33,019:INFO:Preparing display monitor
2023-12-19 16:52:33,034:INFO:Initializing Logistic Regression
2023-12-19 16:52:33,034:INFO:Total runtime is 0.0 minutes
2023-12-19 16:52:33,034:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:33,034:INFO:Initializing create_model()
2023-12-19 16:52:33,034:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:33,034:INFO:Checking exceptions
2023-12-19 16:52:33,034:INFO:Importing libraries
2023-12-19 16:52:33,034:INFO:Copying training dataset
2023-12-19 16:52:33,034:INFO:Defining folds
2023-12-19 16:52:33,034:INFO:Declaring metric variables
2023-12-19 16:52:33,045:INFO:Importing untrained model
2023-12-19 16:52:33,047:INFO:Logistic Regression Imported successfully
2023-12-19 16:52:33,052:INFO:Starting cross validation
2023-12-19 16:52:33,053:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:33,629:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-19 16:52:33,632:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-19 16:52:33,632:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:33,647:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:33,647:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-19 16:52:33,647:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:33,663:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-19 16:52:33,727:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-19 16:52:33,743:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:33,743:INFO:Calculating mean and std
2023-12-19 16:52:33,743:INFO:Creating metrics dataframe
2023-12-19 16:52:33,743:INFO:Uploading results into container
2023-12-19 16:52:33,743:INFO:Uploading model into container now
2023-12-19 16:52:33,743:INFO:_master_model_container: 1
2023-12-19 16:52:33,743:INFO:_display_container: 2
2023-12-19 16:52:33,743:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=7326, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-19 16:52:33,743:INFO:create_model() successfully completed......................................
2023-12-19 16:52:33,821:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:33,837:INFO:Creating metrics dataframe
2023-12-19 16:52:33,844:INFO:Initializing K Neighbors Classifier
2023-12-19 16:52:33,844:INFO:Total runtime is 0.013507763544718424 minutes
2023-12-19 16:52:33,844:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:33,844:INFO:Initializing create_model()
2023-12-19 16:52:33,844:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:33,844:INFO:Checking exceptions
2023-12-19 16:52:33,844:INFO:Importing libraries
2023-12-19 16:52:33,844:INFO:Copying training dataset
2023-12-19 16:52:33,860:INFO:Defining folds
2023-12-19 16:52:33,860:INFO:Declaring metric variables
2023-12-19 16:52:33,860:INFO:Importing untrained model
2023-12-19 16:52:33,860:INFO:K Neighbors Classifier Imported successfully
2023-12-19 16:52:33,860:INFO:Starting cross validation
2023-12-19 16:52:33,860:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:34,112:INFO:Calculating mean and std
2023-12-19 16:52:34,112:INFO:Creating metrics dataframe
2023-12-19 16:52:34,112:INFO:Uploading results into container
2023-12-19 16:52:34,112:INFO:Uploading model into container now
2023-12-19 16:52:34,112:INFO:_master_model_container: 2
2023-12-19 16:52:34,112:INFO:_display_container: 2
2023-12-19 16:52:34,112:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-19 16:52:34,112:INFO:create_model() successfully completed......................................
2023-12-19 16:52:34,230:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:34,231:INFO:Creating metrics dataframe
2023-12-19 16:52:34,234:INFO:Initializing Naive Bayes
2023-12-19 16:52:34,234:INFO:Total runtime is 0.02000452677408854 minutes
2023-12-19 16:52:34,234:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:34,234:INFO:Initializing create_model()
2023-12-19 16:52:34,234:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:34,234:INFO:Checking exceptions
2023-12-19 16:52:34,234:INFO:Importing libraries
2023-12-19 16:52:34,234:INFO:Copying training dataset
2023-12-19 16:52:34,247:INFO:Defining folds
2023-12-19 16:52:34,247:INFO:Declaring metric variables
2023-12-19 16:52:34,249:INFO:Importing untrained model
2023-12-19 16:52:34,249:INFO:Naive Bayes Imported successfully
2023-12-19 16:52:34,249:INFO:Starting cross validation
2023-12-19 16:52:34,249:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:34,312:INFO:Calculating mean and std
2023-12-19 16:52:34,312:INFO:Creating metrics dataframe
2023-12-19 16:52:34,312:INFO:Uploading results into container
2023-12-19 16:52:34,312:INFO:Uploading model into container now
2023-12-19 16:52:34,312:INFO:_master_model_container: 3
2023-12-19 16:52:34,312:INFO:_display_container: 2
2023-12-19 16:52:34,312:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-19 16:52:34,312:INFO:create_model() successfully completed......................................
2023-12-19 16:52:34,378:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:34,378:INFO:Creating metrics dataframe
2023-12-19 16:52:34,393:INFO:Initializing Decision Tree Classifier
2023-12-19 16:52:34,393:INFO:Total runtime is 0.022662075360616048 minutes
2023-12-19 16:52:34,393:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:34,393:INFO:Initializing create_model()
2023-12-19 16:52:34,393:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:34,393:INFO:Checking exceptions
2023-12-19 16:52:34,393:INFO:Importing libraries
2023-12-19 16:52:34,393:INFO:Copying training dataset
2023-12-19 16:52:34,393:INFO:Defining folds
2023-12-19 16:52:34,393:INFO:Declaring metric variables
2023-12-19 16:52:34,393:INFO:Importing untrained model
2023-12-19 16:52:34,409:INFO:Decision Tree Classifier Imported successfully
2023-12-19 16:52:34,412:INFO:Starting cross validation
2023-12-19 16:52:34,412:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:34,500:INFO:Calculating mean and std
2023-12-19 16:52:34,500:INFO:Creating metrics dataframe
2023-12-19 16:52:34,511:INFO:Uploading results into container
2023-12-19 16:52:34,511:INFO:Uploading model into container now
2023-12-19 16:52:34,512:INFO:_master_model_container: 4
2023-12-19 16:52:34,512:INFO:_display_container: 2
2023-12-19 16:52:34,512:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=7326, splitter='best')
2023-12-19 16:52:34,512:INFO:create_model() successfully completed......................................
2023-12-19 16:52:34,593:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:34,594:INFO:Creating metrics dataframe
2023-12-19 16:52:34,600:INFO:Initializing SVM - Linear Kernel
2023-12-19 16:52:34,600:INFO:Total runtime is 0.02610531250635783 minutes
2023-12-19 16:52:34,602:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:34,602:INFO:Initializing create_model()
2023-12-19 16:52:34,602:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:34,602:INFO:Checking exceptions
2023-12-19 16:52:34,602:INFO:Importing libraries
2023-12-19 16:52:34,602:INFO:Copying training dataset
2023-12-19 16:52:34,602:INFO:Defining folds
2023-12-19 16:52:34,602:INFO:Declaring metric variables
2023-12-19 16:52:34,602:INFO:Importing untrained model
2023-12-19 16:52:34,612:INFO:SVM - Linear Kernel Imported successfully
2023-12-19 16:52:34,612:INFO:Starting cross validation
2023-12-19 16:52:34,612:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:34,732:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:52:34,733:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:52:34,734:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:34,735:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:34,736:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:52:34,736:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:52:34,751:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-19 16:52:34,751:INFO:Calculating mean and std
2023-12-19 16:52:34,751:INFO:Creating metrics dataframe
2023-12-19 16:52:34,760:INFO:Uploading results into container
2023-12-19 16:52:34,760:INFO:Uploading model into container now
2023-12-19 16:52:34,761:INFO:_master_model_container: 5
2023-12-19 16:52:34,761:INFO:_display_container: 2
2023-12-19 16:52:34,761:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=7326, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-19 16:52:34,761:INFO:create_model() successfully completed......................................
2023-12-19 16:52:34,827:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:34,827:INFO:Creating metrics dataframe
2023-12-19 16:52:34,844:INFO:Initializing Ridge Classifier
2023-12-19 16:52:34,844:INFO:Total runtime is 0.030169717470804852 minutes
2023-12-19 16:52:34,844:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:34,844:INFO:Initializing create_model()
2023-12-19 16:52:34,844:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:34,844:INFO:Checking exceptions
2023-12-19 16:52:34,844:INFO:Importing libraries
2023-12-19 16:52:34,844:INFO:Copying training dataset
2023-12-19 16:52:34,844:INFO:Defining folds
2023-12-19 16:52:34,844:INFO:Declaring metric variables
2023-12-19 16:52:34,844:INFO:Importing untrained model
2023-12-19 16:52:34,844:INFO:Ridge Classifier Imported successfully
2023-12-19 16:52:34,860:INFO:Starting cross validation
2023-12-19 16:52:34,861:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:34,894:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:34,909:INFO:Calculating mean and std
2023-12-19 16:52:34,910:INFO:Creating metrics dataframe
2023-12-19 16:52:34,910:INFO:Uploading results into container
2023-12-19 16:52:34,910:INFO:Uploading model into container now
2023-12-19 16:52:34,910:INFO:_master_model_container: 6
2023-12-19 16:52:34,910:INFO:_display_container: 2
2023-12-19 16:52:34,913:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=7326, solver='auto',
                tol=0.0001)
2023-12-19 16:52:34,913:INFO:create_model() successfully completed......................................
2023-12-19 16:52:34,980:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:34,980:INFO:Creating metrics dataframe
2023-12-19 16:52:34,986:INFO:Initializing Random Forest Classifier
2023-12-19 16:52:34,986:INFO:Total runtime is 0.032531356811523436 minutes
2023-12-19 16:52:34,986:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:34,986:INFO:Initializing create_model()
2023-12-19 16:52:34,986:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:34,986:INFO:Checking exceptions
2023-12-19 16:52:34,986:INFO:Importing libraries
2023-12-19 16:52:34,986:INFO:Copying training dataset
2023-12-19 16:52:34,994:INFO:Defining folds
2023-12-19 16:52:34,994:INFO:Declaring metric variables
2023-12-19 16:52:34,994:INFO:Importing untrained model
2023-12-19 16:52:34,994:INFO:Random Forest Classifier Imported successfully
2023-12-19 16:52:34,994:INFO:Starting cross validation
2023-12-19 16:52:34,994:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:35,527:INFO:Calculating mean and std
2023-12-19 16:52:35,527:INFO:Creating metrics dataframe
2023-12-19 16:52:35,527:INFO:Uploading results into container
2023-12-19 16:52:35,527:INFO:Uploading model into container now
2023-12-19 16:52:35,527:INFO:_master_model_container: 7
2023-12-19 16:52:35,527:INFO:_display_container: 2
2023-12-19 16:52:35,527:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7326, verbose=0, warm_start=False)
2023-12-19 16:52:35,527:INFO:create_model() successfully completed......................................
2023-12-19 16:52:35,614:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:35,614:INFO:Creating metrics dataframe
2023-12-19 16:52:35,626:INFO:Initializing Quadratic Discriminant Analysis
2023-12-19 16:52:35,626:INFO:Total runtime is 0.04321149984995524 minutes
2023-12-19 16:52:35,628:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:35,628:INFO:Initializing create_model()
2023-12-19 16:52:35,628:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:35,628:INFO:Checking exceptions
2023-12-19 16:52:35,628:INFO:Importing libraries
2023-12-19 16:52:35,628:INFO:Copying training dataset
2023-12-19 16:52:35,628:INFO:Defining folds
2023-12-19 16:52:35,628:INFO:Declaring metric variables
2023-12-19 16:52:35,628:INFO:Importing untrained model
2023-12-19 16:52:35,628:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-19 16:52:35,643:INFO:Starting cross validation
2023-12-19 16:52:35,645:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:35,698:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:52:35,698:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:52:35,698:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:52:35,698:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-19 16:52:35,727:INFO:Calculating mean and std
2023-12-19 16:52:35,727:INFO:Creating metrics dataframe
2023-12-19 16:52:35,727:INFO:Uploading results into container
2023-12-19 16:52:35,727:INFO:Uploading model into container now
2023-12-19 16:52:35,727:INFO:_master_model_container: 8
2023-12-19 16:52:35,727:INFO:_display_container: 2
2023-12-19 16:52:35,727:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-19 16:52:35,727:INFO:create_model() successfully completed......................................
2023-12-19 16:52:35,796:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:35,796:INFO:Creating metrics dataframe
2023-12-19 16:52:35,816:INFO:Initializing Ada Boost Classifier
2023-12-19 16:52:35,816:INFO:Total runtime is 0.04636340141296386 minutes
2023-12-19 16:52:35,819:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:35,819:INFO:Initializing create_model()
2023-12-19 16:52:35,819:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:35,819:INFO:Checking exceptions
2023-12-19 16:52:35,819:INFO:Importing libraries
2023-12-19 16:52:35,819:INFO:Copying training dataset
2023-12-19 16:52:35,819:INFO:Defining folds
2023-12-19 16:52:35,819:INFO:Declaring metric variables
2023-12-19 16:52:35,829:INFO:Importing untrained model
2023-12-19 16:52:35,831:INFO:Ada Boost Classifier Imported successfully
2023-12-19 16:52:35,832:INFO:Starting cross validation
2023-12-19 16:52:35,832:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:36,227:INFO:Calculating mean and std
2023-12-19 16:52:36,227:INFO:Creating metrics dataframe
2023-12-19 16:52:36,227:INFO:Uploading results into container
2023-12-19 16:52:36,227:INFO:Uploading model into container now
2023-12-19 16:52:36,227:INFO:_master_model_container: 9
2023-12-19 16:52:36,227:INFO:_display_container: 2
2023-12-19 16:52:36,227:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7326)
2023-12-19 16:52:36,227:INFO:create_model() successfully completed......................................
2023-12-19 16:52:36,311:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:36,311:INFO:Creating metrics dataframe
2023-12-19 16:52:36,314:INFO:Initializing Gradient Boosting Classifier
2023-12-19 16:52:36,314:INFO:Total runtime is 0.054677160580952955 minutes
2023-12-19 16:52:36,314:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:36,314:INFO:Initializing create_model()
2023-12-19 16:52:36,314:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:36,314:INFO:Checking exceptions
2023-12-19 16:52:36,314:INFO:Importing libraries
2023-12-19 16:52:36,314:INFO:Copying training dataset
2023-12-19 16:52:36,326:INFO:Defining folds
2023-12-19 16:52:36,326:INFO:Declaring metric variables
2023-12-19 16:52:36,326:INFO:Importing untrained model
2023-12-19 16:52:36,326:INFO:Gradient Boosting Classifier Imported successfully
2023-12-19 16:52:36,335:INFO:Starting cross validation
2023-12-19 16:52:36,335:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:39,745:INFO:Calculating mean and std
2023-12-19 16:52:39,745:INFO:Creating metrics dataframe
2023-12-19 16:52:39,745:INFO:Uploading results into container
2023-12-19 16:52:39,751:INFO:Uploading model into container now
2023-12-19 16:52:39,752:INFO:_master_model_container: 10
2023-12-19 16:52:39,752:INFO:_display_container: 2
2023-12-19 16:52:39,752:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7326, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-19 16:52:39,752:INFO:create_model() successfully completed......................................
2023-12-19 16:52:39,827:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:39,828:INFO:Creating metrics dataframe
2023-12-19 16:52:39,833:INFO:Initializing Linear Discriminant Analysis
2023-12-19 16:52:39,833:INFO:Total runtime is 0.11332578659057616 minutes
2023-12-19 16:52:39,833:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:39,843:INFO:Initializing create_model()
2023-12-19 16:52:39,843:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:39,843:INFO:Checking exceptions
2023-12-19 16:52:39,843:INFO:Importing libraries
2023-12-19 16:52:39,843:INFO:Copying training dataset
2023-12-19 16:52:39,849:INFO:Defining folds
2023-12-19 16:52:39,849:INFO:Declaring metric variables
2023-12-19 16:52:39,851:INFO:Importing untrained model
2023-12-19 16:52:39,851:INFO:Linear Discriminant Analysis Imported successfully
2023-12-19 16:52:39,851:INFO:Starting cross validation
2023-12-19 16:52:39,851:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:39,943:INFO:Calculating mean and std
2023-12-19 16:52:39,944:INFO:Creating metrics dataframe
2023-12-19 16:52:39,946:INFO:Uploading results into container
2023-12-19 16:52:39,946:INFO:Uploading model into container now
2023-12-19 16:52:39,946:INFO:_master_model_container: 11
2023-12-19 16:52:39,946:INFO:_display_container: 2
2023-12-19 16:52:39,946:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-19 16:52:39,946:INFO:create_model() successfully completed......................................
2023-12-19 16:52:40,016:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:40,016:INFO:Creating metrics dataframe
2023-12-19 16:52:40,019:INFO:Initializing Extra Trees Classifier
2023-12-19 16:52:40,019:INFO:Total runtime is 0.11641539732615151 minutes
2023-12-19 16:52:40,027:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:40,027:INFO:Initializing create_model()
2023-12-19 16:52:40,027:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:40,027:INFO:Checking exceptions
2023-12-19 16:52:40,027:INFO:Importing libraries
2023-12-19 16:52:40,027:INFO:Copying training dataset
2023-12-19 16:52:40,032:INFO:Defining folds
2023-12-19 16:52:40,032:INFO:Declaring metric variables
2023-12-19 16:52:40,035:INFO:Importing untrained model
2023-12-19 16:52:40,035:INFO:Extra Trees Classifier Imported successfully
2023-12-19 16:52:40,035:INFO:Starting cross validation
2023-12-19 16:52:40,043:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:40,482:INFO:Calculating mean and std
2023-12-19 16:52:40,483:INFO:Creating metrics dataframe
2023-12-19 16:52:40,485:INFO:Uploading results into container
2023-12-19 16:52:40,485:INFO:Uploading model into container now
2023-12-19 16:52:40,485:INFO:_master_model_container: 12
2023-12-19 16:52:40,485:INFO:_display_container: 2
2023-12-19 16:52:40,485:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=7326, verbose=0, warm_start=False)
2023-12-19 16:52:40,485:INFO:create_model() successfully completed......................................
2023-12-19 16:52:40,560:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:40,560:INFO:Creating metrics dataframe
2023-12-19 16:52:40,569:INFO:Initializing Extreme Gradient Boosting
2023-12-19 16:52:40,569:INFO:Total runtime is 0.12558171749114988 minutes
2023-12-19 16:52:40,569:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:40,569:INFO:Initializing create_model()
2023-12-19 16:52:40,569:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:40,569:INFO:Checking exceptions
2023-12-19 16:52:40,569:INFO:Importing libraries
2023-12-19 16:52:40,569:INFO:Copying training dataset
2023-12-19 16:52:40,576:INFO:Defining folds
2023-12-19 16:52:40,576:INFO:Declaring metric variables
2023-12-19 16:52:40,578:INFO:Importing untrained model
2023-12-19 16:52:40,580:INFO:Extreme Gradient Boosting Imported successfully
2023-12-19 16:52:40,584:INFO:Starting cross validation
2023-12-19 16:52:40,584:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:41,577:INFO:Calculating mean and std
2023-12-19 16:52:41,577:INFO:Creating metrics dataframe
2023-12-19 16:52:41,594:INFO:Uploading results into container
2023-12-19 16:52:41,595:INFO:Uploading model into container now
2023-12-19 16:52:41,595:INFO:_master_model_container: 13
2023-12-19 16:52:41,595:INFO:_display_container: 2
2023-12-19 16:52:41,596:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-19 16:52:41,596:INFO:create_model() successfully completed......................................
2023-12-19 16:52:41,701:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:41,701:INFO:Creating metrics dataframe
2023-12-19 16:52:41,718:INFO:Initializing Light Gradient Boosting Machine
2023-12-19 16:52:41,718:INFO:Total runtime is 0.14474219083786008 minutes
2023-12-19 16:52:41,720:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:41,720:INFO:Initializing create_model()
2023-12-19 16:52:41,720:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:41,720:INFO:Checking exceptions
2023-12-19 16:52:41,720:INFO:Importing libraries
2023-12-19 16:52:41,720:INFO:Copying training dataset
2023-12-19 16:52:41,727:INFO:Defining folds
2023-12-19 16:52:41,727:INFO:Declaring metric variables
2023-12-19 16:52:41,730:INFO:Importing untrained model
2023-12-19 16:52:41,731:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-19 16:52:41,737:INFO:Starting cross validation
2023-12-19 16:52:41,738:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:44,970:INFO:Calculating mean and std
2023-12-19 16:52:44,970:INFO:Creating metrics dataframe
2023-12-19 16:52:44,977:INFO:Uploading results into container
2023-12-19 16:52:44,978:INFO:Uploading model into container now
2023-12-19 16:52:44,979:INFO:_master_model_container: 14
2023-12-19 16:52:44,979:INFO:_display_container: 2
2023-12-19 16:52:44,980:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:52:44,980:INFO:create_model() successfully completed......................................
2023-12-19 16:52:45,084:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:45,084:INFO:Creating metrics dataframe
2023-12-19 16:52:45,084:INFO:Initializing Dummy Classifier
2023-12-19 16:52:45,084:INFO:Total runtime is 0.20083993673324582 minutes
2023-12-19 16:52:45,100:INFO:SubProcess create_model() called ==================================
2023-12-19 16:52:45,100:INFO:Initializing create_model()
2023-12-19 16:52:45,100:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF4E0050>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:45,100:INFO:Checking exceptions
2023-12-19 16:52:45,100:INFO:Importing libraries
2023-12-19 16:52:45,100:INFO:Copying training dataset
2023-12-19 16:52:45,100:INFO:Defining folds
2023-12-19 16:52:45,100:INFO:Declaring metric variables
2023-12-19 16:52:45,108:INFO:Importing untrained model
2023-12-19 16:52:45,111:INFO:Dummy Classifier Imported successfully
2023-12-19 16:52:45,113:INFO:Starting cross validation
2023-12-19 16:52:45,118:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:52:45,155:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:45,163:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:45,163:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:45,163:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:45,163:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-19 16:52:45,163:INFO:Calculating mean and std
2023-12-19 16:52:45,163:INFO:Creating metrics dataframe
2023-12-19 16:52:45,163:INFO:Uploading results into container
2023-12-19 16:52:45,163:INFO:Uploading model into container now
2023-12-19 16:52:45,163:INFO:_master_model_container: 15
2023-12-19 16:52:45,163:INFO:_display_container: 2
2023-12-19 16:52:45,163:INFO:DummyClassifier(constant=None, random_state=7326, strategy='prior')
2023-12-19 16:52:45,163:INFO:create_model() successfully completed......................................
2023-12-19 16:52:45,255:INFO:SubProcess create_model() end ==================================
2023-12-19 16:52:45,255:INFO:Creating metrics dataframe
2023-12-19 16:52:45,274:INFO:Initializing create_model()
2023-12-19 16:52:45,274:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:45,274:INFO:Checking exceptions
2023-12-19 16:52:45,277:INFO:Importing libraries
2023-12-19 16:52:45,277:INFO:Copying training dataset
2023-12-19 16:52:45,278:INFO:Defining folds
2023-12-19 16:52:45,278:INFO:Declaring metric variables
2023-12-19 16:52:45,278:INFO:Importing untrained model
2023-12-19 16:52:45,278:INFO:Declaring custom model
2023-12-19 16:52:45,278:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-19 16:52:45,278:INFO:Cross validation set to False
2023-12-19 16:52:45,278:INFO:Fitting Model
2023-12-19 16:52:45,314:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000984 seconds.
2023-12-19 16:52:45,314:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-19 16:52:45,314:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-19 16:52:45,315:INFO:[LightGBM] [Info] Total Bins 1741
2023-12-19 16:52:45,315:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 47
2023-12-19 16:52:45,315:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-19 16:52:45,315:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-19 16:52:45,316:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-19 16:52:45,594:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:52:45,594:INFO:create_model() successfully completed......................................
2023-12-19 16:52:45,710:INFO:Initializing create_model()
2023-12-19 16:52:45,710:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7326, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:45,710:INFO:Checking exceptions
2023-12-19 16:52:45,717:INFO:Importing libraries
2023-12-19 16:52:45,717:INFO:Copying training dataset
2023-12-19 16:52:45,723:INFO:Defining folds
2023-12-19 16:52:45,723:INFO:Declaring metric variables
2023-12-19 16:52:45,723:INFO:Importing untrained model
2023-12-19 16:52:45,723:INFO:Declaring custom model
2023-12-19 16:52:45,723:INFO:Gradient Boosting Classifier Imported successfully
2023-12-19 16:52:45,723:INFO:Cross validation set to False
2023-12-19 16:52:45,723:INFO:Fitting Model
2023-12-19 16:52:49,469:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7326, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-19 16:52:49,469:INFO:create_model() successfully completed......................................
2023-12-19 16:52:49,569:INFO:Initializing create_model()
2023-12-19 16:52:49,569:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:49,569:INFO:Checking exceptions
2023-12-19 16:52:49,569:INFO:Importing libraries
2023-12-19 16:52:49,569:INFO:Copying training dataset
2023-12-19 16:52:49,577:INFO:Defining folds
2023-12-19 16:52:49,577:INFO:Declaring metric variables
2023-12-19 16:52:49,577:INFO:Importing untrained model
2023-12-19 16:52:49,577:INFO:Declaring custom model
2023-12-19 16:52:49,577:INFO:Extreme Gradient Boosting Imported successfully
2023-12-19 16:52:49,577:INFO:Cross validation set to False
2023-12-19 16:52:49,577:INFO:Fitting Model
2023-12-19 16:52:49,893:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-19 16:52:49,893:INFO:create_model() successfully completed......................................
2023-12-19 16:52:50,010:INFO:Initializing create_model()
2023-12-19 16:52:50,010:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7326, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:50,010:INFO:Checking exceptions
2023-12-19 16:52:50,016:INFO:Importing libraries
2023-12-19 16:52:50,016:INFO:Copying training dataset
2023-12-19 16:52:50,021:INFO:Defining folds
2023-12-19 16:52:50,021:INFO:Declaring metric variables
2023-12-19 16:52:50,021:INFO:Importing untrained model
2023-12-19 16:52:50,021:INFO:Declaring custom model
2023-12-19 16:52:50,022:INFO:Random Forest Classifier Imported successfully
2023-12-19 16:52:50,022:INFO:Cross validation set to False
2023-12-19 16:52:50,022:INFO:Fitting Model
2023-12-19 16:52:50,252:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7326, verbose=0, warm_start=False)
2023-12-19 16:52:50,253:INFO:create_model() successfully completed......................................
2023-12-19 16:52:50,342:INFO:Initializing create_model()
2023-12-19 16:52:50,342:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7326), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:52:50,342:INFO:Checking exceptions
2023-12-19 16:52:50,343:INFO:Importing libraries
2023-12-19 16:52:50,343:INFO:Copying training dataset
2023-12-19 16:52:50,345:INFO:Defining folds
2023-12-19 16:52:50,345:INFO:Declaring metric variables
2023-12-19 16:52:50,345:INFO:Importing untrained model
2023-12-19 16:52:50,345:INFO:Declaring custom model
2023-12-19 16:52:50,345:INFO:Ada Boost Classifier Imported successfully
2023-12-19 16:52:50,345:INFO:Cross validation set to False
2023-12-19 16:52:50,345:INFO:Fitting Model
2023-12-19 16:52:50,710:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7326)
2023-12-19 16:52:50,710:INFO:create_model() successfully completed......................................
2023-12-19 16:52:50,850:INFO:_master_model_container: 15
2023-12-19 16:52:50,851:INFO:_display_container: 2
2023-12-19 16:52:50,856:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7326, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7326, verbose=0, warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7326)]
2023-12-19 16:52:50,856:INFO:compare_models() successfully completed......................................
2023-12-19 16:52:50,884:INFO:Initializing tune_model()
2023-12-19 16:52:50,885:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-19 16:52:50,885:INFO:Checking exceptions
2023-12-19 16:52:50,911:INFO:Copying training dataset
2023-12-19 16:52:50,923:INFO:Checking base model
2023-12-19 16:52:50,923:INFO:Base model : Light Gradient Boosting Machine
2023-12-19 16:52:50,928:INFO:Declaring metric variables
2023-12-19 16:52:50,932:INFO:Defining Hyperparameters
2023-12-19 16:52:51,020:INFO:Tuning with n_jobs=-1
2023-12-19 16:52:51,020:INFO:Initializing RandomizedSearchCV
2023-12-19 16:53:14,900:INFO:best_params: {'actual_estimator__reg_lambda': 1e-06, 'actual_estimator__reg_alpha': 0.15, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 150, 'actual_estimator__min_split_gain': 0.7, 'actual_estimator__min_child_samples': 81, 'actual_estimator__learning_rate': 0.3, 'actual_estimator__feature_fraction': 0.7, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.9}
2023-12-19 16:53:14,901:INFO:Hyperparameter search completed
2023-12-19 16:53:14,902:INFO:SubProcess create_model() called ==================================
2023-12-19 16:53:14,902:INFO:Initializing create_model()
2023-12-19 16:53:14,902:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000203DF3B79D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 1e-06, 'reg_alpha': 0.15, 'num_leaves': 70, 'n_estimators': 150, 'min_split_gain': 0.7, 'min_child_samples': 81, 'learning_rate': 0.3, 'feature_fraction': 0.7, 'bagging_freq': 1, 'bagging_fraction': 0.9})
2023-12-19 16:53:14,902:INFO:Checking exceptions
2023-12-19 16:53:14,902:INFO:Importing libraries
2023-12-19 16:53:14,902:INFO:Copying training dataset
2023-12-19 16:53:14,906:INFO:Defining folds
2023-12-19 16:53:14,906:INFO:Declaring metric variables
2023-12-19 16:53:14,918:INFO:Importing untrained model
2023-12-19 16:53:14,919:INFO:Declaring custom model
2023-12-19 16:53:14,920:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-19 16:53:14,920:INFO:Starting cross validation
2023-12-19 16:53:14,920:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:53:16,085:INFO:Calculating mean and std
2023-12-19 16:53:16,086:INFO:Creating metrics dataframe
2023-12-19 16:53:16,086:INFO:Finalizing model
2023-12-19 16:53:16,121:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2023-12-19 16:53:16,121:INFO:[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9
2023-12-19 16:53:16,121:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2023-12-19 16:53:16,121:INFO:[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7
2023-12-19 16:53:16,121:INFO:[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9
2023-12-19 16:53:16,121:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2023-12-19 16:53:16,135:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001907 seconds.
2023-12-19 16:53:16,136:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-19 16:53:16,136:INFO:[LightGBM] [Info] Total Bins 1739
2023-12-19 16:53:16,136:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 46
2023-12-19 16:53:16,137:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-19 16:53:16,137:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-19 16:53:16,137:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-19 16:53:16,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,164:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,200:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,205:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,218:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,235:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,246:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,251:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,253:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,258:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,261:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,265:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,268:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,284:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,286:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,301:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,303:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,305:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,334:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,350:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,351:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,353:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,354:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,355:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,356:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,357:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,358:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,360:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,361:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,363:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,364:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,365:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,367:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,368:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,372:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,384:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,386:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,389:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,401:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,402:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,403:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,405:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,418:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,419:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,420:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,421:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,434:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,434:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,438:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,451:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,451:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,452:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,453:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,453:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,454:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,454:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,455:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,455:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,456:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,457:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,459:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,461:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,461:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,461:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,462:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,463:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,465:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,467:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-19 16:53:16,502:INFO:Uploading results into container
2023-12-19 16:53:16,504:INFO:Uploading model into container now
2023-12-19 16:53:16,504:INFO:_master_model_container: 16
2023-12-19 16:53:16,505:INFO:_display_container: 3
2023-12-19 16:53:16,505:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=81, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=150, n_jobs=-1, num_leaves=70, objective=None,
               random_state=7326, reg_alpha=0.15, reg_lambda=1e-06,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:53:16,505:INFO:create_model() successfully completed......................................
2023-12-19 16:53:16,625:INFO:SubProcess create_model() end ==================================
2023-12-19 16:53:16,625:INFO:choose_better activated
2023-12-19 16:53:16,626:INFO:SubProcess create_model() called ==================================
2023-12-19 16:53:16,626:INFO:Initializing create_model()
2023-12-19 16:53:16,626:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-19 16:53:16,626:INFO:Checking exceptions
2023-12-19 16:53:16,626:INFO:Importing libraries
2023-12-19 16:53:16,626:INFO:Copying training dataset
2023-12-19 16:53:16,636:INFO:Defining folds
2023-12-19 16:53:16,636:INFO:Declaring metric variables
2023-12-19 16:53:16,636:INFO:Importing untrained model
2023-12-19 16:53:16,636:INFO:Declaring custom model
2023-12-19 16:53:16,636:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-19 16:53:16,636:INFO:Starting cross validation
2023-12-19 16:53:16,637:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-19 16:53:20,138:INFO:Calculating mean and std
2023-12-19 16:53:20,138:INFO:Creating metrics dataframe
2023-12-19 16:53:20,138:INFO:Finalizing model
2023-12-19 16:53:20,169:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002145 seconds.
2023-12-19 16:53:20,169:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-19 16:53:20,169:INFO:[LightGBM] [Info] Total Bins 1741
2023-12-19 16:53:20,169:INFO:[LightGBM] [Info] Number of data points in the train set: 6719, number of used features: 47
2023-12-19 16:53:20,169:INFO:[LightGBM] [Info] Start training from score -0.465104
2023-12-19 16:53:20,169:INFO:[LightGBM] [Info] Start training from score -3.357373
2023-12-19 16:53:20,169:INFO:[LightGBM] [Info] Start training from score -1.087365
2023-12-19 16:53:20,556:INFO:Uploading results into container
2023-12-19 16:53:20,557:INFO:Uploading model into container now
2023-12-19 16:53:20,558:INFO:_master_model_container: 17
2023-12-19 16:53:20,558:INFO:_display_container: 4
2023-12-19 16:53:20,558:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:53:20,558:INFO:create_model() successfully completed......................................
2023-12-19 16:53:20,672:INFO:SubProcess create_model() end ==================================
2023-12-19 16:53:20,672:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7326, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8241
2023-12-19 16:53:20,673:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=81, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=150, n_jobs=-1, num_leaves=70, objective=None,
               random_state=7326, reg_alpha=0.15, reg_lambda=1e-06,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8282
2023-12-19 16:53:20,673:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=81, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=150, n_jobs=-1, num_leaves=70, objective=None,
               random_state=7326, reg_alpha=0.15, reg_lambda=1e-06,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-19 16:53:20,673:INFO:choose_better completed
2023-12-19 16:53:20,677:INFO:_master_model_container: 17
2023-12-19 16:53:20,677:INFO:_display_container: 3
2023-12-19 16:53:20,677:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=81, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=150, n_jobs=-1, num_leaves=70, objective=None,
               random_state=7326, reg_alpha=0.15, reg_lambda=1e-06,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-19 16:53:20,677:INFO:tune_model() successfully completed......................................
2023-12-19 16:53:20,776:INFO:Initializing evaluate_model()
2023-12-19 16:53:20,776:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=81, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=150, n_jobs=-1, num_leaves=70, objective=None,
               random_state=7326, reg_alpha=0.15, reg_lambda=1e-06,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-19 16:53:20,792:INFO:Initializing plot_model()
2023-12-19 16:53:20,792:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000203DC0BBFD0>, estimator=LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.7,
               importance_type='split', learning_rate=0.3, max_depth=-1,
               min_child_samples=81, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=150, n_jobs=-1, num_leaves=70, objective=None,
               random_state=7326, reg_alpha=0.15, reg_lambda=1e-06,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-19 16:53:20,792:INFO:Checking exceptions
2023-12-19 16:53:20,797:INFO:Preloading libraries
2023-12-19 16:53:20,811:INFO:Copying training dataset
2023-12-19 16:53:20,811:INFO:Plot type: pipeline
2023-12-19 16:53:20,886:INFO:Visual Rendered Successfully
2023-12-19 16:53:20,968:INFO:plot_model() successfully completed......................................
2023-12-20 09:37:32,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 09:37:32,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 09:37:32,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 09:37:32,519:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 09:42:58,783:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 44208 (\N{HANGUL SYLLABLE GYEOL}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2023-12-20 09:42:58,783:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 50668 (\N{HANGUL SYLLABLE YEO}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2023-12-20 09:42:58,783:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 51221 (\N{HANGUL SYLLABLE JEONG}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2023-12-20 09:42:58,783:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 49345 (\N{HANGUL SYLLABLE SANG}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2023-12-20 09:42:58,783:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 44284 (\N{HANGUL SYLLABLE GWA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2023-12-20 09:42:58,783:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 45796 (\N{HANGUL SYLLABLE DA}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2023-12-20 09:53:11,575:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,576:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,577:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,577:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,578:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,579:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,579:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,579:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,579:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,579:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  df_o = pd.get_dummies(df_o)

2023-12-20 09:53:11,586:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:11,587:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2895479089.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  test_o = pd.get_dummies(test_o)

2023-12-20 09:53:20,206:INFO:PyCaret ClassificationExperiment
2023-12-20 09:53:20,206:INFO:Logging name: clf-default-name
2023-12-20 09:53:20,206:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 09:53:20,206:INFO:version 3.2.0
2023-12-20 09:53:20,206:INFO:Initializing setup()
2023-12-20 09:53:20,206:INFO:self.USI: c06d
2023-12-20 09:53:20,206:INFO:self._variable_keys: {'_ml_usecase', 'gpu_n_jobs_param', 'X_test', 'target_param', 'fix_imbalance', 'y_test', 'X', 'fold_groups_param', 'USI', 'y', 'idx', 'html_param', 'n_jobs_param', 'exp_id', 'log_plots_param', 'logging_param', 'pipeline', 'gpu_param', 'y_train', 'fold_shuffle_param', 'seed', 'exp_name_log', 'data', 'X_train', 'fold_generator', '_available_plots', 'is_multiclass', 'memory'}
2023-12-20 09:53:20,206:INFO:Checking environment
2023-12-20 09:53:20,206:INFO:python_version: 3.11.5
2023-12-20 09:53:20,206:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 09:53:20,206:INFO:machine: AMD64
2023-12-20 09:53:20,206:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 09:53:20,206:INFO:Memory: svmem(total=16718413824, available=5969514496, percent=64.3, used=10748899328, free=5969514496)
2023-12-20 09:53:20,206:INFO:Physical Core: 12
2023-12-20 09:53:20,206:INFO:Logical Core: 16
2023-12-20 09:53:20,206:INFO:Checking libraries
2023-12-20 09:53:20,206:INFO:System:
2023-12-20 09:53:20,206:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 09:53:20,206:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 09:53:20,206:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 09:53:20,206:INFO:PyCaret required dependencies:
2023-12-20 09:53:20,783:INFO:                 pip: 23.2.1
2023-12-20 09:53:20,783:INFO:          setuptools: 68.0.0
2023-12-20 09:53:20,783:INFO:             pycaret: 3.2.0
2023-12-20 09:53:20,783:INFO:             IPython: 8.15.0
2023-12-20 09:53:20,783:INFO:          ipywidgets: 8.0.4
2023-12-20 09:53:20,783:INFO:                tqdm: 4.65.0
2023-12-20 09:53:20,783:INFO:               numpy: 1.24.3
2023-12-20 09:53:20,783:INFO:              pandas: 1.5.3
2023-12-20 09:53:20,783:INFO:              jinja2: 3.1.2
2023-12-20 09:53:20,783:INFO:               scipy: 1.10.1
2023-12-20 09:53:20,783:INFO:              joblib: 1.2.0
2023-12-20 09:53:20,783:INFO:             sklearn: 1.2.1
2023-12-20 09:53:20,783:INFO:                pyod: 1.1.2
2023-12-20 09:53:20,783:INFO:            imblearn: 0.11.0
2023-12-20 09:53:20,783:INFO:   category_encoders: 2.6.3
2023-12-20 09:53:20,783:INFO:            lightgbm: 4.1.0
2023-12-20 09:53:20,783:INFO:               numba: 0.57.1
2023-12-20 09:53:20,783:INFO:            requests: 2.31.0
2023-12-20 09:53:20,783:INFO:          matplotlib: 3.6.0
2023-12-20 09:53:20,783:INFO:          scikitplot: 0.3.7
2023-12-20 09:53:20,783:INFO:         yellowbrick: 1.5
2023-12-20 09:53:20,783:INFO:              plotly: 5.9.0
2023-12-20 09:53:20,783:INFO:    plotly-resampler: Not installed
2023-12-20 09:53:20,783:INFO:             kaleido: 0.2.1
2023-12-20 09:53:20,783:INFO:           schemdraw: 0.15
2023-12-20 09:53:20,783:INFO:         statsmodels: 0.14.0
2023-12-20 09:53:20,783:INFO:              sktime: 0.21.1
2023-12-20 09:53:20,783:INFO:               tbats: 1.1.3
2023-12-20 09:53:20,783:INFO:            pmdarima: 2.0.4
2023-12-20 09:53:20,783:INFO:              psutil: 5.9.0
2023-12-20 09:53:20,783:INFO:          markupsafe: 2.1.1
2023-12-20 09:53:20,783:INFO:             pickle5: Not installed
2023-12-20 09:53:20,783:INFO:         cloudpickle: 2.2.1
2023-12-20 09:53:20,783:INFO:         deprecation: 2.1.0
2023-12-20 09:53:20,783:INFO:              xxhash: 2.0.2
2023-12-20 09:53:20,783:INFO:           wurlitzer: Not installed
2023-12-20 09:53:20,783:INFO:PyCaret optional dependencies:
2023-12-20 09:53:20,802:INFO:                shap: Not installed
2023-12-20 09:53:20,802:INFO:           interpret: Not installed
2023-12-20 09:53:20,802:INFO:                umap: Not installed
2023-12-20 09:53:20,802:INFO:     ydata_profiling: Not installed
2023-12-20 09:53:20,802:INFO:  explainerdashboard: Not installed
2023-12-20 09:53:20,802:INFO:             autoviz: Not installed
2023-12-20 09:53:20,802:INFO:           fairlearn: Not installed
2023-12-20 09:53:20,802:INFO:          deepchecks: Not installed
2023-12-20 09:53:20,802:INFO:             xgboost: 2.0.2
2023-12-20 09:53:20,802:INFO:            catboost: Not installed
2023-12-20 09:53:20,802:INFO:              kmodes: Not installed
2023-12-20 09:53:20,802:INFO:             mlxtend: Not installed
2023-12-20 09:53:20,802:INFO:       statsforecast: Not installed
2023-12-20 09:53:20,802:INFO:        tune_sklearn: Not installed
2023-12-20 09:53:20,802:INFO:                 ray: Not installed
2023-12-20 09:53:20,802:INFO:            hyperopt: Not installed
2023-12-20 09:53:20,802:INFO:              optuna: Not installed
2023-12-20 09:53:20,802:INFO:               skopt: Not installed
2023-12-20 09:53:20,802:INFO:              mlflow: Not installed
2023-12-20 09:53:20,802:INFO:              gradio: Not installed
2023-12-20 09:53:20,802:INFO:             fastapi: Not installed
2023-12-20 09:53:20,802:INFO:             uvicorn: Not installed
2023-12-20 09:53:20,802:INFO:              m2cgen: Not installed
2023-12-20 09:53:20,802:INFO:           evidently: Not installed
2023-12-20 09:53:20,802:INFO:               fugue: Not installed
2023-12-20 09:53:20,802:INFO:           streamlit: Not installed
2023-12-20 09:53:20,802:INFO:             prophet: Not installed
2023-12-20 09:53:20,802:INFO:None
2023-12-20 09:53:20,802:INFO:Set up data.
2023-12-20 09:53:20,824:INFO:Set up folding strategy.
2023-12-20 09:53:20,824:INFO:Set up train/test split.
2023-12-20 09:53:20,824:INFO:Set up index.
2023-12-20 09:53:20,824:INFO:Assigning column types.
2023-12-20 09:53:20,824:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 09:53:20,853:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 09:53:20,853:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 09:53:20,869:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 09:53:20,869:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 09:53:20,904:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 09:53:20,904:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 09:53:20,920:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 09:53:20,920:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 09:53:20,920:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 09:53:20,936:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 09:53:20,952:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 09:53:20,952:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 09:53:20,989:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 09:53:21,003:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 09:53:21,003:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 09:53:21,003:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 09:53:21,052:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 09:53:21,052:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 09:53:21,086:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 09:53:21,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 09:53:21,086:INFO:Preparing preprocessing pipeline...
2023-12-20 09:53:21,101:INFO:Set up simple imputation.
2023-12-20 09:53:21,101:INFO:Set up column name cleaning.
2023-12-20 09:53:21,153:INFO:Finished creating preprocessing pipeline.
2023-12-20 09:53:21,153:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'He...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 09:53:21,153:INFO:Creating final display dataframe.
2023-12-20 09:53:21,236:INFO:Setup _display_container:                     Description             Value
0                    Session id              8689
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 53)
4        Transformed data shape        (7905, 53)
5   Transformed train set shape        (6324, 53)
6    Transformed test set shape        (1581, 53)
7              Numeric features                52
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              c06d
2023-12-20 09:53:21,275:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 09:53:21,286:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 09:53:21,325:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 09:53:21,325:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 09:53:21,325:INFO:setup() successfully completed in 1.22s...............
2023-12-20 09:53:25,136:INFO:Initializing compare_models()
2023-12-20 09:53:25,137:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 09:53:25,137:INFO:Checking exceptions
2023-12-20 09:53:25,140:INFO:Preparing display monitor
2023-12-20 09:53:25,161:INFO:Initializing Logistic Regression
2023-12-20 09:53:25,161:INFO:Total runtime is 1.6709168752034504e-05 minutes
2023-12-20 09:53:25,161:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:25,161:INFO:Initializing create_model()
2023-12-20 09:53:25,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:25,161:INFO:Checking exceptions
2023-12-20 09:53:25,161:INFO:Importing libraries
2023-12-20 09:53:25,161:INFO:Copying training dataset
2023-12-20 09:53:25,169:INFO:Defining folds
2023-12-20 09:53:25,170:INFO:Declaring metric variables
2023-12-20 09:53:25,172:INFO:Importing untrained model
2023-12-20 09:53:25,175:INFO:Logistic Regression Imported successfully
2023-12-20 09:53:25,184:INFO:Starting cross validation
2023-12-20 09:53:25,185:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:29,141:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 09:53:29,186:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 09:53:29,186:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 09:53:29,203:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:29,203:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:29,203:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 09:53:29,236:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 09:53:29,241:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:29,253:INFO:Calculating mean and std
2023-12-20 09:53:29,253:INFO:Creating metrics dataframe
2023-12-20 09:53:29,253:INFO:Uploading results into container
2023-12-20 09:53:29,253:INFO:Uploading model into container now
2023-12-20 09:53:29,253:INFO:_master_model_container: 1
2023-12-20 09:53:29,253:INFO:_display_container: 2
2023-12-20 09:53:29,253:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8689, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 09:53:29,253:INFO:create_model() successfully completed......................................
2023-12-20 09:53:29,388:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:29,388:INFO:Creating metrics dataframe
2023-12-20 09:53:29,403:INFO:Initializing K Neighbors Classifier
2023-12-20 09:53:29,403:INFO:Total runtime is 0.0707221229871114 minutes
2023-12-20 09:53:29,403:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:29,403:INFO:Initializing create_model()
2023-12-20 09:53:29,418:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:29,418:INFO:Checking exceptions
2023-12-20 09:53:29,418:INFO:Importing libraries
2023-12-20 09:53:29,418:INFO:Copying training dataset
2023-12-20 09:53:29,420:INFO:Defining folds
2023-12-20 09:53:29,420:INFO:Declaring metric variables
2023-12-20 09:53:29,431:INFO:Importing untrained model
2023-12-20 09:53:29,434:INFO:K Neighbors Classifier Imported successfully
2023-12-20 09:53:29,438:INFO:Starting cross validation
2023-12-20 09:53:29,439:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:32,320:INFO:Calculating mean and std
2023-12-20 09:53:32,320:INFO:Creating metrics dataframe
2023-12-20 09:53:32,336:INFO:Uploading results into container
2023-12-20 09:53:32,336:INFO:Uploading model into container now
2023-12-20 09:53:32,336:INFO:_master_model_container: 2
2023-12-20 09:53:32,336:INFO:_display_container: 2
2023-12-20 09:53:32,336:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 09:53:32,336:INFO:create_model() successfully completed......................................
2023-12-20 09:53:32,452:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:32,452:INFO:Creating metrics dataframe
2023-12-20 09:53:32,473:INFO:Initializing Naive Bayes
2023-12-20 09:53:32,473:INFO:Total runtime is 0.1218805988629659 minutes
2023-12-20 09:53:32,476:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:32,476:INFO:Initializing create_model()
2023-12-20 09:53:32,476:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:32,476:INFO:Checking exceptions
2023-12-20 09:53:32,476:INFO:Importing libraries
2023-12-20 09:53:32,476:INFO:Copying training dataset
2023-12-20 09:53:32,479:INFO:Defining folds
2023-12-20 09:53:32,479:INFO:Declaring metric variables
2023-12-20 09:53:32,485:INFO:Importing untrained model
2023-12-20 09:53:32,486:INFO:Naive Bayes Imported successfully
2023-12-20 09:53:32,486:INFO:Starting cross validation
2023-12-20 09:53:32,486:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:35,302:INFO:Calculating mean and std
2023-12-20 09:53:35,302:INFO:Creating metrics dataframe
2023-12-20 09:53:35,302:INFO:Uploading results into container
2023-12-20 09:53:35,302:INFO:Uploading model into container now
2023-12-20 09:53:35,302:INFO:_master_model_container: 3
2023-12-20 09:53:35,302:INFO:_display_container: 2
2023-12-20 09:53:35,302:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 09:53:35,302:INFO:create_model() successfully completed......................................
2023-12-20 09:53:35,461:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:35,461:INFO:Creating metrics dataframe
2023-12-20 09:53:35,469:INFO:Initializing Decision Tree Classifier
2023-12-20 09:53:35,469:INFO:Total runtime is 0.17181756099065143 minutes
2023-12-20 09:53:35,469:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:35,469:INFO:Initializing create_model()
2023-12-20 09:53:35,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:35,469:INFO:Checking exceptions
2023-12-20 09:53:35,469:INFO:Importing libraries
2023-12-20 09:53:35,469:INFO:Copying training dataset
2023-12-20 09:53:35,486:INFO:Defining folds
2023-12-20 09:53:35,486:INFO:Declaring metric variables
2023-12-20 09:53:35,493:INFO:Importing untrained model
2023-12-20 09:53:35,496:INFO:Decision Tree Classifier Imported successfully
2023-12-20 09:53:35,496:INFO:Starting cross validation
2023-12-20 09:53:35,496:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:37,736:INFO:Calculating mean and std
2023-12-20 09:53:37,736:INFO:Creating metrics dataframe
2023-12-20 09:53:37,746:INFO:Uploading results into container
2023-12-20 09:53:37,746:INFO:Uploading model into container now
2023-12-20 09:53:37,747:INFO:_master_model_container: 4
2023-12-20 09:53:37,747:INFO:_display_container: 2
2023-12-20 09:53:37,747:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8689, splitter='best')
2023-12-20 09:53:37,747:INFO:create_model() successfully completed......................................
2023-12-20 09:53:37,869:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:37,869:INFO:Creating metrics dataframe
2023-12-20 09:53:37,869:INFO:Initializing SVM - Linear Kernel
2023-12-20 09:53:37,885:INFO:Total runtime is 0.2120772918065389 minutes
2023-12-20 09:53:37,886:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:37,886:INFO:Initializing create_model()
2023-12-20 09:53:37,886:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:37,886:INFO:Checking exceptions
2023-12-20 09:53:37,886:INFO:Importing libraries
2023-12-20 09:53:37,886:INFO:Copying training dataset
2023-12-20 09:53:37,890:INFO:Defining folds
2023-12-20 09:53:37,890:INFO:Declaring metric variables
2023-12-20 09:53:37,896:INFO:Importing untrained model
2023-12-20 09:53:37,897:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 09:53:37,903:INFO:Starting cross validation
2023-12-20 09:53:37,904:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:38,003:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 09:53:38,003:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:38,018:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 09:53:38,020:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 09:53:38,020:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 09:53:38,035:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:38,035:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 09:53:38,035:INFO:Calculating mean and std
2023-12-20 09:53:38,035:INFO:Creating metrics dataframe
2023-12-20 09:53:38,035:INFO:Uploading results into container
2023-12-20 09:53:38,035:INFO:Uploading model into container now
2023-12-20 09:53:38,035:INFO:_master_model_container: 5
2023-12-20 09:53:38,035:INFO:_display_container: 2
2023-12-20 09:53:38,035:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8689, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 09:53:38,035:INFO:create_model() successfully completed......................................
2023-12-20 09:53:38,170:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:38,170:INFO:Creating metrics dataframe
2023-12-20 09:53:38,189:INFO:Initializing Ridge Classifier
2023-12-20 09:53:38,190:INFO:Total runtime is 0.2171733816464742 minutes
2023-12-20 09:53:38,193:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:38,193:INFO:Initializing create_model()
2023-12-20 09:53:38,193:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:38,193:INFO:Checking exceptions
2023-12-20 09:53:38,193:INFO:Importing libraries
2023-12-20 09:53:38,193:INFO:Copying training dataset
2023-12-20 09:53:38,202:INFO:Defining folds
2023-12-20 09:53:38,202:INFO:Declaring metric variables
2023-12-20 09:53:38,203:INFO:Importing untrained model
2023-12-20 09:53:38,206:INFO:Ridge Classifier Imported successfully
2023-12-20 09:53:38,209:INFO:Starting cross validation
2023-12-20 09:53:38,209:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:38,258:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 09:53:38,260:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 09:53:38,261:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:38,263:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:38,266:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 09:53:38,269:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:38,269:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 09:53:38,269:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 09:53:38,269:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:38,269:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:38,269:INFO:Calculating mean and std
2023-12-20 09:53:38,269:INFO:Creating metrics dataframe
2023-12-20 09:53:38,269:INFO:Uploading results into container
2023-12-20 09:53:38,269:INFO:Uploading model into container now
2023-12-20 09:53:38,269:INFO:_master_model_container: 6
2023-12-20 09:53:38,269:INFO:_display_container: 2
2023-12-20 09:53:38,269:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=8689, solver='auto',
                tol=0.0001)
2023-12-20 09:53:38,269:INFO:create_model() successfully completed......................................
2023-12-20 09:53:38,409:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:38,409:INFO:Creating metrics dataframe
2023-12-20 09:53:38,419:INFO:Initializing Random Forest Classifier
2023-12-20 09:53:38,419:INFO:Total runtime is 0.22098743120829265 minutes
2023-12-20 09:53:38,419:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:38,419:INFO:Initializing create_model()
2023-12-20 09:53:38,419:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:38,419:INFO:Checking exceptions
2023-12-20 09:53:38,419:INFO:Importing libraries
2023-12-20 09:53:38,419:INFO:Copying training dataset
2023-12-20 09:53:38,419:INFO:Defining folds
2023-12-20 09:53:38,419:INFO:Declaring metric variables
2023-12-20 09:53:38,431:INFO:Importing untrained model
2023-12-20 09:53:38,433:INFO:Random Forest Classifier Imported successfully
2023-12-20 09:53:38,438:INFO:Starting cross validation
2023-12-20 09:53:38,439:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:38,922:INFO:Calculating mean and std
2023-12-20 09:53:38,922:INFO:Creating metrics dataframe
2023-12-20 09:53:38,922:INFO:Uploading results into container
2023-12-20 09:53:38,922:INFO:Uploading model into container now
2023-12-20 09:53:38,922:INFO:_master_model_container: 7
2023-12-20 09:53:38,922:INFO:_display_container: 2
2023-12-20 09:53:38,922:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8689, verbose=0, warm_start=False)
2023-12-20 09:53:38,922:INFO:create_model() successfully completed......................................
2023-12-20 09:53:39,056:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:39,056:INFO:Creating metrics dataframe
2023-12-20 09:53:39,072:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 09:53:39,072:INFO:Total runtime is 0.23186673323313395 minutes
2023-12-20 09:53:39,072:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:39,072:INFO:Initializing create_model()
2023-12-20 09:53:39,072:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:39,072:INFO:Checking exceptions
2023-12-20 09:53:39,072:INFO:Importing libraries
2023-12-20 09:53:39,072:INFO:Copying training dataset
2023-12-20 09:53:39,072:INFO:Defining folds
2023-12-20 09:53:39,072:INFO:Declaring metric variables
2023-12-20 09:53:39,089:INFO:Importing untrained model
2023-12-20 09:53:39,090:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 09:53:39,090:INFO:Starting cross validation
2023-12-20 09:53:39,090:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:39,152:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 09:53:39,152:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 09:53:39,153:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 09:53:39,153:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 09:53:39,154:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 09:53:39,169:INFO:Calculating mean and std
2023-12-20 09:53:39,169:INFO:Creating metrics dataframe
2023-12-20 09:53:39,169:INFO:Uploading results into container
2023-12-20 09:53:39,169:INFO:Uploading model into container now
2023-12-20 09:53:39,169:INFO:_master_model_container: 8
2023-12-20 09:53:39,169:INFO:_display_container: 2
2023-12-20 09:53:39,169:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 09:53:39,169:INFO:create_model() successfully completed......................................
2023-12-20 09:53:39,303:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:39,303:INFO:Creating metrics dataframe
2023-12-20 09:53:39,303:INFO:Initializing Ada Boost Classifier
2023-12-20 09:53:39,303:INFO:Total runtime is 0.23572310209274291 minutes
2023-12-20 09:53:39,319:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:39,319:INFO:Initializing create_model()
2023-12-20 09:53:39,319:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:39,319:INFO:Checking exceptions
2023-12-20 09:53:39,319:INFO:Importing libraries
2023-12-20 09:53:39,319:INFO:Copying training dataset
2023-12-20 09:53:39,319:INFO:Defining folds
2023-12-20 09:53:39,319:INFO:Declaring metric variables
2023-12-20 09:53:39,328:INFO:Importing untrained model
2023-12-20 09:53:39,329:INFO:Ada Boost Classifier Imported successfully
2023-12-20 09:53:39,335:INFO:Starting cross validation
2023-12-20 09:53:39,336:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:39,719:INFO:Calculating mean and std
2023-12-20 09:53:39,719:INFO:Creating metrics dataframe
2023-12-20 09:53:39,719:INFO:Uploading results into container
2023-12-20 09:53:39,719:INFO:Uploading model into container now
2023-12-20 09:53:39,719:INFO:_master_model_container: 9
2023-12-20 09:53:39,719:INFO:_display_container: 2
2023-12-20 09:53:39,719:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8689)
2023-12-20 09:53:39,719:INFO:create_model() successfully completed......................................
2023-12-20 09:53:39,857:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:39,857:INFO:Creating metrics dataframe
2023-12-20 09:53:39,875:INFO:Initializing Gradient Boosting Classifier
2023-12-20 09:53:39,875:INFO:Total runtime is 0.2452552040417989 minutes
2023-12-20 09:53:39,878:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:39,878:INFO:Initializing create_model()
2023-12-20 09:53:39,878:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:39,878:INFO:Checking exceptions
2023-12-20 09:53:39,878:INFO:Importing libraries
2023-12-20 09:53:39,878:INFO:Copying training dataset
2023-12-20 09:53:39,883:INFO:Defining folds
2023-12-20 09:53:39,883:INFO:Declaring metric variables
2023-12-20 09:53:39,886:INFO:Importing untrained model
2023-12-20 09:53:39,886:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 09:53:39,886:INFO:Starting cross validation
2023-12-20 09:53:39,886:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:43,202:INFO:Calculating mean and std
2023-12-20 09:53:43,202:INFO:Creating metrics dataframe
2023-12-20 09:53:43,202:INFO:Uploading results into container
2023-12-20 09:53:43,202:INFO:Uploading model into container now
2023-12-20 09:53:43,202:INFO:_master_model_container: 10
2023-12-20 09:53:43,217:INFO:_display_container: 2
2023-12-20 09:53:43,217:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8689, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 09:53:43,217:INFO:create_model() successfully completed......................................
2023-12-20 09:53:43,336:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:43,336:INFO:Creating metrics dataframe
2023-12-20 09:53:43,359:INFO:Initializing Linear Discriminant Analysis
2023-12-20 09:53:43,359:INFO:Total runtime is 0.30331350167592364 minutes
2023-12-20 09:53:43,359:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:43,359:INFO:Initializing create_model()
2023-12-20 09:53:43,359:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:43,359:INFO:Checking exceptions
2023-12-20 09:53:43,359:INFO:Importing libraries
2023-12-20 09:53:43,359:INFO:Copying training dataset
2023-12-20 09:53:43,372:INFO:Defining folds
2023-12-20 09:53:43,372:INFO:Declaring metric variables
2023-12-20 09:53:43,373:INFO:Importing untrained model
2023-12-20 09:53:43,377:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 09:53:43,378:INFO:Starting cross validation
2023-12-20 09:53:43,378:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:43,459:INFO:Calculating mean and std
2023-12-20 09:53:43,468:INFO:Creating metrics dataframe
2023-12-20 09:53:43,470:INFO:Uploading results into container
2023-12-20 09:53:43,470:INFO:Uploading model into container now
2023-12-20 09:53:43,471:INFO:_master_model_container: 11
2023-12-20 09:53:43,471:INFO:_display_container: 2
2023-12-20 09:53:43,471:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 09:53:43,471:INFO:create_model() successfully completed......................................
2023-12-20 09:53:43,602:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:43,602:INFO:Creating metrics dataframe
2023-12-20 09:53:43,619:INFO:Initializing Extra Trees Classifier
2023-12-20 09:53:43,619:INFO:Total runtime is 0.30765689214070635 minutes
2023-12-20 09:53:43,619:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:43,619:INFO:Initializing create_model()
2023-12-20 09:53:43,619:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:43,619:INFO:Checking exceptions
2023-12-20 09:53:43,619:INFO:Importing libraries
2023-12-20 09:53:43,619:INFO:Copying training dataset
2023-12-20 09:53:43,636:INFO:Defining folds
2023-12-20 09:53:43,636:INFO:Declaring metric variables
2023-12-20 09:53:43,636:INFO:Importing untrained model
2023-12-20 09:53:43,644:INFO:Extra Trees Classifier Imported successfully
2023-12-20 09:53:43,652:INFO:Starting cross validation
2023-12-20 09:53:43,653:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:44,059:INFO:Calculating mean and std
2023-12-20 09:53:44,059:INFO:Creating metrics dataframe
2023-12-20 09:53:44,069:INFO:Uploading results into container
2023-12-20 09:53:44,070:INFO:Uploading model into container now
2023-12-20 09:53:44,070:INFO:_master_model_container: 12
2023-12-20 09:53:44,070:INFO:_display_container: 2
2023-12-20 09:53:44,071:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8689, verbose=0, warm_start=False)
2023-12-20 09:53:44,071:INFO:create_model() successfully completed......................................
2023-12-20 09:53:44,203:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:44,203:INFO:Creating metrics dataframe
2023-12-20 09:53:44,203:INFO:Initializing Extreme Gradient Boosting
2023-12-20 09:53:44,203:INFO:Total runtime is 0.31738120714823403 minutes
2023-12-20 09:53:44,203:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:44,203:INFO:Initializing create_model()
2023-12-20 09:53:44,203:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:44,203:INFO:Checking exceptions
2023-12-20 09:53:44,203:INFO:Importing libraries
2023-12-20 09:53:44,218:INFO:Copying training dataset
2023-12-20 09:53:44,223:INFO:Defining folds
2023-12-20 09:53:44,223:INFO:Declaring metric variables
2023-12-20 09:53:44,226:INFO:Importing untrained model
2023-12-20 09:53:44,228:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 09:53:44,228:INFO:Starting cross validation
2023-12-20 09:53:44,235:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:44,852:INFO:Calculating mean and std
2023-12-20 09:53:44,853:INFO:Creating metrics dataframe
2023-12-20 09:53:44,855:INFO:Uploading results into container
2023-12-20 09:53:44,856:INFO:Uploading model into container now
2023-12-20 09:53:44,856:INFO:_master_model_container: 13
2023-12-20 09:53:44,856:INFO:_display_container: 2
2023-12-20 09:53:44,856:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 09:53:44,856:INFO:create_model() successfully completed......................................
2023-12-20 09:53:44,987:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:44,987:INFO:Creating metrics dataframe
2023-12-20 09:53:44,987:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 09:53:44,987:INFO:Total runtime is 0.33044272263844804 minutes
2023-12-20 09:53:45,002:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:45,002:INFO:Initializing create_model()
2023-12-20 09:53:45,002:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:45,002:INFO:Checking exceptions
2023-12-20 09:53:45,002:INFO:Importing libraries
2023-12-20 09:53:45,002:INFO:Copying training dataset
2023-12-20 09:53:45,002:INFO:Defining folds
2023-12-20 09:53:45,002:INFO:Declaring metric variables
2023-12-20 09:53:45,012:INFO:Importing untrained model
2023-12-20 09:53:45,012:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 09:53:45,018:INFO:Starting cross validation
2023-12-20 09:53:45,019:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:48,019:INFO:Calculating mean and std
2023-12-20 09:53:48,035:INFO:Creating metrics dataframe
2023-12-20 09:53:48,039:INFO:Uploading results into container
2023-12-20 09:53:48,040:INFO:Uploading model into container now
2023-12-20 09:53:48,041:INFO:_master_model_container: 14
2023-12-20 09:53:48,041:INFO:_display_container: 2
2023-12-20 09:53:48,041:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 09:53:48,041:INFO:create_model() successfully completed......................................
2023-12-20 09:53:48,203:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:48,203:INFO:Creating metrics dataframe
2023-12-20 09:53:48,219:INFO:Initializing Dummy Classifier
2023-12-20 09:53:48,219:INFO:Total runtime is 0.38432402610778804 minutes
2023-12-20 09:53:48,219:INFO:SubProcess create_model() called ==================================
2023-12-20 09:53:48,219:INFO:Initializing create_model()
2023-12-20 09:53:48,219:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FC77D7D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:48,219:INFO:Checking exceptions
2023-12-20 09:53:48,219:INFO:Importing libraries
2023-12-20 09:53:48,219:INFO:Copying training dataset
2023-12-20 09:53:48,219:INFO:Defining folds
2023-12-20 09:53:48,219:INFO:Declaring metric variables
2023-12-20 09:53:48,235:INFO:Importing untrained model
2023-12-20 09:53:48,236:INFO:Dummy Classifier Imported successfully
2023-12-20 09:53:48,253:INFO:Starting cross validation
2023-12-20 09:53:48,253:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:53:48,309:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:48,310:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:48,312:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:48,319:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:48,319:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 09:53:48,322:INFO:Calculating mean and std
2023-12-20 09:53:48,323:INFO:Creating metrics dataframe
2023-12-20 09:53:48,325:INFO:Uploading results into container
2023-12-20 09:53:48,326:INFO:Uploading model into container now
2023-12-20 09:53:48,326:INFO:_master_model_container: 15
2023-12-20 09:53:48,326:INFO:_display_container: 2
2023-12-20 09:53:48,326:INFO:DummyClassifier(constant=None, random_state=8689, strategy='prior')
2023-12-20 09:53:48,326:INFO:create_model() successfully completed......................................
2023-12-20 09:53:48,462:INFO:SubProcess create_model() end ==================================
2023-12-20 09:53:48,462:INFO:Creating metrics dataframe
2023-12-20 09:53:48,479:INFO:Initializing create_model()
2023-12-20 09:53:48,480:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:48,480:INFO:Checking exceptions
2023-12-20 09:53:48,480:INFO:Importing libraries
2023-12-20 09:53:48,480:INFO:Copying training dataset
2023-12-20 09:53:48,485:INFO:Defining folds
2023-12-20 09:53:48,485:INFO:Declaring metric variables
2023-12-20 09:53:48,485:INFO:Importing untrained model
2023-12-20 09:53:48,485:INFO:Declaring custom model
2023-12-20 09:53:48,486:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 09:53:48,487:INFO:Cross validation set to False
2023-12-20 09:53:48,487:INFO:Fitting Model
2023-12-20 09:53:48,518:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001252 seconds.
2023-12-20 09:53:48,518:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 09:53:48,519:INFO:[LightGBM] [Info] Total Bins 1744
2023-12-20 09:53:48,519:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 09:53:48,520:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 09:53:48,520:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 09:53:48,520:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 09:53:48,756:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 09:53:48,756:INFO:create_model() successfully completed......................................
2023-12-20 09:53:48,935:INFO:Initializing create_model()
2023-12-20 09:53:48,935:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8689, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:48,935:INFO:Checking exceptions
2023-12-20 09:53:48,941:INFO:Importing libraries
2023-12-20 09:53:48,941:INFO:Copying training dataset
2023-12-20 09:53:48,943:INFO:Defining folds
2023-12-20 09:53:48,943:INFO:Declaring metric variables
2023-12-20 09:53:48,943:INFO:Importing untrained model
2023-12-20 09:53:48,943:INFO:Declaring custom model
2023-12-20 09:53:48,943:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 09:53:48,943:INFO:Cross validation set to False
2023-12-20 09:53:48,943:INFO:Fitting Model
2023-12-20 09:53:52,435:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8689, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 09:53:52,435:INFO:create_model() successfully completed......................................
2023-12-20 09:53:52,590:INFO:Initializing create_model()
2023-12-20 09:53:52,590:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8689, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:52,590:INFO:Checking exceptions
2023-12-20 09:53:52,599:INFO:Importing libraries
2023-12-20 09:53:52,599:INFO:Copying training dataset
2023-12-20 09:53:52,603:INFO:Defining folds
2023-12-20 09:53:52,603:INFO:Declaring metric variables
2023-12-20 09:53:52,603:INFO:Importing untrained model
2023-12-20 09:53:52,603:INFO:Declaring custom model
2023-12-20 09:53:52,603:INFO:Random Forest Classifier Imported successfully
2023-12-20 09:53:52,603:INFO:Cross validation set to False
2023-12-20 09:53:52,603:INFO:Fitting Model
2023-12-20 09:53:52,836:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8689, verbose=0, warm_start=False)
2023-12-20 09:53:52,836:INFO:create_model() successfully completed......................................
2023-12-20 09:53:52,996:INFO:Initializing create_model()
2023-12-20 09:53:52,997:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:52,997:INFO:Checking exceptions
2023-12-20 09:53:52,999:INFO:Importing libraries
2023-12-20 09:53:52,999:INFO:Copying training dataset
2023-12-20 09:53:53,003:INFO:Defining folds
2023-12-20 09:53:53,003:INFO:Declaring metric variables
2023-12-20 09:53:53,003:INFO:Importing untrained model
2023-12-20 09:53:53,003:INFO:Declaring custom model
2023-12-20 09:53:53,003:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 09:53:53,003:INFO:Cross validation set to False
2023-12-20 09:53:53,003:INFO:Fitting Model
2023-12-20 09:53:53,320:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 09:53:53,320:INFO:create_model() successfully completed......................................
2023-12-20 09:53:53,454:INFO:Initializing create_model()
2023-12-20 09:53:53,454:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8689), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:53:53,454:INFO:Checking exceptions
2023-12-20 09:53:53,458:INFO:Importing libraries
2023-12-20 09:53:53,459:INFO:Copying training dataset
2023-12-20 09:53:53,460:INFO:Defining folds
2023-12-20 09:53:53,460:INFO:Declaring metric variables
2023-12-20 09:53:53,460:INFO:Importing untrained model
2023-12-20 09:53:53,460:INFO:Declaring custom model
2023-12-20 09:53:53,460:INFO:Ada Boost Classifier Imported successfully
2023-12-20 09:53:53,460:INFO:Cross validation set to False
2023-12-20 09:53:53,460:INFO:Fitting Model
2023-12-20 09:53:53,784:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8689)
2023-12-20 09:53:53,784:INFO:create_model() successfully completed......................................
2023-12-20 09:53:53,977:INFO:_master_model_container: 15
2023-12-20 09:53:53,977:INFO:_display_container: 2
2023-12-20 09:53:53,978:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8689, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8689, verbose=0, warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8689)]
2023-12-20 09:53:53,978:INFO:compare_models() successfully completed......................................
2023-12-20 09:53:56,388:INFO:Initializing tune_model()
2023-12-20 09:53:56,388:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 09:53:56,388:INFO:Checking exceptions
2023-12-20 09:53:56,410:INFO:Copying training dataset
2023-12-20 09:53:56,410:INFO:Checking base model
2023-12-20 09:53:56,410:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 09:53:56,418:INFO:Declaring metric variables
2023-12-20 09:53:56,422:INFO:Defining Hyperparameters
2023-12-20 09:53:56,574:INFO:Tuning with n_jobs=-1
2023-12-20 09:53:56,574:INFO:Initializing RandomizedSearchCV
2023-12-20 09:54:41,693:INFO:best_params: {'actual_estimator__reg_lambda': 2, 'actual_estimator__reg_alpha': 0.0001, 'actual_estimator__num_leaves': 150, 'actual_estimator__n_estimators': 120, 'actual_estimator__min_split_gain': 0.5, 'actual_estimator__min_child_samples': 76, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.5, 'actual_estimator__bagging_freq': 7, 'actual_estimator__bagging_fraction': 0.8}
2023-12-20 09:54:41,693:INFO:Hyperparameter search completed
2023-12-20 09:54:41,693:INFO:SubProcess create_model() called ==================================
2023-12-20 09:54:41,693:INFO:Initializing create_model()
2023-12-20 09:54:41,693:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237F091AF90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 2, 'reg_alpha': 0.0001, 'num_leaves': 150, 'n_estimators': 120, 'min_split_gain': 0.5, 'min_child_samples': 76, 'learning_rate': 0.1, 'feature_fraction': 0.5, 'bagging_freq': 7, 'bagging_fraction': 0.8})
2023-12-20 09:54:41,693:INFO:Checking exceptions
2023-12-20 09:54:41,693:INFO:Importing libraries
2023-12-20 09:54:41,693:INFO:Copying training dataset
2023-12-20 09:54:41,706:INFO:Defining folds
2023-12-20 09:54:41,706:INFO:Declaring metric variables
2023-12-20 09:54:41,714:INFO:Importing untrained model
2023-12-20 09:54:41,714:INFO:Declaring custom model
2023-12-20 09:54:41,720:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 09:54:41,720:INFO:Starting cross validation
2023-12-20 09:54:41,720:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:54:44,519:INFO:Calculating mean and std
2023-12-20 09:54:44,519:INFO:Creating metrics dataframe
2023-12-20 09:54:44,534:INFO:Finalizing model
2023-12-20 09:54:44,569:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-12-20 09:54:44,570:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 09:54:44,570:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-12-20 09:54:44,571:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-12-20 09:54:44,571:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 09:54:44,571:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-12-20 09:54:44,584:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002434 seconds.
2023-12-20 09:54:44,584:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 09:54:44,584:INFO:[LightGBM] [Info] Total Bins 1742
2023-12-20 09:54:44,585:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 48
2023-12-20 09:54:44,586:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 09:54:44,587:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 09:54:44,587:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 09:54:44,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,618:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,672:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,689:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,716:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,721:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,768:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,791:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 09:54:44,985:INFO:Uploading results into container
2023-12-20 09:54:44,987:INFO:Uploading model into container now
2023-12-20 09:54:44,987:INFO:_master_model_container: 16
2023-12-20 09:54:44,988:INFO:_display_container: 3
2023-12-20 09:54:44,988:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=120, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8689, reg_alpha=0.0001, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 09:54:44,989:INFO:create_model() successfully completed......................................
2023-12-20 09:54:45,152:INFO:SubProcess create_model() end ==================================
2023-12-20 09:54:45,152:INFO:choose_better activated
2023-12-20 09:54:45,152:INFO:SubProcess create_model() called ==================================
2023-12-20 09:54:45,152:INFO:Initializing create_model()
2023-12-20 09:54:45,152:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 09:54:45,152:INFO:Checking exceptions
2023-12-20 09:54:45,167:INFO:Importing libraries
2023-12-20 09:54:45,167:INFO:Copying training dataset
2023-12-20 09:54:45,167:INFO:Defining folds
2023-12-20 09:54:45,167:INFO:Declaring metric variables
2023-12-20 09:54:45,167:INFO:Importing untrained model
2023-12-20 09:54:45,167:INFO:Declaring custom model
2023-12-20 09:54:45,167:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 09:54:45,167:INFO:Starting cross validation
2023-12-20 09:54:45,167:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 09:54:48,426:INFO:Calculating mean and std
2023-12-20 09:54:48,426:INFO:Creating metrics dataframe
2023-12-20 09:54:48,426:INFO:Finalizing model
2023-12-20 09:54:48,470:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002236 seconds.
2023-12-20 09:54:48,470:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 09:54:48,471:INFO:[LightGBM] [Info] Total Bins 1744
2023-12-20 09:54:48,471:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 09:54:48,471:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 09:54:48,472:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 09:54:48,472:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 09:54:48,798:INFO:Uploading results into container
2023-12-20 09:54:48,798:INFO:Uploading model into container now
2023-12-20 09:54:48,798:INFO:_master_model_container: 17
2023-12-20 09:54:48,798:INFO:_display_container: 4
2023-12-20 09:54:48,798:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 09:54:48,798:INFO:create_model() successfully completed......................................
2023-12-20 09:54:48,967:INFO:SubProcess create_model() end ==================================
2023-12-20 09:54:48,967:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8689, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8188
2023-12-20 09:54:48,967:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=120, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8689, reg_alpha=0.0001, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.822
2023-12-20 09:54:48,967:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=120, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8689, reg_alpha=0.0001, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 09:54:48,967:INFO:choose_better completed
2023-12-20 09:54:48,967:INFO:_master_model_container: 17
2023-12-20 09:54:48,967:INFO:_display_container: 3
2023-12-20 09:54:48,967:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=120, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8689, reg_alpha=0.0001, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 09:54:48,967:INFO:tune_model() successfully completed......................................
2023-12-20 09:54:57,751:INFO:Initializing evaluate_model()
2023-12-20 09:54:57,751:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=120, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8689, reg_alpha=0.0001, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 09:54:57,774:INFO:Initializing plot_model()
2023-12-20 09:54:57,774:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=120, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8689, reg_alpha=0.0001, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 09:54:57,774:INFO:Checking exceptions
2023-12-20 09:54:57,774:INFO:Preloading libraries
2023-12-20 09:54:57,808:INFO:Copying training dataset
2023-12-20 09:54:57,809:INFO:Plot type: pipeline
2023-12-20 09:54:57,942:INFO:Visual Rendered Successfully
2023-12-20 09:54:58,079:INFO:plot_model() successfully completed......................................
2023-12-20 09:54:59,129:INFO:Initializing plot_model()
2023-12-20 09:54:59,129:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=120, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8689, reg_alpha=0.0001, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 09:54:59,129:INFO:Checking exceptions
2023-12-20 09:54:59,131:INFO:Preloading libraries
2023-12-20 09:54:59,158:INFO:Copying training dataset
2023-12-20 09:54:59,158:INFO:Plot type: parameter
2023-12-20 09:54:59,162:INFO:Visual Rendered Successfully
2023-12-20 09:54:59,313:INFO:plot_model() successfully completed......................................
2023-12-20 09:55:10,674:INFO:Initializing plot_model()
2023-12-20 09:55:10,674:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000237F74AC450>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=7, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.5,
               n_estimators=120, n_jobs=-1, num_leaves=150, objective=None,
               random_state=8689, reg_alpha=0.0001, reg_lambda=2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 09:55:10,674:INFO:Checking exceptions
2023-12-20 09:55:10,676:INFO:Preloading libraries
2023-12-20 09:55:10,701:INFO:Copying training dataset
2023-12-20 09:55:10,701:INFO:Plot type: auc
2023-12-20 09:55:10,811:INFO:Fitting Model
2023-12-20 09:55:10,811:INFO:Scoring test/hold-out set
2023-12-20 09:55:10,811:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-12-20 09:55:10,811:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 09:55:10,811:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-12-20 09:55:10,826:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-12-20 09:55:10,826:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 09:55:10,826:INFO:[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7
2023-12-20 09:55:10,995:INFO:Visual Rendered Successfully
2023-12-20 09:55:11,132:INFO:plot_model() successfully completed......................................
2023-12-20 09:58:53,211:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,212:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,214:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,215:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,215:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,216:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,217:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,217:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,217:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 09:58:53,217:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_25340\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:00:40,817:INFO:PyCaret ClassificationExperiment
2023-12-20 10:00:40,817:INFO:Logging name: clf-default-name
2023-12-20 10:00:40,817:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 10:00:40,817:INFO:version 3.2.0
2023-12-20 10:00:40,817:INFO:Initializing setup()
2023-12-20 10:00:40,817:INFO:self.USI: 0a13
2023-12-20 10:00:40,817:INFO:self._variable_keys: {'_ml_usecase', 'gpu_n_jobs_param', 'X_test', 'target_param', 'fix_imbalance', 'y_test', 'X', 'fold_groups_param', 'USI', 'y', 'idx', 'html_param', 'n_jobs_param', 'exp_id', 'log_plots_param', 'logging_param', 'pipeline', 'gpu_param', 'y_train', 'fold_shuffle_param', 'seed', 'exp_name_log', 'data', 'X_train', 'fold_generator', '_available_plots', 'is_multiclass', 'memory'}
2023-12-20 10:00:40,817:INFO:Checking environment
2023-12-20 10:00:40,817:INFO:python_version: 3.11.5
2023-12-20 10:00:40,817:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 10:00:40,817:INFO:machine: AMD64
2023-12-20 10:00:40,817:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 10:00:40,817:INFO:Memory: svmem(total=16718413824, available=6218887168, percent=62.8, used=10499526656, free=6218887168)
2023-12-20 10:00:40,817:INFO:Physical Core: 12
2023-12-20 10:00:40,817:INFO:Logical Core: 16
2023-12-20 10:00:40,817:INFO:Checking libraries
2023-12-20 10:00:40,817:INFO:System:
2023-12-20 10:00:40,817:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 10:00:40,817:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 10:00:40,817:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 10:00:40,817:INFO:PyCaret required dependencies:
2023-12-20 10:00:40,817:INFO:                 pip: 23.2.1
2023-12-20 10:00:40,817:INFO:          setuptools: 68.0.0
2023-12-20 10:00:40,818:INFO:             pycaret: 3.2.0
2023-12-20 10:00:40,818:INFO:             IPython: 8.15.0
2023-12-20 10:00:40,818:INFO:          ipywidgets: 8.0.4
2023-12-20 10:00:40,818:INFO:                tqdm: 4.65.0
2023-12-20 10:00:40,818:INFO:               numpy: 1.24.3
2023-12-20 10:00:40,818:INFO:              pandas: 1.5.3
2023-12-20 10:00:40,818:INFO:              jinja2: 3.1.2
2023-12-20 10:00:40,818:INFO:               scipy: 1.10.1
2023-12-20 10:00:40,818:INFO:              joblib: 1.2.0
2023-12-20 10:00:40,818:INFO:             sklearn: 1.2.1
2023-12-20 10:00:40,818:INFO:                pyod: 1.1.2
2023-12-20 10:00:40,818:INFO:            imblearn: 0.11.0
2023-12-20 10:00:40,818:INFO:   category_encoders: 2.6.3
2023-12-20 10:00:40,818:INFO:            lightgbm: 4.1.0
2023-12-20 10:00:40,818:INFO:               numba: 0.57.1
2023-12-20 10:00:40,818:INFO:            requests: 2.31.0
2023-12-20 10:00:40,818:INFO:          matplotlib: 3.6.0
2023-12-20 10:00:40,818:INFO:          scikitplot: 0.3.7
2023-12-20 10:00:40,818:INFO:         yellowbrick: 1.5
2023-12-20 10:00:40,818:INFO:              plotly: 5.9.0
2023-12-20 10:00:40,818:INFO:    plotly-resampler: Not installed
2023-12-20 10:00:40,818:INFO:             kaleido: 0.2.1
2023-12-20 10:00:40,818:INFO:           schemdraw: 0.15
2023-12-20 10:00:40,818:INFO:         statsmodels: 0.14.0
2023-12-20 10:00:40,818:INFO:              sktime: 0.21.1
2023-12-20 10:00:40,818:INFO:               tbats: 1.1.3
2023-12-20 10:00:40,818:INFO:            pmdarima: 2.0.4
2023-12-20 10:00:40,818:INFO:              psutil: 5.9.0
2023-12-20 10:00:40,818:INFO:          markupsafe: 2.1.1
2023-12-20 10:00:40,818:INFO:             pickle5: Not installed
2023-12-20 10:00:40,818:INFO:         cloudpickle: 2.2.1
2023-12-20 10:00:40,818:INFO:         deprecation: 2.1.0
2023-12-20 10:00:40,818:INFO:              xxhash: 2.0.2
2023-12-20 10:00:40,818:INFO:           wurlitzer: Not installed
2023-12-20 10:00:40,818:INFO:PyCaret optional dependencies:
2023-12-20 10:00:40,818:INFO:                shap: Not installed
2023-12-20 10:00:40,818:INFO:           interpret: Not installed
2023-12-20 10:00:40,818:INFO:                umap: Not installed
2023-12-20 10:00:40,818:INFO:     ydata_profiling: Not installed
2023-12-20 10:00:40,818:INFO:  explainerdashboard: Not installed
2023-12-20 10:00:40,818:INFO:             autoviz: Not installed
2023-12-20 10:00:40,818:INFO:           fairlearn: Not installed
2023-12-20 10:00:40,818:INFO:          deepchecks: Not installed
2023-12-20 10:00:40,818:INFO:             xgboost: 2.0.2
2023-12-20 10:00:40,818:INFO:            catboost: Not installed
2023-12-20 10:00:40,818:INFO:              kmodes: Not installed
2023-12-20 10:00:40,818:INFO:             mlxtend: Not installed
2023-12-20 10:00:40,818:INFO:       statsforecast: Not installed
2023-12-20 10:00:40,819:INFO:        tune_sklearn: Not installed
2023-12-20 10:00:40,819:INFO:                 ray: Not installed
2023-12-20 10:00:40,819:INFO:            hyperopt: Not installed
2023-12-20 10:00:40,819:INFO:              optuna: Not installed
2023-12-20 10:00:40,819:INFO:               skopt: Not installed
2023-12-20 10:00:40,819:INFO:              mlflow: Not installed
2023-12-20 10:00:40,819:INFO:              gradio: Not installed
2023-12-20 10:00:40,819:INFO:             fastapi: Not installed
2023-12-20 10:00:40,819:INFO:             uvicorn: Not installed
2023-12-20 10:00:40,819:INFO:              m2cgen: Not installed
2023-12-20 10:00:40,819:INFO:           evidently: Not installed
2023-12-20 10:00:40,819:INFO:               fugue: Not installed
2023-12-20 10:00:40,819:INFO:           streamlit: Not installed
2023-12-20 10:00:40,819:INFO:             prophet: Not installed
2023-12-20 10:00:40,819:INFO:None
2023-12-20 10:00:40,819:INFO:Set up data.
2023-12-20 10:00:40,834:INFO:Set up folding strategy.
2023-12-20 10:00:40,834:INFO:Set up train/test split.
2023-12-20 10:00:40,842:INFO:Set up index.
2023-12-20 10:00:40,842:INFO:Assigning column types.
2023-12-20 10:00:40,843:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 10:00:40,871:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:00:40,871:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:00:40,888:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:00:40,888:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:00:40,921:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:00:40,921:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:00:40,938:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:00:40,938:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:00:40,938:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 10:00:40,955:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:00:40,971:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:00:40,971:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:00:41,005:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:00:41,021:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:00:41,021:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:00:41,021:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 10:00:41,071:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:00:41,080:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:00:41,121:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:00:41,121:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:00:41,121:INFO:Preparing preprocessing pipeline...
2023-12-20 10:00:41,121:INFO:Set up simple imputation.
2023-12-20 10:00:41,121:INFO:Set up column name cleaning.
2023-12-20 10:00:41,154:INFO:Finished creating preprocessing pipeline.
2023-12-20 10:00:41,154:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'He...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 10:00:41,154:INFO:Creating final display dataframe.
2023-12-20 10:00:41,239:INFO:Setup _display_container:                     Description             Value
0                    Session id              3380
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 53)
4        Transformed data shape        (7905, 53)
5   Transformed train set shape        (6324, 53)
6    Transformed test set shape        (1581, 53)
7              Numeric features                52
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              0a13
2023-12-20 10:00:41,291:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:00:41,291:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:00:41,338:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:00:41,338:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:00:41,338:INFO:setup() successfully completed in 0.53s...............
2023-12-20 10:00:42,679:INFO:Initializing compare_models()
2023-12-20 10:00:42,680:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 10:00:42,680:INFO:Checking exceptions
2023-12-20 10:00:42,684:INFO:Preparing display monitor
2023-12-20 10:00:42,699:INFO:Initializing Logistic Regression
2023-12-20 10:00:42,699:INFO:Total runtime is 0.0 minutes
2023-12-20 10:00:42,703:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:42,703:INFO:Initializing create_model()
2023-12-20 10:00:42,703:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:42,704:INFO:Checking exceptions
2023-12-20 10:00:42,704:INFO:Importing libraries
2023-12-20 10:00:42,704:INFO:Copying training dataset
2023-12-20 10:00:42,709:INFO:Defining folds
2023-12-20 10:00:42,709:INFO:Declaring metric variables
2023-12-20 10:00:42,711:INFO:Importing untrained model
2023-12-20 10:00:42,711:INFO:Logistic Regression Imported successfully
2023-12-20 10:00:42,721:INFO:Starting cross validation
2023-12-20 10:00:42,722:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:46,789:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 10:00:46,813:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 10:00:46,841:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:46,843:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 10:00:46,853:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 10:00:46,855:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:46,875:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2023-12-20 10:00:46,888:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:46,888:INFO:Calculating mean and std
2023-12-20 10:00:46,888:INFO:Creating metrics dataframe
2023-12-20 10:00:46,905:INFO:Uploading results into container
2023-12-20 10:00:46,905:INFO:Uploading model into container now
2023-12-20 10:00:46,905:INFO:_master_model_container: 1
2023-12-20 10:00:46,905:INFO:_display_container: 2
2023-12-20 10:00:46,905:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=3380, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 10:00:46,905:INFO:create_model() successfully completed......................................
2023-12-20 10:00:47,088:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:47,088:INFO:Creating metrics dataframe
2023-12-20 10:00:47,091:INFO:Initializing K Neighbors Classifier
2023-12-20 10:00:47,091:INFO:Total runtime is 0.07319960594177247 minutes
2023-12-20 10:00:47,091:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:47,091:INFO:Initializing create_model()
2023-12-20 10:00:47,091:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:47,091:INFO:Checking exceptions
2023-12-20 10:00:47,091:INFO:Importing libraries
2023-12-20 10:00:47,091:INFO:Copying training dataset
2023-12-20 10:00:47,103:INFO:Defining folds
2023-12-20 10:00:47,104:INFO:Declaring metric variables
2023-12-20 10:00:47,107:INFO:Importing untrained model
2023-12-20 10:00:47,107:INFO:K Neighbors Classifier Imported successfully
2023-12-20 10:00:47,107:INFO:Starting cross validation
2023-12-20 10:00:47,107:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:50,294:INFO:Calculating mean and std
2023-12-20 10:00:50,294:INFO:Creating metrics dataframe
2023-12-20 10:00:50,294:INFO:Uploading results into container
2023-12-20 10:00:50,303:INFO:Uploading model into container now
2023-12-20 10:00:50,303:INFO:_master_model_container: 2
2023-12-20 10:00:50,303:INFO:_display_container: 2
2023-12-20 10:00:50,304:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 10:00:50,304:INFO:create_model() successfully completed......................................
2023-12-20 10:00:50,479:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:50,479:INFO:Creating metrics dataframe
2023-12-20 10:00:50,491:INFO:Initializing Naive Bayes
2023-12-20 10:00:50,492:INFO:Total runtime is 0.1298826495806376 minutes
2023-12-20 10:00:50,494:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:50,494:INFO:Initializing create_model()
2023-12-20 10:00:50,495:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:50,495:INFO:Checking exceptions
2023-12-20 10:00:50,495:INFO:Importing libraries
2023-12-20 10:00:50,495:INFO:Copying training dataset
2023-12-20 10:00:50,502:INFO:Defining folds
2023-12-20 10:00:50,502:INFO:Declaring metric variables
2023-12-20 10:00:50,504:INFO:Importing untrained model
2023-12-20 10:00:50,504:INFO:Naive Bayes Imported successfully
2023-12-20 10:00:50,504:INFO:Starting cross validation
2023-12-20 10:00:50,504:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:53,388:INFO:Calculating mean and std
2023-12-20 10:00:53,388:INFO:Creating metrics dataframe
2023-12-20 10:00:53,388:INFO:Uploading results into container
2023-12-20 10:00:53,388:INFO:Uploading model into container now
2023-12-20 10:00:53,388:INFO:_master_model_container: 3
2023-12-20 10:00:53,388:INFO:_display_container: 2
2023-12-20 10:00:53,388:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 10:00:53,388:INFO:create_model() successfully completed......................................
2023-12-20 10:00:53,573:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:53,573:INFO:Creating metrics dataframe
2023-12-20 10:00:53,580:INFO:Initializing Decision Tree Classifier
2023-12-20 10:00:53,580:INFO:Total runtime is 0.18135071198145547 minutes
2023-12-20 10:00:53,588:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:53,588:INFO:Initializing create_model()
2023-12-20 10:00:53,588:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:53,588:INFO:Checking exceptions
2023-12-20 10:00:53,588:INFO:Importing libraries
2023-12-20 10:00:53,588:INFO:Copying training dataset
2023-12-20 10:00:53,588:INFO:Defining folds
2023-12-20 10:00:53,588:INFO:Declaring metric variables
2023-12-20 10:00:53,588:INFO:Importing untrained model
2023-12-20 10:00:53,598:INFO:Decision Tree Classifier Imported successfully
2023-12-20 10:00:53,604:INFO:Starting cross validation
2023-12-20 10:00:53,604:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:56,005:INFO:Calculating mean and std
2023-12-20 10:00:56,005:INFO:Creating metrics dataframe
2023-12-20 10:00:56,012:INFO:Uploading results into container
2023-12-20 10:00:56,012:INFO:Uploading model into container now
2023-12-20 10:00:56,012:INFO:_master_model_container: 4
2023-12-20 10:00:56,012:INFO:_display_container: 2
2023-12-20 10:00:56,013:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=3380, splitter='best')
2023-12-20 10:00:56,013:INFO:create_model() successfully completed......................................
2023-12-20 10:00:56,153:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:56,153:INFO:Creating metrics dataframe
2023-12-20 10:00:56,154:INFO:Initializing SVM - Linear Kernel
2023-12-20 10:00:56,154:INFO:Total runtime is 0.2242475112279256 minutes
2023-12-20 10:00:56,165:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:56,166:INFO:Initializing create_model()
2023-12-20 10:00:56,166:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:56,166:INFO:Checking exceptions
2023-12-20 10:00:56,166:INFO:Importing libraries
2023-12-20 10:00:56,166:INFO:Copying training dataset
2023-12-20 10:00:56,173:INFO:Defining folds
2023-12-20 10:00:56,173:INFO:Declaring metric variables
2023-12-20 10:00:56,176:INFO:Importing untrained model
2023-12-20 10:00:56,177:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 10:00:56,177:INFO:Starting cross validation
2023-12-20 10:00:56,185:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:56,304:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:00:56,304:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:00:56,319:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:00:56,319:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:00:56,319:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:00:56,319:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:56,319:INFO:Calculating mean and std
2023-12-20 10:00:56,319:INFO:Creating metrics dataframe
2023-12-20 10:00:56,333:INFO:Uploading results into container
2023-12-20 10:00:56,333:INFO:Uploading model into container now
2023-12-20 10:00:56,333:INFO:_master_model_container: 5
2023-12-20 10:00:56,333:INFO:_display_container: 2
2023-12-20 10:00:56,334:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=3380, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 10:00:56,334:INFO:create_model() successfully completed......................................
2023-12-20 10:00:56,483:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:56,483:INFO:Creating metrics dataframe
2023-12-20 10:00:56,485:INFO:Initializing Ridge Classifier
2023-12-20 10:00:56,485:INFO:Total runtime is 0.22977294127146403 minutes
2023-12-20 10:00:56,499:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:56,499:INFO:Initializing create_model()
2023-12-20 10:00:56,499:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:56,499:INFO:Checking exceptions
2023-12-20 10:00:56,499:INFO:Importing libraries
2023-12-20 10:00:56,499:INFO:Copying training dataset
2023-12-20 10:00:56,507:INFO:Defining folds
2023-12-20 10:00:56,507:INFO:Declaring metric variables
2023-12-20 10:00:56,512:INFO:Importing untrained model
2023-12-20 10:00:56,514:INFO:Ridge Classifier Imported successfully
2023-12-20 10:00:56,514:INFO:Starting cross validation
2023-12-20 10:00:56,521:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:56,560:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:00:56,564:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:56,571:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:00:56,571:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:56,571:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:00:56,571:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:00:56,571:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:00:56,571:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:56,571:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:56,571:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:00:56,585:INFO:Calculating mean and std
2023-12-20 10:00:56,585:INFO:Creating metrics dataframe
2023-12-20 10:00:56,588:INFO:Uploading results into container
2023-12-20 10:00:56,588:INFO:Uploading model into container now
2023-12-20 10:00:56,588:INFO:_master_model_container: 6
2023-12-20 10:00:56,588:INFO:_display_container: 2
2023-12-20 10:00:56,588:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=3380, solver='auto',
                tol=0.0001)
2023-12-20 10:00:56,588:INFO:create_model() successfully completed......................................
2023-12-20 10:00:56,726:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:56,726:INFO:Creating metrics dataframe
2023-12-20 10:00:56,742:INFO:Initializing Random Forest Classifier
2023-12-20 10:00:56,742:INFO:Total runtime is 0.234055229028066 minutes
2023-12-20 10:00:56,742:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:56,742:INFO:Initializing create_model()
2023-12-20 10:00:56,742:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:56,742:INFO:Checking exceptions
2023-12-20 10:00:56,742:INFO:Importing libraries
2023-12-20 10:00:56,742:INFO:Copying training dataset
2023-12-20 10:00:56,742:INFO:Defining folds
2023-12-20 10:00:56,742:INFO:Declaring metric variables
2023-12-20 10:00:56,757:INFO:Importing untrained model
2023-12-20 10:00:56,760:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:00:56,761:INFO:Starting cross validation
2023-12-20 10:00:56,761:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:57,241:INFO:Calculating mean and std
2023-12-20 10:00:57,241:INFO:Creating metrics dataframe
2023-12-20 10:00:57,241:INFO:Uploading results into container
2023-12-20 10:00:57,241:INFO:Uploading model into container now
2023-12-20 10:00:57,241:INFO:_master_model_container: 7
2023-12-20 10:00:57,241:INFO:_display_container: 2
2023-12-20 10:00:57,241:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=3380, verbose=0, warm_start=False)
2023-12-20 10:00:57,241:INFO:create_model() successfully completed......................................
2023-12-20 10:00:57,400:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:57,400:INFO:Creating metrics dataframe
2023-12-20 10:00:57,411:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 10:00:57,412:INFO:Total runtime is 0.2452065904935201 minutes
2023-12-20 10:00:57,416:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:57,416:INFO:Initializing create_model()
2023-12-20 10:00:57,416:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:57,416:INFO:Checking exceptions
2023-12-20 10:00:57,416:INFO:Importing libraries
2023-12-20 10:00:57,416:INFO:Copying training dataset
2023-12-20 10:00:57,424:INFO:Defining folds
2023-12-20 10:00:57,425:INFO:Declaring metric variables
2023-12-20 10:00:57,427:INFO:Importing untrained model
2023-12-20 10:00:57,429:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 10:00:57,434:INFO:Starting cross validation
2023-12-20 10:00:57,435:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:57,479:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:00:57,480:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:00:57,480:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:00:57,480:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:00:57,486:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:00:57,506:INFO:Calculating mean and std
2023-12-20 10:00:57,506:INFO:Creating metrics dataframe
2023-12-20 10:00:57,508:INFO:Uploading results into container
2023-12-20 10:00:57,509:INFO:Uploading model into container now
2023-12-20 10:00:57,509:INFO:_master_model_container: 8
2023-12-20 10:00:57,509:INFO:_display_container: 2
2023-12-20 10:00:57,509:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 10:00:57,509:INFO:create_model() successfully completed......................................
2023-12-20 10:00:57,674:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:57,674:INFO:Creating metrics dataframe
2023-12-20 10:00:57,674:INFO:Initializing Ada Boost Classifier
2023-12-20 10:00:57,674:INFO:Total runtime is 0.24958265622456868 minutes
2023-12-20 10:00:57,674:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:57,674:INFO:Initializing create_model()
2023-12-20 10:00:57,674:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:57,674:INFO:Checking exceptions
2023-12-20 10:00:57,674:INFO:Importing libraries
2023-12-20 10:00:57,674:INFO:Copying training dataset
2023-12-20 10:00:57,690:INFO:Defining folds
2023-12-20 10:00:57,690:INFO:Declaring metric variables
2023-12-20 10:00:57,690:INFO:Importing untrained model
2023-12-20 10:00:57,696:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:00:57,698:INFO:Starting cross validation
2023-12-20 10:00:57,703:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:00:58,103:INFO:Calculating mean and std
2023-12-20 10:00:58,103:INFO:Creating metrics dataframe
2023-12-20 10:00:58,103:INFO:Uploading results into container
2023-12-20 10:00:58,116:INFO:Uploading model into container now
2023-12-20 10:00:58,116:INFO:_master_model_container: 9
2023-12-20 10:00:58,116:INFO:_display_container: 2
2023-12-20 10:00:58,116:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=3380)
2023-12-20 10:00:58,116:INFO:create_model() successfully completed......................................
2023-12-20 10:00:58,308:INFO:SubProcess create_model() end ==================================
2023-12-20 10:00:58,308:INFO:Creating metrics dataframe
2023-12-20 10:00:58,321:INFO:Initializing Gradient Boosting Classifier
2023-12-20 10:00:58,321:INFO:Total runtime is 0.2603677789370219 minutes
2023-12-20 10:00:58,336:INFO:SubProcess create_model() called ==================================
2023-12-20 10:00:58,336:INFO:Initializing create_model()
2023-12-20 10:00:58,336:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:00:58,336:INFO:Checking exceptions
2023-12-20 10:00:58,336:INFO:Importing libraries
2023-12-20 10:00:58,336:INFO:Copying training dataset
2023-12-20 10:00:58,336:INFO:Defining folds
2023-12-20 10:00:58,336:INFO:Declaring metric variables
2023-12-20 10:00:58,347:INFO:Importing untrained model
2023-12-20 10:00:58,349:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:00:58,354:INFO:Starting cross validation
2023-12-20 10:00:58,356:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:01:01,839:INFO:Calculating mean and std
2023-12-20 10:01:01,839:INFO:Creating metrics dataframe
2023-12-20 10:01:01,847:INFO:Uploading results into container
2023-12-20 10:01:01,847:INFO:Uploading model into container now
2023-12-20 10:01:01,847:INFO:_master_model_container: 10
2023-12-20 10:01:01,847:INFO:_display_container: 2
2023-12-20 10:01:01,848:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3380, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:01:01,848:INFO:create_model() successfully completed......................................
2023-12-20 10:01:01,997:INFO:SubProcess create_model() end ==================================
2023-12-20 10:01:01,997:INFO:Creating metrics dataframe
2023-12-20 10:01:02,013:INFO:Initializing Linear Discriminant Analysis
2023-12-20 10:01:02,013:INFO:Total runtime is 0.3218906839688619 minutes
2023-12-20 10:01:02,015:INFO:SubProcess create_model() called ==================================
2023-12-20 10:01:02,016:INFO:Initializing create_model()
2023-12-20 10:01:02,016:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:02,016:INFO:Checking exceptions
2023-12-20 10:01:02,016:INFO:Importing libraries
2023-12-20 10:01:02,016:INFO:Copying training dataset
2023-12-20 10:01:02,021:INFO:Defining folds
2023-12-20 10:01:02,021:INFO:Declaring metric variables
2023-12-20 10:01:02,023:INFO:Importing untrained model
2023-12-20 10:01:02,023:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 10:01:02,031:INFO:Starting cross validation
2023-12-20 10:01:02,032:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:01:02,113:INFO:Calculating mean and std
2023-12-20 10:01:02,113:INFO:Creating metrics dataframe
2023-12-20 10:01:02,113:INFO:Uploading results into container
2023-12-20 10:01:02,113:INFO:Uploading model into container now
2023-12-20 10:01:02,113:INFO:_master_model_container: 11
2023-12-20 10:01:02,113:INFO:_display_container: 2
2023-12-20 10:01:02,113:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 10:01:02,113:INFO:create_model() successfully completed......................................
2023-12-20 10:01:02,262:INFO:SubProcess create_model() end ==================================
2023-12-20 10:01:02,262:INFO:Creating metrics dataframe
2023-12-20 10:01:02,266:INFO:Initializing Extra Trees Classifier
2023-12-20 10:01:02,266:INFO:Total runtime is 0.32611745993296304 minutes
2023-12-20 10:01:02,266:INFO:SubProcess create_model() called ==================================
2023-12-20 10:01:02,266:INFO:Initializing create_model()
2023-12-20 10:01:02,266:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:02,266:INFO:Checking exceptions
2023-12-20 10:01:02,266:INFO:Importing libraries
2023-12-20 10:01:02,266:INFO:Copying training dataset
2023-12-20 10:01:02,279:INFO:Defining folds
2023-12-20 10:01:02,279:INFO:Declaring metric variables
2023-12-20 10:01:02,281:INFO:Importing untrained model
2023-12-20 10:01:02,282:INFO:Extra Trees Classifier Imported successfully
2023-12-20 10:01:02,282:INFO:Starting cross validation
2023-12-20 10:01:02,282:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:01:02,699:INFO:Calculating mean and std
2023-12-20 10:01:02,699:INFO:Creating metrics dataframe
2023-12-20 10:01:02,699:INFO:Uploading results into container
2023-12-20 10:01:02,699:INFO:Uploading model into container now
2023-12-20 10:01:02,699:INFO:_master_model_container: 12
2023-12-20 10:01:02,699:INFO:_display_container: 2
2023-12-20 10:01:02,706:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=3380, verbose=0, warm_start=False)
2023-12-20 10:01:02,706:INFO:create_model() successfully completed......................................
2023-12-20 10:01:02,847:INFO:SubProcess create_model() end ==================================
2023-12-20 10:01:02,847:INFO:Creating metrics dataframe
2023-12-20 10:01:02,847:INFO:Initializing Extreme Gradient Boosting
2023-12-20 10:01:02,847:INFO:Total runtime is 0.3357965588569641 minutes
2023-12-20 10:01:02,863:INFO:SubProcess create_model() called ==================================
2023-12-20 10:01:02,863:INFO:Initializing create_model()
2023-12-20 10:01:02,863:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:02,863:INFO:Checking exceptions
2023-12-20 10:01:02,863:INFO:Importing libraries
2023-12-20 10:01:02,863:INFO:Copying training dataset
2023-12-20 10:01:02,865:INFO:Defining folds
2023-12-20 10:01:02,865:INFO:Declaring metric variables
2023-12-20 10:01:02,865:INFO:Importing untrained model
2023-12-20 10:01:02,873:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:01:02,873:INFO:Starting cross validation
2023-12-20 10:01:02,878:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:01:03,528:INFO:Calculating mean and std
2023-12-20 10:01:03,529:INFO:Creating metrics dataframe
2023-12-20 10:01:03,532:INFO:Uploading results into container
2023-12-20 10:01:03,532:INFO:Uploading model into container now
2023-12-20 10:01:03,532:INFO:_master_model_container: 13
2023-12-20 10:01:03,532:INFO:_display_container: 2
2023-12-20 10:01:03,532:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 10:01:03,532:INFO:create_model() successfully completed......................................
2023-12-20 10:01:03,681:INFO:SubProcess create_model() end ==================================
2023-12-20 10:01:03,681:INFO:Creating metrics dataframe
2023-12-20 10:01:03,681:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 10:01:03,681:INFO:Total runtime is 0.34970633586247757 minutes
2023-12-20 10:01:03,681:INFO:SubProcess create_model() called ==================================
2023-12-20 10:01:03,695:INFO:Initializing create_model()
2023-12-20 10:01:03,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:03,695:INFO:Checking exceptions
2023-12-20 10:01:03,695:INFO:Importing libraries
2023-12-20 10:01:03,695:INFO:Copying training dataset
2023-12-20 10:01:03,695:INFO:Defining folds
2023-12-20 10:01:03,695:INFO:Declaring metric variables
2023-12-20 10:01:03,702:INFO:Importing untrained model
2023-12-20 10:01:03,704:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:01:03,704:INFO:Starting cross validation
2023-12-20 10:01:03,704:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:01:06,815:INFO:Calculating mean and std
2023-12-20 10:01:06,815:INFO:Creating metrics dataframe
2023-12-20 10:01:06,815:INFO:Uploading results into container
2023-12-20 10:01:06,830:INFO:Uploading model into container now
2023-12-20 10:01:06,830:INFO:_master_model_container: 14
2023-12-20 10:01:06,830:INFO:_display_container: 2
2023-12-20 10:01:06,830:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:01:06,832:INFO:create_model() successfully completed......................................
2023-12-20 10:01:07,001:INFO:SubProcess create_model() end ==================================
2023-12-20 10:01:07,001:INFO:Creating metrics dataframe
2023-12-20 10:01:07,012:INFO:Initializing Dummy Classifier
2023-12-20 10:01:07,012:INFO:Total runtime is 0.4052170872688293 minutes
2023-12-20 10:01:07,012:INFO:SubProcess create_model() called ==================================
2023-12-20 10:01:07,012:INFO:Initializing create_model()
2023-12-20 10:01:07,012:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000237FE020C10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:07,012:INFO:Checking exceptions
2023-12-20 10:01:07,012:INFO:Importing libraries
2023-12-20 10:01:07,012:INFO:Copying training dataset
2023-12-20 10:01:07,012:INFO:Defining folds
2023-12-20 10:01:07,012:INFO:Declaring metric variables
2023-12-20 10:01:07,030:INFO:Importing untrained model
2023-12-20 10:01:07,031:INFO:Dummy Classifier Imported successfully
2023-12-20 10:01:07,031:INFO:Starting cross validation
2023-12-20 10:01:07,031:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:01:07,082:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:01:07,084:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:01:07,085:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:01:07,085:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:01:07,090:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:01:07,093:INFO:Calculating mean and std
2023-12-20 10:01:07,093:INFO:Creating metrics dataframe
2023-12-20 10:01:07,096:INFO:Uploading results into container
2023-12-20 10:01:07,096:INFO:Uploading model into container now
2023-12-20 10:01:07,096:INFO:_master_model_container: 15
2023-12-20 10:01:07,096:INFO:_display_container: 2
2023-12-20 10:01:07,096:INFO:DummyClassifier(constant=None, random_state=3380, strategy='prior')
2023-12-20 10:01:07,096:INFO:create_model() successfully completed......................................
2023-12-20 10:01:07,247:INFO:SubProcess create_model() end ==================================
2023-12-20 10:01:07,247:INFO:Creating metrics dataframe
2023-12-20 10:01:07,265:INFO:Initializing create_model()
2023-12-20 10:01:07,265:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:07,265:INFO:Checking exceptions
2023-12-20 10:01:07,265:INFO:Importing libraries
2023-12-20 10:01:07,265:INFO:Copying training dataset
2023-12-20 10:01:07,265:INFO:Defining folds
2023-12-20 10:01:07,265:INFO:Declaring metric variables
2023-12-20 10:01:07,265:INFO:Importing untrained model
2023-12-20 10:01:07,265:INFO:Declaring custom model
2023-12-20 10:01:07,265:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:01:07,265:INFO:Cross validation set to False
2023-12-20 10:01:07,265:INFO:Fitting Model
2023-12-20 10:01:07,298:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001246 seconds.
2023-12-20 10:01:07,298:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:01:07,298:INFO:[LightGBM] [Info] Total Bins 1745
2023-12-20 10:01:07,298:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 10:01:07,299:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:01:07,299:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:01:07,299:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:01:07,532:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:01:07,532:INFO:create_model() successfully completed......................................
2023-12-20 10:01:07,731:INFO:Initializing create_model()
2023-12-20 10:01:07,731:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:07,731:INFO:Checking exceptions
2023-12-20 10:01:07,732:INFO:Importing libraries
2023-12-20 10:01:07,732:INFO:Copying training dataset
2023-12-20 10:01:07,736:INFO:Defining folds
2023-12-20 10:01:07,736:INFO:Declaring metric variables
2023-12-20 10:01:07,736:INFO:Importing untrained model
2023-12-20 10:01:07,736:INFO:Declaring custom model
2023-12-20 10:01:07,736:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:01:07,736:INFO:Cross validation set to False
2023-12-20 10:01:07,736:INFO:Fitting Model
2023-12-20 10:01:08,029:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 10:01:08,029:INFO:create_model() successfully completed......................................
2023-12-20 10:01:08,213:INFO:Initializing create_model()
2023-12-20 10:01:08,213:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3380, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:08,213:INFO:Checking exceptions
2023-12-20 10:01:08,222:INFO:Importing libraries
2023-12-20 10:01:08,222:INFO:Copying training dataset
2023-12-20 10:01:08,224:INFO:Defining folds
2023-12-20 10:01:08,224:INFO:Declaring metric variables
2023-12-20 10:01:08,224:INFO:Importing untrained model
2023-12-20 10:01:08,224:INFO:Declaring custom model
2023-12-20 10:01:08,224:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:01:08,224:INFO:Cross validation set to False
2023-12-20 10:01:08,224:INFO:Fitting Model
2023-12-20 10:01:11,777:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3380, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:01:11,777:INFO:create_model() successfully completed......................................
2023-12-20 10:01:11,955:INFO:Initializing create_model()
2023-12-20 10:01:11,955:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=3380, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:11,955:INFO:Checking exceptions
2023-12-20 10:01:11,970:INFO:Importing libraries
2023-12-20 10:01:11,970:INFO:Copying training dataset
2023-12-20 10:01:11,973:INFO:Defining folds
2023-12-20 10:01:11,973:INFO:Declaring metric variables
2023-12-20 10:01:11,973:INFO:Importing untrained model
2023-12-20 10:01:11,973:INFO:Declaring custom model
2023-12-20 10:01:11,978:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:01:11,978:INFO:Cross validation set to False
2023-12-20 10:01:11,978:INFO:Fitting Model
2023-12-20 10:01:12,198:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=3380, verbose=0, warm_start=False)
2023-12-20 10:01:12,198:INFO:create_model() successfully completed......................................
2023-12-20 10:01:12,360:INFO:Initializing create_model()
2023-12-20 10:01:12,375:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=3380), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:12,375:INFO:Checking exceptions
2023-12-20 10:01:12,378:INFO:Importing libraries
2023-12-20 10:01:12,379:INFO:Copying training dataset
2023-12-20 10:01:12,386:INFO:Defining folds
2023-12-20 10:01:12,386:INFO:Declaring metric variables
2023-12-20 10:01:12,386:INFO:Importing untrained model
2023-12-20 10:01:12,386:INFO:Declaring custom model
2023-12-20 10:01:12,386:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:01:12,386:INFO:Cross validation set to False
2023-12-20 10:01:12,386:INFO:Fitting Model
2023-12-20 10:01:12,715:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=3380)
2023-12-20 10:01:12,715:INFO:create_model() successfully completed......................................
2023-12-20 10:01:12,915:INFO:_master_model_container: 15
2023-12-20 10:01:12,915:INFO:_display_container: 2
2023-12-20 10:01:12,929:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=3380, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=3380, verbose=0, warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=3380)]
2023-12-20 10:01:12,929:INFO:compare_models() successfully completed......................................
2023-12-20 10:01:16,479:INFO:Initializing tune_model()
2023-12-20 10:01:16,479:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 10:01:16,479:INFO:Checking exceptions
2023-12-20 10:01:16,505:INFO:Copying training dataset
2023-12-20 10:01:16,511:INFO:Checking base model
2023-12-20 10:01:16,511:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 10:01:16,516:INFO:Declaring metric variables
2023-12-20 10:01:16,519:INFO:Defining Hyperparameters
2023-12-20 10:01:16,663:INFO:Tuning with n_jobs=-1
2023-12-20 10:01:16,663:INFO:Initializing RandomizedSearchCV
2023-12-20 10:01:40,312:INFO:best_params: {'actual_estimator__reg_lambda': 0.2, 'actual_estimator__reg_alpha': 5, 'actual_estimator__num_leaves': 2, 'actual_estimator__n_estimators': 170, 'actual_estimator__min_split_gain': 0.2, 'actual_estimator__min_child_samples': 16, 'actual_estimator__learning_rate': 0.4, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 5, 'actual_estimator__bagging_fraction': 0.4}
2023-12-20 10:01:40,324:INFO:Hyperparameter search completed
2023-12-20 10:01:40,324:INFO:SubProcess create_model() called ==================================
2023-12-20 10:01:40,324:INFO:Initializing create_model()
2023-12-20 10:01:40,324:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000023781BE9310>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.2, 'reg_alpha': 5, 'num_leaves': 2, 'n_estimators': 170, 'min_split_gain': 0.2, 'min_child_samples': 16, 'learning_rate': 0.4, 'feature_fraction': 0.4, 'bagging_freq': 5, 'bagging_fraction': 0.4})
2023-12-20 10:01:40,324:INFO:Checking exceptions
2023-12-20 10:01:40,324:INFO:Importing libraries
2023-12-20 10:01:40,324:INFO:Copying training dataset
2023-12-20 10:01:40,337:INFO:Defining folds
2023-12-20 10:01:40,337:INFO:Declaring metric variables
2023-12-20 10:01:40,337:INFO:Importing untrained model
2023-12-20 10:01:40,337:INFO:Declaring custom model
2023-12-20 10:01:40,345:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:01:40,345:INFO:Starting cross validation
2023-12-20 10:01:40,345:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:01:40,996:INFO:Calculating mean and std
2023-12-20 10:01:40,998:INFO:Creating metrics dataframe
2023-12-20 10:01:41,003:INFO:Finalizing model
2023-12-20 10:01:41,032:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-12-20 10:01:41,033:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2023-12-20 10:01:41,033:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 10:01:41,036:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-12-20 10:01:41,044:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2023-12-20 10:01:41,044:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 10:01:41,049:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002553 seconds.
2023-12-20 10:01:41,049:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:01:41,049:INFO:[LightGBM] [Info] Total Bins 1749
2023-12-20 10:01:41,049:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 51
2023-12-20 10:01:41,051:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:01:41,051:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:01:41,051:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:01:41,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,145:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,162:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:01:41,237:INFO:Uploading results into container
2023-12-20 10:01:41,238:INFO:Uploading model into container now
2023-12-20 10:01:41,238:INFO:_master_model_container: 16
2023-12-20 10:01:41,238:INFO:_display_container: 3
2023-12-20 10:01:41,238:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.4, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=170, n_jobs=-1, num_leaves=2, objective=None,
               random_state=3380, reg_alpha=5, reg_lambda=0.2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:01:41,238:INFO:create_model() successfully completed......................................
2023-12-20 10:01:41,432:INFO:SubProcess create_model() end ==================================
2023-12-20 10:01:41,432:INFO:choose_better activated
2023-12-20 10:01:41,432:INFO:SubProcess create_model() called ==================================
2023-12-20 10:01:41,432:INFO:Initializing create_model()
2023-12-20 10:01:41,432:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:01:41,432:INFO:Checking exceptions
2023-12-20 10:01:41,432:INFO:Importing libraries
2023-12-20 10:01:41,432:INFO:Copying training dataset
2023-12-20 10:01:41,448:INFO:Defining folds
2023-12-20 10:01:41,448:INFO:Declaring metric variables
2023-12-20 10:01:41,448:INFO:Importing untrained model
2023-12-20 10:01:41,448:INFO:Declaring custom model
2023-12-20 10:01:41,448:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:01:41,448:INFO:Starting cross validation
2023-12-20 10:01:41,448:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:01:44,749:INFO:Calculating mean and std
2023-12-20 10:01:44,749:INFO:Creating metrics dataframe
2023-12-20 10:01:44,749:INFO:Finalizing model
2023-12-20 10:01:44,795:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002625 seconds.
2023-12-20 10:01:44,795:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:01:44,795:INFO:[LightGBM] [Info] Total Bins 1745
2023-12-20 10:01:44,795:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 10:01:44,795:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:01:44,795:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:01:44,795:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:01:45,153:INFO:Uploading results into container
2023-12-20 10:01:45,153:INFO:Uploading model into container now
2023-12-20 10:01:45,153:INFO:_master_model_container: 17
2023-12-20 10:01:45,153:INFO:_display_container: 4
2023-12-20 10:01:45,153:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:01:45,153:INFO:create_model() successfully completed......................................
2023-12-20 10:01:45,331:INFO:SubProcess create_model() end ==================================
2023-12-20 10:01:45,344:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8232
2023-12-20 10:01:45,344:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.4,
               importance_type='split', learning_rate=0.4, max_depth=-1,
               min_child_samples=16, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=170, n_jobs=-1, num_leaves=2, objective=None,
               random_state=3380, reg_alpha=5, reg_lambda=0.2, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8221
2023-12-20 10:01:45,344:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 10:01:45,344:INFO:choose_better completed
2023-12-20 10:01:45,344:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-12-20 10:01:45,344:INFO:_master_model_container: 17
2023-12-20 10:01:45,344:INFO:_display_container: 3
2023-12-20 10:01:45,344:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:01:45,344:INFO:tune_model() successfully completed......................................
2023-12-20 10:01:52,884:INFO:Initializing evaluate_model()
2023-12-20 10:01:52,884:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 10:01:52,900:INFO:Initializing plot_model()
2023-12-20 10:01:52,900:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:01:52,900:INFO:Checking exceptions
2023-12-20 10:01:52,903:INFO:Preloading libraries
2023-12-20 10:01:52,914:INFO:Copying training dataset
2023-12-20 10:01:52,914:INFO:Plot type: pipeline
2023-12-20 10:01:52,982:INFO:Visual Rendered Successfully
2023-12-20 10:01:53,145:INFO:plot_model() successfully completed......................................
2023-12-20 10:03:27,091:INFO:Initializing plot_model()
2023-12-20 10:03:27,091:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=threshold, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:03:27,091:INFO:Checking exceptions
2023-12-20 10:03:31,747:INFO:Initializing plot_model()
2023-12-20 10:03:31,747:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:03:31,748:INFO:Checking exceptions
2023-12-20 10:03:31,750:INFO:Preloading libraries
2023-12-20 10:03:31,763:INFO:Copying training dataset
2023-12-20 10:03:31,763:INFO:Plot type: feature
2023-12-20 10:03:31,764:WARNING:No coef_ found. Trying feature_importances_
2023-12-20 10:03:31,896:INFO:Visual Rendered Successfully
2023-12-20 10:03:32,073:INFO:plot_model() successfully completed......................................
2023-12-20 10:03:32,842:INFO:Initializing plot_model()
2023-12-20 10:03:32,842:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=dimension, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:03:32,842:INFO:Checking exceptions
2023-12-20 10:03:32,846:INFO:Preloading libraries
2023-12-20 10:03:32,852:INFO:Copying training dataset
2023-12-20 10:03:32,852:INFO:Plot type: dimension
2023-12-20 10:03:32,932:INFO:Fitting StandardScaler()
2023-12-20 10:03:32,963:INFO:Fitting PCA()
2023-12-20 10:03:33,052:INFO:Fitting & Transforming Model
2023-12-20 10:03:33,288:INFO:Visual Rendered Successfully
2023-12-20 10:03:33,438:INFO:plot_model() successfully completed......................................
2023-12-20 10:03:33,565:INFO:Initializing plot_model()
2023-12-20 10:03:33,565:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:03:33,565:INFO:Checking exceptions
2023-12-20 10:03:33,569:INFO:Preloading libraries
2023-12-20 10:03:33,586:INFO:Copying training dataset
2023-12-20 10:03:33,586:INFO:Plot type: feature
2023-12-20 10:03:33,587:WARNING:No coef_ found. Trying feature_importances_
2023-12-20 10:03:33,704:INFO:Visual Rendered Successfully
2023-12-20 10:03:33,857:INFO:plot_model() successfully completed......................................
2023-12-20 10:03:47,156:INFO:Initializing plot_model()
2023-12-20 10:03:47,156:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature_all, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:03:47,156:INFO:Checking exceptions
2023-12-20 10:03:47,160:INFO:Preloading libraries
2023-12-20 10:03:47,174:INFO:Copying training dataset
2023-12-20 10:03:47,174:INFO:Plot type: feature_all
2023-12-20 10:03:47,208:WARNING:No coef_ found. Trying feature_importances_
2023-12-20 10:03:47,476:INFO:Visual Rendered Successfully
2023-12-20 10:03:47,626:INFO:plot_model() successfully completed......................................
2023-12-20 10:03:59,493:INFO:Initializing plot_model()
2023-12-20 10:03:59,494:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pr, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:03:59,494:INFO:Checking exceptions
2023-12-20 10:03:59,499:INFO:Preloading libraries
2023-12-20 10:03:59,509:INFO:Copying training dataset
2023-12-20 10:03:59,509:INFO:Plot type: pr
2023-12-20 10:03:59,601:INFO:Fitting Model
2023-12-20 10:03:59,618:INFO:[LightGBM] [Info] Number of positive: 3972, number of negative: 2352
2023-12-20 10:03:59,620:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000915 seconds.
2023-12-20 10:03:59,620:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:03:59,620:INFO:[LightGBM] [Info] Total Bins 1745
2023-12-20 10:03:59,620:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 10:03:59,621:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.628083 -> initscore=0.524004
2023-12-20 10:03:59,621:INFO:[LightGBM] [Info] Start training from score 0.524004
2023-12-20 10:03:59,690:INFO:[LightGBM] [Info] Number of positive: 220, number of negative: 6104
2023-12-20 10:03:59,706:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001139 seconds.
2023-12-20 10:03:59,706:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:03:59,706:INFO:[LightGBM] [Info] Total Bins 1745
2023-12-20 10:03:59,706:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 10:03:59,706:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034788 -> initscore=-3.323072
2023-12-20 10:03:59,706:INFO:[LightGBM] [Info] Start training from score -3.323072
2023-12-20 10:03:59,816:INFO:[LightGBM] [Info] Number of positive: 2132, number of negative: 4192
2023-12-20 10:03:59,816:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001096 seconds.
2023-12-20 10:03:59,816:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:03:59,816:INFO:[LightGBM] [Info] Total Bins 1745
2023-12-20 10:03:59,816:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 10:03:59,816:INFO:[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.337128 -> initscore=-0.676117
2023-12-20 10:03:59,816:INFO:[LightGBM] [Info] Start training from score -0.676117
2023-12-20 10:03:59,894:INFO:Scoring test/hold-out set
2023-12-20 10:04:00,068:INFO:Visual Rendered Successfully
2023-12-20 10:04:00,229:INFO:plot_model() successfully completed......................................
2023-12-20 10:04:00,473:INFO:Initializing plot_model()
2023-12-20 10:04:00,473:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=feature, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:04:00,473:INFO:Checking exceptions
2023-12-20 10:04:00,476:INFO:Preloading libraries
2023-12-20 10:04:00,488:INFO:Copying training dataset
2023-12-20 10:04:00,488:INFO:Plot type: feature
2023-12-20 10:04:00,489:WARNING:No coef_ found. Trying feature_importances_
2023-12-20 10:04:00,638:INFO:Visual Rendered Successfully
2023-12-20 10:04:00,785:INFO:plot_model() successfully completed......................................
2023-12-20 10:04:02,130:INFO:Initializing plot_model()
2023-12-20 10:04:02,130:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=boundary, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:04:02,130:INFO:Checking exceptions
2023-12-20 10:04:02,134:INFO:Preloading libraries
2023-12-20 10:04:02,148:INFO:Copying training dataset
2023-12-20 10:04:02,148:INFO:Plot type: boundary
2023-12-20 10:04:02,229:INFO:Fitting StandardScaler()
2023-12-20 10:04:02,235:INFO:Fitting PCA()
2023-12-20 10:04:02,292:INFO:Fitting Model
2023-12-20 10:04:02,292:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000077 seconds.
2023-12-20 10:04:02,292:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:04:02,292:INFO:[LightGBM] [Info] Total Bins 510
2023-12-20 10:04:02,292:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 2
2023-12-20 10:04:02,292:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:04:02,292:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:04:02,292:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:04:02,684:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\IPython\core\pylabtools.py:152: UserWarning: Glyph 8722 (\N{MINUS SIGN}) missing from current font.
  fig.canvas.print_figure(bytes_io, **kw)

2023-12-20 10:04:03,167:INFO:Visual Rendered Successfully
2023-12-20 10:04:03,376:INFO:plot_model() successfully completed......................................
2023-12-20 10:04:05,346:INFO:Initializing plot_model()
2023-12-20 10:04:05,346:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=calibration, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:04:05,346:INFO:Checking exceptions
2023-12-20 10:04:06,152:INFO:Initializing plot_model()
2023-12-20 10:04:06,152:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=manifold, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:04:06,153:INFO:Checking exceptions
2023-12-20 10:04:08,061:INFO:Initializing plot_model()
2023-12-20 10:04:08,061:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=learning, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:04:08,061:INFO:Checking exceptions
2023-12-20 10:04:08,063:INFO:Preloading libraries
2023-12-20 10:04:08,077:INFO:Copying training dataset
2023-12-20 10:04:08,077:INFO:Plot type: learning
2023-12-20 10:04:08,178:INFO:Fitting Model
2023-12-20 10:04:37,986:INFO:Visual Rendered Successfully
2023-12-20 10:04:38,158:INFO:plot_model() successfully completed......................................
2023-12-20 10:04:38,241:INFO:Initializing plot_model()
2023-12-20 10:04:38,241:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000023781BAB450>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=3380, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:04:38,241:INFO:Checking exceptions
2023-12-20 10:04:38,244:INFO:Preloading libraries
2023-12-20 10:04:38,250:INFO:Copying training dataset
2023-12-20 10:04:38,250:INFO:Plot type: auc
2023-12-20 10:04:38,372:INFO:Fitting Model
2023-12-20 10:04:38,372:INFO:Scoring test/hold-out set
2023-12-20 10:04:38,538:INFO:Visual Rendered Successfully
2023-12-20 10:04:38,707:INFO:plot_model() successfully completed......................................
2023-12-20 10:17:27,330:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:17:27,330:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:17:27,330:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:17:27,330:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:17:30,413:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,413:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,415:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,415:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,415:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,415:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,415:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,420:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,420:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:17:30,421:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\2482900554.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,828:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,829:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,830:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,830:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,831:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,832:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,833:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,833:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,833:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:19:29,833:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_2348\1028375931.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:53,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:20:53,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:20:53,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:20:53,493:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,627:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_24452\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:20:56,993:INFO:PyCaret ClassificationExperiment
2023-12-20 10:20:56,993:INFO:Logging name: clf-default-name
2023-12-20 10:20:56,993:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 10:20:56,993:INFO:version 3.2.0
2023-12-20 10:20:56,993:INFO:Initializing setup()
2023-12-20 10:20:57,009:INFO:self.USI: 3489
2023-12-20 10:20:57,009:INFO:self._variable_keys: {'pipeline', 'X', 'fold_shuffle_param', 'is_multiclass', 'log_plots_param', 'seed', 'exp_id', 'USI', 'fold_groups_param', 'idx', 'exp_name_log', '_available_plots', 'memory', 'fix_imbalance', 'y', 'data', 'logging_param', 'y_train', 'X_train', 'html_param', 'y_test', '_ml_usecase', 'target_param', 'X_test', 'fold_generator', 'n_jobs_param', 'gpu_param', 'gpu_n_jobs_param'}
2023-12-20 10:20:57,009:INFO:Checking environment
2023-12-20 10:20:57,009:INFO:python_version: 3.11.5
2023-12-20 10:20:57,009:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 10:20:57,009:INFO:machine: AMD64
2023-12-20 10:20:57,009:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 10:20:57,009:INFO:Memory: svmem(total=16718413824, available=6032330752, percent=63.9, used=10686083072, free=6032330752)
2023-12-20 10:20:57,009:INFO:Physical Core: 12
2023-12-20 10:20:57,009:INFO:Logical Core: 16
2023-12-20 10:20:57,009:INFO:Checking libraries
2023-12-20 10:20:57,009:INFO:System:
2023-12-20 10:20:57,009:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 10:20:57,009:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 10:20:57,009:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 10:20:57,009:INFO:PyCaret required dependencies:
2023-12-20 10:20:57,745:INFO:                 pip: 23.2.1
2023-12-20 10:20:57,745:INFO:          setuptools: 68.0.0
2023-12-20 10:20:57,745:INFO:             pycaret: 3.2.0
2023-12-20 10:20:57,745:INFO:             IPython: 8.15.0
2023-12-20 10:20:57,745:INFO:          ipywidgets: 8.0.4
2023-12-20 10:20:57,745:INFO:                tqdm: 4.65.0
2023-12-20 10:20:57,745:INFO:               numpy: 1.24.3
2023-12-20 10:20:57,745:INFO:              pandas: 1.5.3
2023-12-20 10:20:57,745:INFO:              jinja2: 3.1.2
2023-12-20 10:20:57,745:INFO:               scipy: 1.10.1
2023-12-20 10:20:57,745:INFO:              joblib: 1.2.0
2023-12-20 10:20:57,745:INFO:             sklearn: 1.2.1
2023-12-20 10:20:57,745:INFO:                pyod: 1.1.2
2023-12-20 10:20:57,745:INFO:            imblearn: 0.11.0
2023-12-20 10:20:57,745:INFO:   category_encoders: 2.6.3
2023-12-20 10:20:57,745:INFO:            lightgbm: 4.1.0
2023-12-20 10:20:57,745:INFO:               numba: 0.57.1
2023-12-20 10:20:57,745:INFO:            requests: 2.31.0
2023-12-20 10:20:57,745:INFO:          matplotlib: 3.6.0
2023-12-20 10:20:57,745:INFO:          scikitplot: 0.3.7
2023-12-20 10:20:57,745:INFO:         yellowbrick: 1.5
2023-12-20 10:20:57,745:INFO:              plotly: 5.9.0
2023-12-20 10:20:57,745:INFO:    plotly-resampler: Not installed
2023-12-20 10:20:57,745:INFO:             kaleido: 0.2.1
2023-12-20 10:20:57,745:INFO:           schemdraw: 0.15
2023-12-20 10:20:57,745:INFO:         statsmodels: 0.14.0
2023-12-20 10:20:57,745:INFO:              sktime: 0.21.1
2023-12-20 10:20:57,745:INFO:               tbats: 1.1.3
2023-12-20 10:20:57,745:INFO:            pmdarima: 2.0.4
2023-12-20 10:20:57,745:INFO:              psutil: 5.9.0
2023-12-20 10:20:57,745:INFO:          markupsafe: 2.1.1
2023-12-20 10:20:57,745:INFO:             pickle5: Not installed
2023-12-20 10:20:57,745:INFO:         cloudpickle: 2.2.1
2023-12-20 10:20:57,745:INFO:         deprecation: 2.1.0
2023-12-20 10:20:57,745:INFO:              xxhash: 2.0.2
2023-12-20 10:20:57,745:INFO:           wurlitzer: Not installed
2023-12-20 10:20:57,745:INFO:PyCaret optional dependencies:
2023-12-20 10:20:57,785:INFO:                shap: Not installed
2023-12-20 10:20:57,785:INFO:           interpret: Not installed
2023-12-20 10:20:57,785:INFO:                umap: Not installed
2023-12-20 10:20:57,785:INFO:     ydata_profiling: Not installed
2023-12-20 10:20:57,785:INFO:  explainerdashboard: Not installed
2023-12-20 10:20:57,785:INFO:             autoviz: Not installed
2023-12-20 10:20:57,785:INFO:           fairlearn: Not installed
2023-12-20 10:20:57,785:INFO:          deepchecks: Not installed
2023-12-20 10:20:57,785:INFO:             xgboost: 2.0.2
2023-12-20 10:20:57,785:INFO:            catboost: Not installed
2023-12-20 10:20:57,785:INFO:              kmodes: Not installed
2023-12-20 10:20:57,785:INFO:             mlxtend: Not installed
2023-12-20 10:20:57,785:INFO:       statsforecast: Not installed
2023-12-20 10:20:57,785:INFO:        tune_sklearn: Not installed
2023-12-20 10:20:57,785:INFO:                 ray: Not installed
2023-12-20 10:20:57,785:INFO:            hyperopt: Not installed
2023-12-20 10:20:57,785:INFO:              optuna: Not installed
2023-12-20 10:20:57,785:INFO:               skopt: Not installed
2023-12-20 10:20:57,785:INFO:              mlflow: Not installed
2023-12-20 10:20:57,785:INFO:              gradio: Not installed
2023-12-20 10:20:57,785:INFO:             fastapi: Not installed
2023-12-20 10:20:57,785:INFO:             uvicorn: Not installed
2023-12-20 10:20:57,785:INFO:              m2cgen: Not installed
2023-12-20 10:20:57,785:INFO:           evidently: Not installed
2023-12-20 10:20:57,785:INFO:               fugue: Not installed
2023-12-20 10:20:57,785:INFO:           streamlit: Not installed
2023-12-20 10:20:57,785:INFO:             prophet: Not installed
2023-12-20 10:20:57,785:INFO:None
2023-12-20 10:20:57,785:INFO:Set up data.
2023-12-20 10:20:57,794:INFO:Set up folding strategy.
2023-12-20 10:20:57,794:INFO:Set up train/test split.
2023-12-20 10:20:57,794:INFO:Set up index.
2023-12-20 10:20:57,794:INFO:Assigning column types.
2023-12-20 10:20:57,809:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 10:20:57,834:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:20:57,835:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:20:57,853:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:20:57,859:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:20:57,885:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:20:57,886:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:20:57,898:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:20:57,898:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:20:57,898:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 10:20:57,928:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:20:57,943:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:20:57,943:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:20:57,963:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:20:57,975:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:20:57,975:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:20:57,975:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 10:20:58,028:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:20:58,028:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:20:58,060:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:20:58,076:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:20:58,076:INFO:Preparing preprocessing pipeline...
2023-12-20 10:20:58,076:INFO:Set up simple imputation.
2023-12-20 10:20:58,076:INFO:Set up column name cleaning.
2023-12-20 10:20:58,114:INFO:Finished creating preprocessing pipeline.
2023-12-20 10:20:58,114:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_Y',
                                             'He...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 10:20:58,114:INFO:Creating final display dataframe.
2023-12-20 10:20:58,177:INFO:Setup _display_container:                     Description             Value
0                    Session id              2084
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 53)
4        Transformed data shape        (7905, 53)
5   Transformed train set shape        (6324, 53)
6    Transformed test set shape        (1581, 53)
7              Numeric features                52
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              3489
2023-12-20 10:20:58,233:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:20:58,243:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:20:58,282:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:20:58,282:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:20:58,282:INFO:setup() successfully completed in 1.38s...............
2023-12-20 10:20:58,304:INFO:Initializing compare_models()
2023-12-20 10:20:58,304:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 10:20:58,304:INFO:Checking exceptions
2023-12-20 10:20:58,313:INFO:Preparing display monitor
2023-12-20 10:20:58,361:INFO:Initializing Logistic Regression
2023-12-20 10:20:58,361:INFO:Total runtime is 0.0 minutes
2023-12-20 10:20:58,366:INFO:SubProcess create_model() called ==================================
2023-12-20 10:20:58,367:INFO:Initializing create_model()
2023-12-20 10:20:58,367:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:20:58,367:INFO:Checking exceptions
2023-12-20 10:20:58,367:INFO:Importing libraries
2023-12-20 10:20:58,367:INFO:Copying training dataset
2023-12-20 10:20:58,377:INFO:Defining folds
2023-12-20 10:20:58,377:INFO:Declaring metric variables
2023-12-20 10:20:58,381:INFO:Importing untrained model
2023-12-20 10:20:58,385:INFO:Logistic Regression Imported successfully
2023-12-20 10:20:58,390:INFO:Starting cross validation
2023-12-20 10:20:58,391:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:02,093:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:02,156:INFO:Calculating mean and std
2023-12-20 10:21:02,156:INFO:Creating metrics dataframe
2023-12-20 10:21:02,160:INFO:Uploading results into container
2023-12-20 10:21:02,160:INFO:Uploading model into container now
2023-12-20 10:21:02,160:INFO:_master_model_container: 1
2023-12-20 10:21:02,160:INFO:_display_container: 2
2023-12-20 10:21:02,160:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2084, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 10:21:02,160:INFO:create_model() successfully completed......................................
2023-12-20 10:21:02,227:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:02,227:INFO:Creating metrics dataframe
2023-12-20 10:21:02,227:INFO:Initializing K Neighbors Classifier
2023-12-20 10:21:02,227:INFO:Total runtime is 0.064425528049469 minutes
2023-12-20 10:21:02,227:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:02,227:INFO:Initializing create_model()
2023-12-20 10:21:02,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:02,227:INFO:Checking exceptions
2023-12-20 10:21:02,227:INFO:Importing libraries
2023-12-20 10:21:02,227:INFO:Copying training dataset
2023-12-20 10:21:02,243:INFO:Defining folds
2023-12-20 10:21:02,243:INFO:Declaring metric variables
2023-12-20 10:21:02,243:INFO:Importing untrained model
2023-12-20 10:21:02,243:INFO:K Neighbors Classifier Imported successfully
2023-12-20 10:21:02,243:INFO:Starting cross validation
2023-12-20 10:21:02,243:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:05,346:INFO:Calculating mean and std
2023-12-20 10:21:05,346:INFO:Creating metrics dataframe
2023-12-20 10:21:05,346:INFO:Uploading results into container
2023-12-20 10:21:05,346:INFO:Uploading model into container now
2023-12-20 10:21:05,359:INFO:_master_model_container: 2
2023-12-20 10:21:05,359:INFO:_display_container: 2
2023-12-20 10:21:05,359:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 10:21:05,361:INFO:create_model() successfully completed......................................
2023-12-20 10:21:05,427:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:05,427:INFO:Creating metrics dataframe
2023-12-20 10:21:05,442:INFO:Initializing Naive Bayes
2023-12-20 10:21:05,442:INFO:Total runtime is 0.118006698290507 minutes
2023-12-20 10:21:05,442:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:05,442:INFO:Initializing create_model()
2023-12-20 10:21:05,442:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:05,442:INFO:Checking exceptions
2023-12-20 10:21:05,442:INFO:Importing libraries
2023-12-20 10:21:05,442:INFO:Copying training dataset
2023-12-20 10:21:05,459:INFO:Defining folds
2023-12-20 10:21:05,459:INFO:Declaring metric variables
2023-12-20 10:21:05,462:INFO:Importing untrained model
2023-12-20 10:21:05,464:INFO:Naive Bayes Imported successfully
2023-12-20 10:21:05,464:INFO:Starting cross validation
2023-12-20 10:21:05,464:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:08,438:INFO:Calculating mean and std
2023-12-20 10:21:08,438:INFO:Creating metrics dataframe
2023-12-20 10:21:08,453:INFO:Uploading results into container
2023-12-20 10:21:08,453:INFO:Uploading model into container now
2023-12-20 10:21:08,453:INFO:_master_model_container: 3
2023-12-20 10:21:08,453:INFO:_display_container: 2
2023-12-20 10:21:08,453:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 10:21:08,453:INFO:create_model() successfully completed......................................
2023-12-20 10:21:08,535:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:08,535:INFO:Creating metrics dataframe
2023-12-20 10:21:08,548:INFO:Initializing Decision Tree Classifier
2023-12-20 10:21:08,548:INFO:Total runtime is 0.1697840134302775 minutes
2023-12-20 10:21:08,550:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:08,550:INFO:Initializing create_model()
2023-12-20 10:21:08,551:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:08,551:INFO:Checking exceptions
2023-12-20 10:21:08,551:INFO:Importing libraries
2023-12-20 10:21:08,551:INFO:Copying training dataset
2023-12-20 10:21:08,551:INFO:Defining folds
2023-12-20 10:21:08,551:INFO:Declaring metric variables
2023-12-20 10:21:08,560:INFO:Importing untrained model
2023-12-20 10:21:08,561:INFO:Decision Tree Classifier Imported successfully
2023-12-20 10:21:08,563:INFO:Starting cross validation
2023-12-20 10:21:08,563:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:10,823:INFO:Calculating mean and std
2023-12-20 10:21:10,823:INFO:Creating metrics dataframe
2023-12-20 10:21:10,839:INFO:Uploading results into container
2023-12-20 10:21:10,841:INFO:Uploading model into container now
2023-12-20 10:21:10,841:INFO:_master_model_container: 4
2023-12-20 10:21:10,841:INFO:_display_container: 2
2023-12-20 10:21:10,841:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2084, splitter='best')
2023-12-20 10:21:10,841:INFO:create_model() successfully completed......................................
2023-12-20 10:21:10,908:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:10,908:INFO:Creating metrics dataframe
2023-12-20 10:21:10,910:INFO:Initializing SVM - Linear Kernel
2023-12-20 10:21:10,910:INFO:Total runtime is 0.20914607445398964 minutes
2023-12-20 10:21:10,910:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:10,910:INFO:Initializing create_model()
2023-12-20 10:21:10,910:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:10,910:INFO:Checking exceptions
2023-12-20 10:21:10,910:INFO:Importing libraries
2023-12-20 10:21:10,910:INFO:Copying training dataset
2023-12-20 10:21:10,910:INFO:Defining folds
2023-12-20 10:21:10,910:INFO:Declaring metric variables
2023-12-20 10:21:10,926:INFO:Importing untrained model
2023-12-20 10:21:10,926:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 10:21:10,931:INFO:Starting cross validation
2023-12-20 10:21:10,931:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:11,037:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:21:11,037:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:21:11,053:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:21:11,053:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:21:11,053:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:21:11,053:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:11,053:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:11,053:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:11,053:INFO:Calculating mean and std
2023-12-20 10:21:11,053:INFO:Creating metrics dataframe
2023-12-20 10:21:11,053:INFO:Uploading results into container
2023-12-20 10:21:11,053:INFO:Uploading model into container now
2023-12-20 10:21:11,053:INFO:_master_model_container: 5
2023-12-20 10:21:11,053:INFO:_display_container: 2
2023-12-20 10:21:11,065:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2084, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 10:21:11,065:INFO:create_model() successfully completed......................................
2023-12-20 10:21:11,127:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:11,127:INFO:Creating metrics dataframe
2023-12-20 10:21:11,127:INFO:Initializing Ridge Classifier
2023-12-20 10:21:11,127:INFO:Total runtime is 0.2127546668052673 minutes
2023-12-20 10:21:11,127:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:11,127:INFO:Initializing create_model()
2023-12-20 10:21:11,127:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:11,127:INFO:Checking exceptions
2023-12-20 10:21:11,127:INFO:Importing libraries
2023-12-20 10:21:11,127:INFO:Copying training dataset
2023-12-20 10:21:11,142:INFO:Defining folds
2023-12-20 10:21:11,142:INFO:Declaring metric variables
2023-12-20 10:21:11,142:INFO:Importing untrained model
2023-12-20 10:21:11,142:INFO:Ridge Classifier Imported successfully
2023-12-20 10:21:11,142:INFO:Starting cross validation
2023-12-20 10:21:11,142:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:11,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:11,210:INFO:Calculating mean and std
2023-12-20 10:21:11,211:INFO:Creating metrics dataframe
2023-12-20 10:21:11,213:INFO:Uploading results into container
2023-12-20 10:21:11,213:INFO:Uploading model into container now
2023-12-20 10:21:11,213:INFO:_master_model_container: 6
2023-12-20 10:21:11,213:INFO:_display_container: 2
2023-12-20 10:21:11,213:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=2084, solver='auto',
                tol=0.0001)
2023-12-20 10:21:11,213:INFO:create_model() successfully completed......................................
2023-12-20 10:21:11,277:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:11,277:INFO:Creating metrics dataframe
2023-12-20 10:21:11,277:INFO:Initializing Random Forest Classifier
2023-12-20 10:21:11,277:INFO:Total runtime is 0.2152525663375854 minutes
2023-12-20 10:21:11,292:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:11,292:INFO:Initializing create_model()
2023-12-20 10:21:11,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:11,292:INFO:Checking exceptions
2023-12-20 10:21:11,292:INFO:Importing libraries
2023-12-20 10:21:11,292:INFO:Copying training dataset
2023-12-20 10:21:11,292:INFO:Defining folds
2023-12-20 10:21:11,292:INFO:Declaring metric variables
2023-12-20 10:21:11,292:INFO:Importing untrained model
2023-12-20 10:21:11,292:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:21:11,308:INFO:Starting cross validation
2023-12-20 10:21:11,308:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:11,777:INFO:Calculating mean and std
2023-12-20 10:21:11,777:INFO:Creating metrics dataframe
2023-12-20 10:21:11,793:INFO:Uploading results into container
2023-12-20 10:21:11,793:INFO:Uploading model into container now
2023-12-20 10:21:11,793:INFO:_master_model_container: 7
2023-12-20 10:21:11,794:INFO:_display_container: 2
2023-12-20 10:21:11,794:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2084, verbose=0, warm_start=False)
2023-12-20 10:21:11,794:INFO:create_model() successfully completed......................................
2023-12-20 10:21:11,843:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:11,843:INFO:Creating metrics dataframe
2023-12-20 10:21:11,866:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 10:21:11,866:INFO:Total runtime is 0.22506924072901405 minutes
2023-12-20 10:21:11,866:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:11,866:INFO:Initializing create_model()
2023-12-20 10:21:11,866:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:11,866:INFO:Checking exceptions
2023-12-20 10:21:11,866:INFO:Importing libraries
2023-12-20 10:21:11,866:INFO:Copying training dataset
2023-12-20 10:21:11,866:INFO:Defining folds
2023-12-20 10:21:11,866:INFO:Declaring metric variables
2023-12-20 10:21:11,875:INFO:Importing untrained model
2023-12-20 10:21:11,875:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 10:21:11,875:INFO:Starting cross validation
2023-12-20 10:21:11,875:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:11,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:21:11,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:21:11,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:21:11,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:21:11,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:21:11,937:INFO:Calculating mean and std
2023-12-20 10:21:11,937:INFO:Creating metrics dataframe
2023-12-20 10:21:11,940:INFO:Uploading results into container
2023-12-20 10:21:11,940:INFO:Uploading model into container now
2023-12-20 10:21:11,940:INFO:_master_model_container: 8
2023-12-20 10:21:11,940:INFO:_display_container: 2
2023-12-20 10:21:11,941:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 10:21:11,941:INFO:create_model() successfully completed......................................
2023-12-20 10:21:12,006:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:12,006:INFO:Creating metrics dataframe
2023-12-20 10:21:12,006:INFO:Initializing Ada Boost Classifier
2023-12-20 10:21:12,006:INFO:Total runtime is 0.22741727828979488 minutes
2023-12-20 10:21:12,006:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:12,006:INFO:Initializing create_model()
2023-12-20 10:21:12,006:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:12,006:INFO:Checking exceptions
2023-12-20 10:21:12,006:INFO:Importing libraries
2023-12-20 10:21:12,006:INFO:Copying training dataset
2023-12-20 10:21:12,022:INFO:Defining folds
2023-12-20 10:21:12,022:INFO:Declaring metric variables
2023-12-20 10:21:12,026:INFO:Importing untrained model
2023-12-20 10:21:12,026:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:21:12,026:INFO:Starting cross validation
2023-12-20 10:21:12,026:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:12,366:INFO:Calculating mean and std
2023-12-20 10:21:12,367:INFO:Creating metrics dataframe
2023-12-20 10:21:12,367:INFO:Uploading results into container
2023-12-20 10:21:12,367:INFO:Uploading model into container now
2023-12-20 10:21:12,367:INFO:_master_model_container: 9
2023-12-20 10:21:12,367:INFO:_display_container: 2
2023-12-20 10:21:12,367:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=2084)
2023-12-20 10:21:12,367:INFO:create_model() successfully completed......................................
2023-12-20 10:21:12,425:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:12,425:INFO:Creating metrics dataframe
2023-12-20 10:21:12,441:INFO:Initializing Gradient Boosting Classifier
2023-12-20 10:21:12,441:INFO:Total runtime is 0.2346562027931213 minutes
2023-12-20 10:21:12,441:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:12,441:INFO:Initializing create_model()
2023-12-20 10:21:12,441:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:12,441:INFO:Checking exceptions
2023-12-20 10:21:12,441:INFO:Importing libraries
2023-12-20 10:21:12,441:INFO:Copying training dataset
2023-12-20 10:21:12,441:INFO:Defining folds
2023-12-20 10:21:12,441:INFO:Declaring metric variables
2023-12-20 10:21:12,456:INFO:Importing untrained model
2023-12-20 10:21:12,456:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:21:12,456:INFO:Starting cross validation
2023-12-20 10:21:12,456:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:15,856:INFO:Calculating mean and std
2023-12-20 10:21:15,856:INFO:Creating metrics dataframe
2023-12-20 10:21:15,870:INFO:Uploading results into container
2023-12-20 10:21:15,870:INFO:Uploading model into container now
2023-12-20 10:21:15,871:INFO:_master_model_container: 10
2023-12-20 10:21:15,871:INFO:_display_container: 2
2023-12-20 10:21:15,871:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2084, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:21:15,871:INFO:create_model() successfully completed......................................
2023-12-20 10:21:15,941:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:15,941:INFO:Creating metrics dataframe
2023-12-20 10:21:15,966:INFO:Initializing Linear Discriminant Analysis
2023-12-20 10:21:15,966:INFO:Total runtime is 0.29341374635696404 minutes
2023-12-20 10:21:15,966:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:15,966:INFO:Initializing create_model()
2023-12-20 10:21:15,966:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:15,966:INFO:Checking exceptions
2023-12-20 10:21:15,966:INFO:Importing libraries
2023-12-20 10:21:15,966:INFO:Copying training dataset
2023-12-20 10:21:15,972:INFO:Defining folds
2023-12-20 10:21:15,972:INFO:Declaring metric variables
2023-12-20 10:21:15,972:INFO:Importing untrained model
2023-12-20 10:21:15,980:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 10:21:15,980:INFO:Starting cross validation
2023-12-20 10:21:15,980:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:16,058:INFO:Calculating mean and std
2023-12-20 10:21:16,058:INFO:Creating metrics dataframe
2023-12-20 10:21:16,058:INFO:Uploading results into container
2023-12-20 10:21:16,058:INFO:Uploading model into container now
2023-12-20 10:21:16,058:INFO:_master_model_container: 11
2023-12-20 10:21:16,058:INFO:_display_container: 2
2023-12-20 10:21:16,058:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 10:21:16,058:INFO:create_model() successfully completed......................................
2023-12-20 10:21:16,131:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:16,131:INFO:Creating metrics dataframe
2023-12-20 10:21:16,151:INFO:Initializing Extra Trees Classifier
2023-12-20 10:21:16,151:INFO:Total runtime is 0.29649143218994134 minutes
2023-12-20 10:21:16,153:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:16,154:INFO:Initializing create_model()
2023-12-20 10:21:16,154:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:16,154:INFO:Checking exceptions
2023-12-20 10:21:16,154:INFO:Importing libraries
2023-12-20 10:21:16,154:INFO:Copying training dataset
2023-12-20 10:21:16,159:INFO:Defining folds
2023-12-20 10:21:16,159:INFO:Declaring metric variables
2023-12-20 10:21:16,162:INFO:Importing untrained model
2023-12-20 10:21:16,165:INFO:Extra Trees Classifier Imported successfully
2023-12-20 10:21:16,166:INFO:Starting cross validation
2023-12-20 10:21:16,166:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:16,596:INFO:Calculating mean and std
2023-12-20 10:21:16,611:INFO:Creating metrics dataframe
2023-12-20 10:21:16,611:INFO:Uploading results into container
2023-12-20 10:21:16,611:INFO:Uploading model into container now
2023-12-20 10:21:16,611:INFO:_master_model_container: 12
2023-12-20 10:21:16,611:INFO:_display_container: 2
2023-12-20 10:21:16,611:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=2084, verbose=0, warm_start=False)
2023-12-20 10:21:16,611:INFO:create_model() successfully completed......................................
2023-12-20 10:21:16,679:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:16,679:INFO:Creating metrics dataframe
2023-12-20 10:21:16,679:INFO:Initializing Extreme Gradient Boosting
2023-12-20 10:21:16,679:INFO:Total runtime is 0.30529199441274 minutes
2023-12-20 10:21:16,695:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:16,695:INFO:Initializing create_model()
2023-12-20 10:21:16,695:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:16,695:INFO:Checking exceptions
2023-12-20 10:21:16,695:INFO:Importing libraries
2023-12-20 10:21:16,695:INFO:Copying training dataset
2023-12-20 10:21:16,695:INFO:Defining folds
2023-12-20 10:21:16,695:INFO:Declaring metric variables
2023-12-20 10:21:16,695:INFO:Importing untrained model
2023-12-20 10:21:16,695:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:21:16,695:INFO:Starting cross validation
2023-12-20 10:21:16,695:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:17,329:INFO:Calculating mean and std
2023-12-20 10:21:17,329:INFO:Creating metrics dataframe
2023-12-20 10:21:17,345:INFO:Uploading results into container
2023-12-20 10:21:17,345:INFO:Uploading model into container now
2023-12-20 10:21:17,345:INFO:_master_model_container: 13
2023-12-20 10:21:17,345:INFO:_display_container: 2
2023-12-20 10:21:17,345:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 10:21:17,345:INFO:create_model() successfully completed......................................
2023-12-20 10:21:17,416:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:17,416:INFO:Creating metrics dataframe
2023-12-20 10:21:17,427:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 10:21:17,427:INFO:Total runtime is 0.3177561958630879 minutes
2023-12-20 10:21:17,427:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:17,427:INFO:Initializing create_model()
2023-12-20 10:21:17,427:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:17,427:INFO:Checking exceptions
2023-12-20 10:21:17,427:INFO:Importing libraries
2023-12-20 10:21:17,427:INFO:Copying training dataset
2023-12-20 10:21:17,427:INFO:Defining folds
2023-12-20 10:21:17,427:INFO:Declaring metric variables
2023-12-20 10:21:17,443:INFO:Importing untrained model
2023-12-20 10:21:17,443:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:21:17,450:INFO:Starting cross validation
2023-12-20 10:21:17,450:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:20,577:INFO:Calculating mean and std
2023-12-20 10:21:20,577:INFO:Creating metrics dataframe
2023-12-20 10:21:20,577:INFO:Uploading results into container
2023-12-20 10:21:20,577:INFO:Uploading model into container now
2023-12-20 10:21:20,577:INFO:_master_model_container: 14
2023-12-20 10:21:20,577:INFO:_display_container: 2
2023-12-20 10:21:20,577:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:21:20,577:INFO:create_model() successfully completed......................................
2023-12-20 10:21:20,677:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:20,677:INFO:Creating metrics dataframe
2023-12-20 10:21:20,699:INFO:Initializing Dummy Classifier
2023-12-20 10:21:20,699:INFO:Total runtime is 0.37229581673940015 minutes
2023-12-20 10:21:20,701:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:20,701:INFO:Initializing create_model()
2023-12-20 10:21:20,701:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E475653D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:20,702:INFO:Checking exceptions
2023-12-20 10:21:20,702:INFO:Importing libraries
2023-12-20 10:21:20,702:INFO:Copying training dataset
2023-12-20 10:21:20,705:INFO:Defining folds
2023-12-20 10:21:20,705:INFO:Declaring metric variables
2023-12-20 10:21:20,707:INFO:Importing untrained model
2023-12-20 10:21:20,710:INFO:Dummy Classifier Imported successfully
2023-12-20 10:21:20,714:INFO:Starting cross validation
2023-12-20 10:21:20,714:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:20,743:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:20,743:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:20,743:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:20,743:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:20,743:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:21:20,759:INFO:Calculating mean and std
2023-12-20 10:21:20,759:INFO:Creating metrics dataframe
2023-12-20 10:21:20,760:INFO:Uploading results into container
2023-12-20 10:21:20,760:INFO:Uploading model into container now
2023-12-20 10:21:20,760:INFO:_master_model_container: 15
2023-12-20 10:21:20,760:INFO:_display_container: 2
2023-12-20 10:21:20,760:INFO:DummyClassifier(constant=None, random_state=2084, strategy='prior')
2023-12-20 10:21:20,760:INFO:create_model() successfully completed......................................
2023-12-20 10:21:20,828:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:20,828:INFO:Creating metrics dataframe
2023-12-20 10:21:20,847:INFO:Initializing create_model()
2023-12-20 10:21:20,847:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:20,847:INFO:Checking exceptions
2023-12-20 10:21:20,847:INFO:Importing libraries
2023-12-20 10:21:20,847:INFO:Copying training dataset
2023-12-20 10:21:20,859:INFO:Defining folds
2023-12-20 10:21:20,859:INFO:Declaring metric variables
2023-12-20 10:21:20,859:INFO:Importing untrained model
2023-12-20 10:21:20,860:INFO:Declaring custom model
2023-12-20 10:21:20,861:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:21:20,861:INFO:Cross validation set to False
2023-12-20 10:21:20,861:INFO:Fitting Model
2023-12-20 10:21:20,886:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000433 seconds.
2023-12-20 10:21:20,886:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-20 10:21:20,886:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-20 10:21:20,886:INFO:[LightGBM] [Info] Total Bins 1761
2023-12-20 10:21:20,886:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 10:21:20,887:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:21:20,887:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:21:20,887:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:21:21,160:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:21:21,160:INFO:create_model() successfully completed......................................
2023-12-20 10:21:21,276:INFO:Initializing create_model()
2023-12-20 10:21:21,276:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2084, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:21,276:INFO:Checking exceptions
2023-12-20 10:21:21,289:INFO:Importing libraries
2023-12-20 10:21:21,289:INFO:Copying training dataset
2023-12-20 10:21:21,293:INFO:Defining folds
2023-12-20 10:21:21,293:INFO:Declaring metric variables
2023-12-20 10:21:21,293:INFO:Importing untrained model
2023-12-20 10:21:21,293:INFO:Declaring custom model
2023-12-20 10:21:21,293:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:21:21,293:INFO:Cross validation set to False
2023-12-20 10:21:21,293:INFO:Fitting Model
2023-12-20 10:21:24,843:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2084, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:21:24,843:INFO:create_model() successfully completed......................................
2023-12-20 10:21:24,926:INFO:Initializing create_model()
2023-12-20 10:21:24,926:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:24,926:INFO:Checking exceptions
2023-12-20 10:21:24,926:INFO:Importing libraries
2023-12-20 10:21:24,926:INFO:Copying training dataset
2023-12-20 10:21:24,926:INFO:Defining folds
2023-12-20 10:21:24,926:INFO:Declaring metric variables
2023-12-20 10:21:24,926:INFO:Importing untrained model
2023-12-20 10:21:24,926:INFO:Declaring custom model
2023-12-20 10:21:24,926:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:21:24,926:INFO:Cross validation set to False
2023-12-20 10:21:24,926:INFO:Fitting Model
2023-12-20 10:21:25,260:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 10:21:25,260:INFO:create_model() successfully completed......................................
2023-12-20 10:21:25,382:INFO:Initializing create_model()
2023-12-20 10:21:25,382:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2084, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:25,382:INFO:Checking exceptions
2023-12-20 10:21:25,392:INFO:Importing libraries
2023-12-20 10:21:25,392:INFO:Copying training dataset
2023-12-20 10:21:25,393:INFO:Defining folds
2023-12-20 10:21:25,393:INFO:Declaring metric variables
2023-12-20 10:21:25,393:INFO:Importing untrained model
2023-12-20 10:21:25,393:INFO:Declaring custom model
2023-12-20 10:21:25,393:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:21:25,393:INFO:Cross validation set to False
2023-12-20 10:21:25,393:INFO:Fitting Model
2023-12-20 10:21:25,610:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2084, verbose=0, warm_start=False)
2023-12-20 10:21:25,610:INFO:create_model() successfully completed......................................
2023-12-20 10:21:25,693:INFO:Initializing create_model()
2023-12-20 10:21:25,693:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=2084), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:25,693:INFO:Checking exceptions
2023-12-20 10:21:25,698:INFO:Importing libraries
2023-12-20 10:21:25,698:INFO:Copying training dataset
2023-12-20 10:21:25,699:INFO:Defining folds
2023-12-20 10:21:25,699:INFO:Declaring metric variables
2023-12-20 10:21:25,699:INFO:Importing untrained model
2023-12-20 10:21:25,699:INFO:Declaring custom model
2023-12-20 10:21:25,699:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:21:25,699:INFO:Cross validation set to False
2023-12-20 10:21:25,699:INFO:Fitting Model
2023-12-20 10:21:25,982:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=2084)
2023-12-20 10:21:25,982:INFO:create_model() successfully completed......................................
2023-12-20 10:21:26,060:INFO:_master_model_container: 15
2023-12-20 10:21:26,060:INFO:_display_container: 2
2023-12-20 10:21:26,060:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2084, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2084, verbose=0, warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=2084)]
2023-12-20 10:21:26,060:INFO:compare_models() successfully completed......................................
2023-12-20 10:21:26,096:INFO:Initializing tune_model()
2023-12-20 10:21:26,096:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 10:21:26,096:INFO:Checking exceptions
2023-12-20 10:21:26,109:INFO:Copying training dataset
2023-12-20 10:21:26,114:INFO:Checking base model
2023-12-20 10:21:26,114:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 10:21:26,114:INFO:Declaring metric variables
2023-12-20 10:21:26,114:INFO:Defining Hyperparameters
2023-12-20 10:21:26,211:INFO:Tuning with n_jobs=-1
2023-12-20 10:21:26,211:INFO:Initializing RandomizedSearchCV
2023-12-20 10:21:55,948:INFO:best_params: {'actual_estimator__reg_lambda': 0.0005, 'actual_estimator__reg_alpha': 0.0001, 'actual_estimator__num_leaves': 6, 'actual_estimator__n_estimators': 300, 'actual_estimator__min_split_gain': 0.1, 'actual_estimator__min_child_samples': 6, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.9, 'actual_estimator__bagging_freq': 5, 'actual_estimator__bagging_fraction': 0.8}
2023-12-20 10:21:55,950:INFO:Hyperparameter search completed
2023-12-20 10:21:55,950:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:55,951:INFO:Initializing create_model()
2023-12-20 10:21:55,951:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000011E46FBA8D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.0005, 'reg_alpha': 0.0001, 'num_leaves': 6, 'n_estimators': 300, 'min_split_gain': 0.1, 'min_child_samples': 6, 'learning_rate': 0.1, 'feature_fraction': 0.9, 'bagging_freq': 5, 'bagging_fraction': 0.8})
2023-12-20 10:21:55,951:INFO:Checking exceptions
2023-12-20 10:21:55,952:INFO:Importing libraries
2023-12-20 10:21:55,952:INFO:Copying training dataset
2023-12-20 10:21:55,960:INFO:Defining folds
2023-12-20 10:21:55,960:INFO:Declaring metric variables
2023-12-20 10:21:55,971:INFO:Importing untrained model
2023-12-20 10:21:55,971:INFO:Declaring custom model
2023-12-20 10:21:55,976:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:21:55,984:INFO:Starting cross validation
2023-12-20 10:21:55,984:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:21:58,301:INFO:Calculating mean and std
2023-12-20 10:21:58,301:INFO:Creating metrics dataframe
2023-12-20 10:21:58,309:INFO:Finalizing model
2023-12-20 10:21:58,330:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2023-12-20 10:21:58,331:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 10:21:58,331:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 10:21:58,334:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2023-12-20 10:21:58,334:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 10:21:58,334:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 10:21:58,345:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002492 seconds.
2023-12-20 10:21:58,345:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:21:58,346:INFO:[LightGBM] [Info] Total Bins 1765
2023-12-20 10:21:58,346:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 51
2023-12-20 10:21:58,347:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:21:58,348:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:21:58,348:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:21:58,733:INFO:Uploading results into container
2023-12-20 10:21:58,734:INFO:Uploading model into container now
2023-12-20 10:21:58,735:INFO:_master_model_container: 16
2023-12-20 10:21:58,735:INFO:_display_container: 3
2023-12-20 10:21:58,735:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=300, n_jobs=-1, num_leaves=6, objective=None,
               random_state=2084, reg_alpha=0.0001, reg_lambda=0.0005,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:21:58,735:INFO:create_model() successfully completed......................................
2023-12-20 10:21:58,849:INFO:SubProcess create_model() end ==================================
2023-12-20 10:21:58,849:INFO:choose_better activated
2023-12-20 10:21:58,849:INFO:SubProcess create_model() called ==================================
2023-12-20 10:21:58,849:INFO:Initializing create_model()
2023-12-20 10:21:58,849:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:21:58,849:INFO:Checking exceptions
2023-12-20 10:21:58,849:INFO:Importing libraries
2023-12-20 10:21:58,849:INFO:Copying training dataset
2023-12-20 10:21:58,864:INFO:Defining folds
2023-12-20 10:21:58,864:INFO:Declaring metric variables
2023-12-20 10:21:58,864:INFO:Importing untrained model
2023-12-20 10:21:58,864:INFO:Declaring custom model
2023-12-20 10:21:58,864:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:21:58,864:INFO:Starting cross validation
2023-12-20 10:21:58,864:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:22:02,261:INFO:Calculating mean and std
2023-12-20 10:22:02,261:INFO:Creating metrics dataframe
2023-12-20 10:22:02,261:INFO:Finalizing model
2023-12-20 10:22:02,295:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002106 seconds.
2023-12-20 10:22:02,295:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-20 10:22:02,295:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-20 10:22:02,295:INFO:[LightGBM] [Info] Total Bins 1761
2023-12-20 10:22:02,295:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 49
2023-12-20 10:22:02,295:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:22:02,295:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:22:02,295:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:22:02,647:INFO:Uploading results into container
2023-12-20 10:22:02,663:INFO:Uploading model into container now
2023-12-20 10:22:02,663:INFO:_master_model_container: 17
2023-12-20 10:22:02,663:INFO:_display_container: 4
2023-12-20 10:22:02,663:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:22:02,663:INFO:create_model() successfully completed......................................
2023-12-20 10:22:02,779:INFO:SubProcess create_model() end ==================================
2023-12-20 10:22:02,779:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2084, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8257
2023-12-20 10:22:02,779:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=300, n_jobs=-1, num_leaves=6, objective=None,
               random_state=2084, reg_alpha=0.0001, reg_lambda=0.0005,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8299
2023-12-20 10:22:02,779:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=300, n_jobs=-1, num_leaves=6, objective=None,
               random_state=2084, reg_alpha=0.0001, reg_lambda=0.0005,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 10:22:02,779:INFO:choose_better completed
2023-12-20 10:22:02,779:INFO:_master_model_container: 17
2023-12-20 10:22:02,779:INFO:_display_container: 3
2023-12-20 10:22:02,779:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=300, n_jobs=-1, num_leaves=6, objective=None,
               random_state=2084, reg_alpha=0.0001, reg_lambda=0.0005,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:22:02,779:INFO:tune_model() successfully completed......................................
2023-12-20 10:22:02,894:INFO:Initializing evaluate_model()
2023-12-20 10:22:02,894:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=300, n_jobs=-1, num_leaves=6, objective=None,
               random_state=2084, reg_alpha=0.0001, reg_lambda=0.0005,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 10:22:02,903:INFO:Initializing plot_model()
2023-12-20 10:22:02,903:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000011E47623CD0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=6, min_child_weight=0.001, min_split_gain=0.1,
               n_estimators=300, n_jobs=-1, num_leaves=6, objective=None,
               random_state=2084, reg_alpha=0.0001, reg_lambda=0.0005,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:22:02,903:INFO:Checking exceptions
2023-12-20 10:22:02,908:INFO:Preloading libraries
2023-12-20 10:22:02,953:INFO:Copying training dataset
2023-12-20 10:22:02,953:INFO:Plot type: pipeline
2023-12-20 10:22:03,075:INFO:Visual Rendered Successfully
2023-12-20 10:22:03,151:INFO:plot_model() successfully completed......................................
2023-12-20 10:23:38,316:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:23:38,316:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:23:38,316:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:23:38,316:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:23:41,476:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,476:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,476:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,491:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,493:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,495:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,497:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,498:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,500:WARNING:C:\Users\yjg10\AppData\Local\Temp\ipykernel_22260\948304051.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)
  combined_o = pd.get_dummies(combined_o)

2023-12-20 10:23:41,840:INFO:PyCaret ClassificationExperiment
2023-12-20 10:23:41,840:INFO:Logging name: clf-default-name
2023-12-20 10:23:41,840:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 10:23:41,855:INFO:version 3.2.0
2023-12-20 10:23:41,855:INFO:Initializing setup()
2023-12-20 10:23:41,855:INFO:self.USI: 3e0a
2023-12-20 10:23:41,855:INFO:self._variable_keys: {'data', 'y', 'target_param', 'y_train', 'exp_id', 'gpu_param', 'USI', 'exp_name_log', 'fix_imbalance', '_available_plots', 'fold_shuffle_param', 'seed', 'log_plots_param', 'logging_param', 'memory', '_ml_usecase', 'fold_generator', 'y_test', 'idx', 'html_param', 'X', 'fold_groups_param', 'X_test', 'is_multiclass', 'n_jobs_param', 'pipeline', 'X_train', 'gpu_n_jobs_param'}
2023-12-20 10:23:41,855:INFO:Checking environment
2023-12-20 10:23:41,855:INFO:python_version: 3.11.5
2023-12-20 10:23:41,855:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 10:23:41,855:INFO:machine: AMD64
2023-12-20 10:23:41,855:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 10:23:41,855:INFO:Memory: svmem(total=16718413824, available=6307987456, percent=62.3, used=10410426368, free=6307987456)
2023-12-20 10:23:41,855:INFO:Physical Core: 12
2023-12-20 10:23:41,855:INFO:Logical Core: 16
2023-12-20 10:23:41,855:INFO:Checking libraries
2023-12-20 10:23:41,855:INFO:System:
2023-12-20 10:23:41,855:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 10:23:41,855:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 10:23:41,855:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 10:23:41,855:INFO:PyCaret required dependencies:
2023-12-20 10:23:42,409:INFO:                 pip: 23.2.1
2023-12-20 10:23:42,409:INFO:          setuptools: 68.0.0
2023-12-20 10:23:42,409:INFO:             pycaret: 3.2.0
2023-12-20 10:23:42,409:INFO:             IPython: 8.15.0
2023-12-20 10:23:42,409:INFO:          ipywidgets: 8.0.4
2023-12-20 10:23:42,409:INFO:                tqdm: 4.65.0
2023-12-20 10:23:42,409:INFO:               numpy: 1.24.3
2023-12-20 10:23:42,409:INFO:              pandas: 1.5.3
2023-12-20 10:23:42,409:INFO:              jinja2: 3.1.2
2023-12-20 10:23:42,409:INFO:               scipy: 1.10.1
2023-12-20 10:23:42,409:INFO:              joblib: 1.2.0
2023-12-20 10:23:42,409:INFO:             sklearn: 1.2.1
2023-12-20 10:23:42,409:INFO:                pyod: 1.1.2
2023-12-20 10:23:42,409:INFO:            imblearn: 0.11.0
2023-12-20 10:23:42,409:INFO:   category_encoders: 2.6.3
2023-12-20 10:23:42,409:INFO:            lightgbm: 4.1.0
2023-12-20 10:23:42,409:INFO:               numba: 0.57.1
2023-12-20 10:23:42,409:INFO:            requests: 2.31.0
2023-12-20 10:23:42,409:INFO:          matplotlib: 3.6.0
2023-12-20 10:23:42,409:INFO:          scikitplot: 0.3.7
2023-12-20 10:23:42,409:INFO:         yellowbrick: 1.5
2023-12-20 10:23:42,409:INFO:              plotly: 5.9.0
2023-12-20 10:23:42,409:INFO:    plotly-resampler: Not installed
2023-12-20 10:23:42,409:INFO:             kaleido: 0.2.1
2023-12-20 10:23:42,409:INFO:           schemdraw: 0.15
2023-12-20 10:23:42,409:INFO:         statsmodels: 0.14.0
2023-12-20 10:23:42,409:INFO:              sktime: 0.21.1
2023-12-20 10:23:42,409:INFO:               tbats: 1.1.3
2023-12-20 10:23:42,409:INFO:            pmdarima: 2.0.4
2023-12-20 10:23:42,409:INFO:              psutil: 5.9.0
2023-12-20 10:23:42,409:INFO:          markupsafe: 2.1.1
2023-12-20 10:23:42,409:INFO:             pickle5: Not installed
2023-12-20 10:23:42,409:INFO:         cloudpickle: 2.2.1
2023-12-20 10:23:42,409:INFO:         deprecation: 2.1.0
2023-12-20 10:23:42,409:INFO:              xxhash: 2.0.2
2023-12-20 10:23:42,409:INFO:           wurlitzer: Not installed
2023-12-20 10:23:42,409:INFO:PyCaret optional dependencies:
2023-12-20 10:23:42,558:INFO:                shap: Not installed
2023-12-20 10:23:42,558:INFO:           interpret: Not installed
2023-12-20 10:23:42,558:INFO:                umap: Not installed
2023-12-20 10:23:42,558:INFO:     ydata_profiling: Not installed
2023-12-20 10:23:42,558:INFO:  explainerdashboard: Not installed
2023-12-20 10:23:42,558:INFO:             autoviz: Not installed
2023-12-20 10:23:42,558:INFO:           fairlearn: Not installed
2023-12-20 10:23:42,558:INFO:          deepchecks: Not installed
2023-12-20 10:23:42,558:INFO:             xgboost: 2.0.2
2023-12-20 10:23:42,558:INFO:            catboost: Not installed
2023-12-20 10:23:42,558:INFO:              kmodes: Not installed
2023-12-20 10:23:42,558:INFO:             mlxtend: Not installed
2023-12-20 10:23:42,558:INFO:       statsforecast: Not installed
2023-12-20 10:23:42,558:INFO:        tune_sklearn: Not installed
2023-12-20 10:23:42,558:INFO:                 ray: Not installed
2023-12-20 10:23:42,558:INFO:            hyperopt: Not installed
2023-12-20 10:23:42,558:INFO:              optuna: Not installed
2023-12-20 10:23:42,558:INFO:               skopt: Not installed
2023-12-20 10:23:42,558:INFO:              mlflow: Not installed
2023-12-20 10:23:42,558:INFO:              gradio: Not installed
2023-12-20 10:23:42,558:INFO:             fastapi: Not installed
2023-12-20 10:23:42,558:INFO:             uvicorn: Not installed
2023-12-20 10:23:42,558:INFO:              m2cgen: Not installed
2023-12-20 10:23:42,558:INFO:           evidently: Not installed
2023-12-20 10:23:42,558:INFO:               fugue: Not installed
2023-12-20 10:23:42,558:INFO:           streamlit: Not installed
2023-12-20 10:23:42,558:INFO:             prophet: Not installed
2023-12-20 10:23:42,558:INFO:None
2023-12-20 10:23:42,558:INFO:Set up data.
2023-12-20 10:23:42,576:INFO:Set up folding strategy.
2023-12-20 10:23:42,576:INFO:Set up train/test split.
2023-12-20 10:23:42,576:INFO:Set up index.
2023-12-20 10:23:42,576:INFO:Assigning column types.
2023-12-20 10:23:42,576:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 10:23:42,610:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:23:42,610:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:23:42,627:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:23:42,627:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:23:42,660:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:23:42,660:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:23:42,678:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:23:42,680:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:23:42,680:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 10:23:42,708:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:23:42,725:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:23:42,725:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:23:42,742:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:23:42,758:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:23:42,774:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:23:42,774:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 10:23:42,808:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:23:42,808:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:23:42,857:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:23:42,859:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:23:42,860:INFO:Preparing preprocessing pipeline...
2023-12-20 10:23:42,860:INFO:Set up simple imputation.
2023-12-20 10:23:42,860:INFO:Set up column name cleaning.
2023-12-20 10:23:42,876:INFO:Finished creating preprocessing pipeline.
2023-12-20 10:23:42,876:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Drug_D-penicillamine',
                                             'Drug_Placebo', 'Sex_F', 'Sex_M',
                                             'Ascites_N', 'Ascites_...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 10:23:42,876:INFO:Creating final display dataframe.
2023-12-20 10:23:42,947:INFO:Setup _display_container:                     Description             Value
0                    Session id               873
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 48)
4        Transformed data shape        (7905, 48)
5   Transformed train set shape        (6324, 48)
6    Transformed test set shape        (1581, 48)
7              Numeric features                47
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              3e0a
2023-12-20 10:23:42,995:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:23:42,995:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:23:43,040:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:23:43,041:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:23:43,042:INFO:setup() successfully completed in 1.31s...............
2023-12-20 10:23:43,055:INFO:Initializing compare_models()
2023-12-20 10:23:43,055:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 10:23:43,055:INFO:Checking exceptions
2023-12-20 10:23:43,061:INFO:Preparing display monitor
2023-12-20 10:23:43,079:INFO:Initializing Logistic Regression
2023-12-20 10:23:43,079:INFO:Total runtime is 0.0 minutes
2023-12-20 10:23:43,081:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:43,082:INFO:Initializing create_model()
2023-12-20 10:23:43,082:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:43,082:INFO:Checking exceptions
2023-12-20 10:23:43,082:INFO:Importing libraries
2023-12-20 10:23:43,082:INFO:Copying training dataset
2023-12-20 10:23:43,082:INFO:Defining folds
2023-12-20 10:23:43,082:INFO:Declaring metric variables
2023-12-20 10:23:43,082:INFO:Importing untrained model
2023-12-20 10:23:43,090:INFO:Logistic Regression Imported successfully
2023-12-20 10:23:43,094:INFO:Starting cross validation
2023-12-20 10:23:43,094:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:46,708:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:46,791:INFO:Calculating mean and std
2023-12-20 10:23:46,791:INFO:Creating metrics dataframe
2023-12-20 10:23:46,807:INFO:Uploading results into container
2023-12-20 10:23:46,808:INFO:Uploading model into container now
2023-12-20 10:23:46,808:INFO:_master_model_container: 1
2023-12-20 10:23:46,808:INFO:_display_container: 2
2023-12-20 10:23:46,808:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=873, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 10:23:46,808:INFO:create_model() successfully completed......................................
2023-12-20 10:23:46,880:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:46,880:INFO:Creating metrics dataframe
2023-12-20 10:23:46,892:INFO:Initializing K Neighbors Classifier
2023-12-20 10:23:46,892:INFO:Total runtime is 0.06355078220367431 minutes
2023-12-20 10:23:46,894:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:46,894:INFO:Initializing create_model()
2023-12-20 10:23:46,894:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:46,894:INFO:Checking exceptions
2023-12-20 10:23:46,894:INFO:Importing libraries
2023-12-20 10:23:46,894:INFO:Copying training dataset
2023-12-20 10:23:46,894:INFO:Defining folds
2023-12-20 10:23:46,894:INFO:Declaring metric variables
2023-12-20 10:23:46,894:INFO:Importing untrained model
2023-12-20 10:23:46,916:INFO:K Neighbors Classifier Imported successfully
2023-12-20 10:23:46,946:INFO:Starting cross validation
2023-12-20 10:23:46,949:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:49,973:INFO:Calculating mean and std
2023-12-20 10:23:49,973:INFO:Creating metrics dataframe
2023-12-20 10:23:49,973:INFO:Uploading results into container
2023-12-20 10:23:49,973:INFO:Uploading model into container now
2023-12-20 10:23:49,973:INFO:_master_model_container: 2
2023-12-20 10:23:49,973:INFO:_display_container: 2
2023-12-20 10:23:49,983:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 10:23:49,983:INFO:create_model() successfully completed......................................
2023-12-20 10:23:50,041:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:50,041:INFO:Creating metrics dataframe
2023-12-20 10:23:50,056:INFO:Initializing Naive Bayes
2023-12-20 10:23:50,056:INFO:Total runtime is 0.11628691752751669 minutes
2023-12-20 10:23:50,056:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:50,056:INFO:Initializing create_model()
2023-12-20 10:23:50,056:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:50,056:INFO:Checking exceptions
2023-12-20 10:23:50,056:INFO:Importing libraries
2023-12-20 10:23:50,056:INFO:Copying training dataset
2023-12-20 10:23:50,056:INFO:Defining folds
2023-12-20 10:23:50,056:INFO:Declaring metric variables
2023-12-20 10:23:50,056:INFO:Importing untrained model
2023-12-20 10:23:50,056:INFO:Naive Bayes Imported successfully
2023-12-20 10:23:50,072:INFO:Starting cross validation
2023-12-20 10:23:50,073:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:52,874:INFO:Calculating mean and std
2023-12-20 10:23:52,874:INFO:Creating metrics dataframe
2023-12-20 10:23:52,874:INFO:Uploading results into container
2023-12-20 10:23:52,874:INFO:Uploading model into container now
2023-12-20 10:23:52,874:INFO:_master_model_container: 3
2023-12-20 10:23:52,874:INFO:_display_container: 2
2023-12-20 10:23:52,874:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 10:23:52,874:INFO:create_model() successfully completed......................................
2023-12-20 10:23:52,945:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:52,945:INFO:Creating metrics dataframe
2023-12-20 10:23:52,945:INFO:Initializing Decision Tree Classifier
2023-12-20 10:23:52,945:INFO:Total runtime is 0.16443325678507487 minutes
2023-12-20 10:23:52,957:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:52,957:INFO:Initializing create_model()
2023-12-20 10:23:52,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:52,957:INFO:Checking exceptions
2023-12-20 10:23:52,957:INFO:Importing libraries
2023-12-20 10:23:52,957:INFO:Copying training dataset
2023-12-20 10:23:52,957:INFO:Defining folds
2023-12-20 10:23:52,957:INFO:Declaring metric variables
2023-12-20 10:23:52,957:INFO:Importing untrained model
2023-12-20 10:23:52,957:INFO:Decision Tree Classifier Imported successfully
2023-12-20 10:23:52,976:INFO:Starting cross validation
2023-12-20 10:23:52,976:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:55,472:INFO:Calculating mean and std
2023-12-20 10:23:55,472:INFO:Creating metrics dataframe
2023-12-20 10:23:55,472:INFO:Uploading results into container
2023-12-20 10:23:55,472:INFO:Uploading model into container now
2023-12-20 10:23:55,472:INFO:_master_model_container: 4
2023-12-20 10:23:55,472:INFO:_display_container: 2
2023-12-20 10:23:55,472:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=873, splitter='best')
2023-12-20 10:23:55,472:INFO:create_model() successfully completed......................................
2023-12-20 10:23:55,555:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:55,555:INFO:Creating metrics dataframe
2023-12-20 10:23:55,555:INFO:Initializing SVM - Linear Kernel
2023-12-20 10:23:55,555:INFO:Total runtime is 0.2079322099685669 minutes
2023-12-20 10:23:55,570:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:55,573:INFO:Initializing create_model()
2023-12-20 10:23:55,574:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:55,574:INFO:Checking exceptions
2023-12-20 10:23:55,574:INFO:Importing libraries
2023-12-20 10:23:55,574:INFO:Copying training dataset
2023-12-20 10:23:55,574:INFO:Defining folds
2023-12-20 10:23:55,574:INFO:Declaring metric variables
2023-12-20 10:23:55,583:INFO:Importing untrained model
2023-12-20 10:23:55,583:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 10:23:55,591:INFO:Starting cross validation
2023-12-20 10:23:55,591:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:55,672:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:23:55,672:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:23:55,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:23:55,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:23:55,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:23:55,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,695:INFO:Calculating mean and std
2023-12-20 10:23:55,695:INFO:Creating metrics dataframe
2023-12-20 10:23:55,695:INFO:Uploading results into container
2023-12-20 10:23:55,695:INFO:Uploading model into container now
2023-12-20 10:23:55,695:INFO:_master_model_container: 5
2023-12-20 10:23:55,695:INFO:_display_container: 2
2023-12-20 10:23:55,710:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=873, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 10:23:55,710:INFO:create_model() successfully completed......................................
2023-12-20 10:23:55,774:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:55,774:INFO:Creating metrics dataframe
2023-12-20 10:23:55,789:INFO:Initializing Ridge Classifier
2023-12-20 10:23:55,789:INFO:Total runtime is 0.21183273792266846 minutes
2023-12-20 10:23:55,793:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:55,794:INFO:Initializing create_model()
2023-12-20 10:23:55,794:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:55,794:INFO:Checking exceptions
2023-12-20 10:23:55,794:INFO:Importing libraries
2023-12-20 10:23:55,794:INFO:Copying training dataset
2023-12-20 10:23:55,795:INFO:Defining folds
2023-12-20 10:23:55,795:INFO:Declaring metric variables
2023-12-20 10:23:55,803:INFO:Importing untrained model
2023-12-20 10:23:55,803:INFO:Ridge Classifier Imported successfully
2023-12-20 10:23:55,812:INFO:Starting cross validation
2023-12-20 10:23:55,812:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,857:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:23:55,873:INFO:Calculating mean and std
2023-12-20 10:23:55,873:INFO:Creating metrics dataframe
2023-12-20 10:23:55,873:INFO:Uploading results into container
2023-12-20 10:23:55,873:INFO:Uploading model into container now
2023-12-20 10:23:55,873:INFO:_master_model_container: 6
2023-12-20 10:23:55,873:INFO:_display_container: 2
2023-12-20 10:23:55,873:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=873, solver='auto',
                tol=0.0001)
2023-12-20 10:23:55,873:INFO:create_model() successfully completed......................................
2023-12-20 10:23:55,943:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:55,943:INFO:Creating metrics dataframe
2023-12-20 10:23:55,957:INFO:Initializing Random Forest Classifier
2023-12-20 10:23:55,958:INFO:Total runtime is 0.21464457511901855 minutes
2023-12-20 10:23:55,958:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:55,958:INFO:Initializing create_model()
2023-12-20 10:23:55,958:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:55,958:INFO:Checking exceptions
2023-12-20 10:23:55,958:INFO:Importing libraries
2023-12-20 10:23:55,958:INFO:Copying training dataset
2023-12-20 10:23:55,958:INFO:Defining folds
2023-12-20 10:23:55,958:INFO:Declaring metric variables
2023-12-20 10:23:55,968:INFO:Importing untrained model
2023-12-20 10:23:55,968:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:23:55,974:INFO:Starting cross validation
2023-12-20 10:23:55,975:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:56,492:INFO:Calculating mean and std
2023-12-20 10:23:56,492:INFO:Creating metrics dataframe
2023-12-20 10:23:56,492:INFO:Uploading results into container
2023-12-20 10:23:56,507:INFO:Uploading model into container now
2023-12-20 10:23:56,507:INFO:_master_model_container: 7
2023-12-20 10:23:56,507:INFO:_display_container: 2
2023-12-20 10:23:56,507:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=873, verbose=0, warm_start=False)
2023-12-20 10:23:56,507:INFO:create_model() successfully completed......................................
2023-12-20 10:23:56,574:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:56,574:INFO:Creating metrics dataframe
2023-12-20 10:23:56,574:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 10:23:56,574:INFO:Total runtime is 0.22491857608159382 minutes
2023-12-20 10:23:56,574:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:56,574:INFO:Initializing create_model()
2023-12-20 10:23:56,574:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:56,574:INFO:Checking exceptions
2023-12-20 10:23:56,574:INFO:Importing libraries
2023-12-20 10:23:56,574:INFO:Copying training dataset
2023-12-20 10:23:56,590:INFO:Defining folds
2023-12-20 10:23:56,590:INFO:Declaring metric variables
2023-12-20 10:23:56,590:INFO:Importing untrained model
2023-12-20 10:23:56,590:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 10:23:56,600:INFO:Starting cross validation
2023-12-20 10:23:56,600:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:56,642:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:23:56,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:23:56,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:23:56,644:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:23:56,644:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:23:56,658:INFO:Calculating mean and std
2023-12-20 10:23:56,658:INFO:Creating metrics dataframe
2023-12-20 10:23:56,658:INFO:Uploading results into container
2023-12-20 10:23:56,658:INFO:Uploading model into container now
2023-12-20 10:23:56,658:INFO:_master_model_container: 8
2023-12-20 10:23:56,658:INFO:_display_container: 2
2023-12-20 10:23:56,658:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 10:23:56,658:INFO:create_model() successfully completed......................................
2023-12-20 10:23:56,724:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:56,724:INFO:Creating metrics dataframe
2023-12-20 10:23:56,744:INFO:Initializing Ada Boost Classifier
2023-12-20 10:23:56,744:INFO:Total runtime is 0.22775144974390665 minutes
2023-12-20 10:23:56,744:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:56,744:INFO:Initializing create_model()
2023-12-20 10:23:56,744:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:56,744:INFO:Checking exceptions
2023-12-20 10:23:56,744:INFO:Importing libraries
2023-12-20 10:23:56,744:INFO:Copying training dataset
2023-12-20 10:23:56,744:INFO:Defining folds
2023-12-20 10:23:56,744:INFO:Declaring metric variables
2023-12-20 10:23:56,744:INFO:Importing untrained model
2023-12-20 10:23:56,757:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:23:56,762:INFO:Starting cross validation
2023-12-20 10:23:56,762:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:23:57,110:INFO:Calculating mean and std
2023-12-20 10:23:57,110:INFO:Creating metrics dataframe
2023-12-20 10:23:57,110:INFO:Uploading results into container
2023-12-20 10:23:57,110:INFO:Uploading model into container now
2023-12-20 10:23:57,110:INFO:_master_model_container: 9
2023-12-20 10:23:57,110:INFO:_display_container: 2
2023-12-20 10:23:57,110:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=873)
2023-12-20 10:23:57,110:INFO:create_model() successfully completed......................................
2023-12-20 10:23:57,204:INFO:SubProcess create_model() end ==================================
2023-12-20 10:23:57,204:INFO:Creating metrics dataframe
2023-12-20 10:23:57,211:INFO:Initializing Gradient Boosting Classifier
2023-12-20 10:23:57,211:INFO:Total runtime is 0.23554016749064127 minutes
2023-12-20 10:23:57,214:INFO:SubProcess create_model() called ==================================
2023-12-20 10:23:57,214:INFO:Initializing create_model()
2023-12-20 10:23:57,214:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:23:57,214:INFO:Checking exceptions
2023-12-20 10:23:57,214:INFO:Importing libraries
2023-12-20 10:23:57,214:INFO:Copying training dataset
2023-12-20 10:23:57,215:INFO:Defining folds
2023-12-20 10:23:57,215:INFO:Declaring metric variables
2023-12-20 10:23:57,221:INFO:Importing untrained model
2023-12-20 10:23:57,223:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:23:57,225:INFO:Starting cross validation
2023-12-20 10:23:57,225:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:24:00,623:INFO:Calculating mean and std
2023-12-20 10:24:00,623:INFO:Creating metrics dataframe
2023-12-20 10:24:00,625:INFO:Uploading results into container
2023-12-20 10:24:00,626:INFO:Uploading model into container now
2023-12-20 10:24:00,626:INFO:_master_model_container: 10
2023-12-20 10:24:00,626:INFO:_display_container: 2
2023-12-20 10:24:00,626:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=873, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:24:00,626:INFO:create_model() successfully completed......................................
2023-12-20 10:24:00,733:INFO:SubProcess create_model() end ==================================
2023-12-20 10:24:00,733:INFO:Creating metrics dataframe
2023-12-20 10:24:00,746:INFO:Initializing Linear Discriminant Analysis
2023-12-20 10:24:00,746:INFO:Total runtime is 0.29445178111394243 minutes
2023-12-20 10:24:00,748:INFO:SubProcess create_model() called ==================================
2023-12-20 10:24:00,748:INFO:Initializing create_model()
2023-12-20 10:24:00,749:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:00,749:INFO:Checking exceptions
2023-12-20 10:24:00,749:INFO:Importing libraries
2023-12-20 10:24:00,750:INFO:Copying training dataset
2023-12-20 10:24:00,752:INFO:Defining folds
2023-12-20 10:24:00,752:INFO:Declaring metric variables
2023-12-20 10:24:00,758:INFO:Importing untrained model
2023-12-20 10:24:00,761:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 10:24:00,767:INFO:Starting cross validation
2023-12-20 10:24:00,768:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:24:00,842:INFO:Calculating mean and std
2023-12-20 10:24:00,842:INFO:Creating metrics dataframe
2023-12-20 10:24:00,858:INFO:Uploading results into container
2023-12-20 10:24:00,859:INFO:Uploading model into container now
2023-12-20 10:24:00,859:INFO:_master_model_container: 11
2023-12-20 10:24:00,859:INFO:_display_container: 2
2023-12-20 10:24:00,860:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 10:24:00,860:INFO:create_model() successfully completed......................................
2023-12-20 10:24:00,924:INFO:SubProcess create_model() end ==================================
2023-12-20 10:24:00,924:INFO:Creating metrics dataframe
2023-12-20 10:24:00,939:INFO:Initializing Extra Trees Classifier
2023-12-20 10:24:00,939:INFO:Total runtime is 0.29767424662907915 minutes
2023-12-20 10:24:00,941:INFO:SubProcess create_model() called ==================================
2023-12-20 10:24:00,942:INFO:Initializing create_model()
2023-12-20 10:24:00,943:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:00,943:INFO:Checking exceptions
2023-12-20 10:24:00,943:INFO:Importing libraries
2023-12-20 10:24:00,943:INFO:Copying training dataset
2023-12-20 10:24:00,943:INFO:Defining folds
2023-12-20 10:24:00,943:INFO:Declaring metric variables
2023-12-20 10:24:00,943:INFO:Importing untrained model
2023-12-20 10:24:00,943:INFO:Extra Trees Classifier Imported successfully
2023-12-20 10:24:00,943:INFO:Starting cross validation
2023-12-20 10:24:00,943:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:24:01,428:INFO:Calculating mean and std
2023-12-20 10:24:01,443:INFO:Creating metrics dataframe
2023-12-20 10:24:01,443:INFO:Uploading results into container
2023-12-20 10:24:01,443:INFO:Uploading model into container now
2023-12-20 10:24:01,443:INFO:_master_model_container: 12
2023-12-20 10:24:01,443:INFO:_display_container: 2
2023-12-20 10:24:01,443:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=873, verbose=0, warm_start=False)
2023-12-20 10:24:01,443:INFO:create_model() successfully completed......................................
2023-12-20 10:24:01,511:INFO:SubProcess create_model() end ==================================
2023-12-20 10:24:01,511:INFO:Creating metrics dataframe
2023-12-20 10:24:01,526:INFO:Initializing Extreme Gradient Boosting
2023-12-20 10:24:01,526:INFO:Total runtime is 0.3074573874473571 minutes
2023-12-20 10:24:01,526:INFO:SubProcess create_model() called ==================================
2023-12-20 10:24:01,526:INFO:Initializing create_model()
2023-12-20 10:24:01,526:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:01,526:INFO:Checking exceptions
2023-12-20 10:24:01,526:INFO:Importing libraries
2023-12-20 10:24:01,526:INFO:Copying training dataset
2023-12-20 10:24:01,526:INFO:Defining folds
2023-12-20 10:24:01,526:INFO:Declaring metric variables
2023-12-20 10:24:01,540:INFO:Importing untrained model
2023-12-20 10:24:01,544:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:24:01,548:INFO:Starting cross validation
2023-12-20 10:24:01,549:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:24:02,160:INFO:Calculating mean and std
2023-12-20 10:24:02,160:INFO:Creating metrics dataframe
2023-12-20 10:24:02,174:INFO:Uploading results into container
2023-12-20 10:24:02,174:INFO:Uploading model into container now
2023-12-20 10:24:02,174:INFO:_master_model_container: 13
2023-12-20 10:24:02,174:INFO:_display_container: 2
2023-12-20 10:24:02,174:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 10:24:02,174:INFO:create_model() successfully completed......................................
2023-12-20 10:24:02,241:INFO:SubProcess create_model() end ==================================
2023-12-20 10:24:02,241:INFO:Creating metrics dataframe
2023-12-20 10:24:02,257:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 10:24:02,257:INFO:Total runtime is 0.3196373661359151 minutes
2023-12-20 10:24:02,257:INFO:SubProcess create_model() called ==================================
2023-12-20 10:24:02,257:INFO:Initializing create_model()
2023-12-20 10:24:02,257:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:02,257:INFO:Checking exceptions
2023-12-20 10:24:02,257:INFO:Importing libraries
2023-12-20 10:24:02,257:INFO:Copying training dataset
2023-12-20 10:24:02,257:INFO:Defining folds
2023-12-20 10:24:02,257:INFO:Declaring metric variables
2023-12-20 10:24:02,257:INFO:Importing untrained model
2023-12-20 10:24:02,257:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:24:02,272:INFO:Starting cross validation
2023-12-20 10:24:02,275:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:24:05,414:INFO:Calculating mean and std
2023-12-20 10:24:05,415:INFO:Creating metrics dataframe
2023-12-20 10:24:05,416:INFO:Uploading results into container
2023-12-20 10:24:05,416:INFO:Uploading model into container now
2023-12-20 10:24:05,416:INFO:_master_model_container: 14
2023-12-20 10:24:05,416:INFO:_display_container: 2
2023-12-20 10:24:05,416:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:24:05,416:INFO:create_model() successfully completed......................................
2023-12-20 10:24:05,530:INFO:SubProcess create_model() end ==================================
2023-12-20 10:24:05,530:INFO:Creating metrics dataframe
2023-12-20 10:24:05,540:INFO:Initializing Dummy Classifier
2023-12-20 10:24:05,540:INFO:Total runtime is 0.3743499914805094 minutes
2023-12-20 10:24:05,542:INFO:SubProcess create_model() called ==================================
2023-12-20 10:24:05,542:INFO:Initializing create_model()
2023-12-20 10:24:05,542:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FF843090>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:05,542:INFO:Checking exceptions
2023-12-20 10:24:05,542:INFO:Importing libraries
2023-12-20 10:24:05,542:INFO:Copying training dataset
2023-12-20 10:24:05,542:INFO:Defining folds
2023-12-20 10:24:05,542:INFO:Declaring metric variables
2023-12-20 10:24:05,542:INFO:Importing untrained model
2023-12-20 10:24:05,542:INFO:Dummy Classifier Imported successfully
2023-12-20 10:24:05,558:INFO:Starting cross validation
2023-12-20 10:24:05,558:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:24:05,582:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:24:05,582:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:24:05,582:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:24:05,582:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:24:05,595:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:24:05,598:INFO:Calculating mean and std
2023-12-20 10:24:05,598:INFO:Creating metrics dataframe
2023-12-20 10:24:05,598:INFO:Uploading results into container
2023-12-20 10:24:05,598:INFO:Uploading model into container now
2023-12-20 10:24:05,598:INFO:_master_model_container: 15
2023-12-20 10:24:05,598:INFO:_display_container: 2
2023-12-20 10:24:05,598:INFO:DummyClassifier(constant=None, random_state=873, strategy='prior')
2023-12-20 10:24:05,598:INFO:create_model() successfully completed......................................
2023-12-20 10:24:05,679:INFO:SubProcess create_model() end ==================================
2023-12-20 10:24:05,679:INFO:Creating metrics dataframe
2023-12-20 10:24:05,699:INFO:Initializing create_model()
2023-12-20 10:24:05,699:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:05,699:INFO:Checking exceptions
2023-12-20 10:24:05,699:INFO:Importing libraries
2023-12-20 10:24:05,699:INFO:Copying training dataset
2023-12-20 10:24:05,699:INFO:Defining folds
2023-12-20 10:24:05,699:INFO:Declaring metric variables
2023-12-20 10:24:05,699:INFO:Importing untrained model
2023-12-20 10:24:05,699:INFO:Declaring custom model
2023-12-20 10:24:05,699:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:24:05,699:INFO:Cross validation set to False
2023-12-20 10:24:05,699:INFO:Fitting Model
2023-12-20 10:24:05,732:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001126 seconds.
2023-12-20 10:24:05,732:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:24:05,732:INFO:[LightGBM] [Info] Total Bins 1798
2023-12-20 10:24:05,732:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 44
2023-12-20 10:24:05,732:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:24:05,732:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:24:05,732:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:24:05,993:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:24:05,993:INFO:create_model() successfully completed......................................
2023-12-20 10:24:06,091:INFO:Initializing create_model()
2023-12-20 10:24:06,091:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=873, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:06,091:INFO:Checking exceptions
2023-12-20 10:24:06,098:INFO:Importing libraries
2023-12-20 10:24:06,099:INFO:Copying training dataset
2023-12-20 10:24:06,099:INFO:Defining folds
2023-12-20 10:24:06,099:INFO:Declaring metric variables
2023-12-20 10:24:06,099:INFO:Importing untrained model
2023-12-20 10:24:06,099:INFO:Declaring custom model
2023-12-20 10:24:06,099:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:24:06,099:INFO:Cross validation set to False
2023-12-20 10:24:06,099:INFO:Fitting Model
2023-12-20 10:24:09,643:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=873, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:24:09,643:INFO:create_model() successfully completed......................................
2023-12-20 10:24:09,728:INFO:Initializing create_model()
2023-12-20 10:24:09,728:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:09,728:INFO:Checking exceptions
2023-12-20 10:24:09,728:INFO:Importing libraries
2023-12-20 10:24:09,728:INFO:Copying training dataset
2023-12-20 10:24:09,728:INFO:Defining folds
2023-12-20 10:24:09,728:INFO:Declaring metric variables
2023-12-20 10:24:09,728:INFO:Importing untrained model
2023-12-20 10:24:09,728:INFO:Declaring custom model
2023-12-20 10:24:09,728:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:24:09,728:INFO:Cross validation set to False
2023-12-20 10:24:09,728:INFO:Fitting Model
2023-12-20 10:24:10,029:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 10:24:10,029:INFO:create_model() successfully completed......................................
2023-12-20 10:24:10,145:INFO:Initializing create_model()
2023-12-20 10:24:10,145:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=873, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:10,145:INFO:Checking exceptions
2023-12-20 10:24:10,156:INFO:Importing libraries
2023-12-20 10:24:10,156:INFO:Copying training dataset
2023-12-20 10:24:10,157:INFO:Defining folds
2023-12-20 10:24:10,157:INFO:Declaring metric variables
2023-12-20 10:24:10,157:INFO:Importing untrained model
2023-12-20 10:24:10,157:INFO:Declaring custom model
2023-12-20 10:24:10,157:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:24:10,157:INFO:Cross validation set to False
2023-12-20 10:24:10,157:INFO:Fitting Model
2023-12-20 10:24:10,373:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=873, verbose=0, warm_start=False)
2023-12-20 10:24:10,373:INFO:create_model() successfully completed......................................
2023-12-20 10:24:10,460:INFO:Initializing create_model()
2023-12-20 10:24:10,460:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=873), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:10,461:INFO:Checking exceptions
2023-12-20 10:24:10,463:INFO:Importing libraries
2023-12-20 10:24:10,463:INFO:Copying training dataset
2023-12-20 10:24:10,467:INFO:Defining folds
2023-12-20 10:24:10,467:INFO:Declaring metric variables
2023-12-20 10:24:10,467:INFO:Importing untrained model
2023-12-20 10:24:10,467:INFO:Declaring custom model
2023-12-20 10:24:10,467:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:24:10,467:INFO:Cross validation set to False
2023-12-20 10:24:10,467:INFO:Fitting Model
2023-12-20 10:24:10,744:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=873)
2023-12-20 10:24:10,744:INFO:create_model() successfully completed......................................
2023-12-20 10:24:10,858:INFO:_master_model_container: 15
2023-12-20 10:24:10,858:INFO:_display_container: 2
2023-12-20 10:24:10,858:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=873, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=873, verbose=0, warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=873)]
2023-12-20 10:24:10,858:INFO:compare_models() successfully completed......................................
2023-12-20 10:24:10,899:INFO:Initializing tune_model()
2023-12-20 10:24:10,900:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 10:24:10,900:INFO:Checking exceptions
2023-12-20 10:24:10,914:INFO:Copying training dataset
2023-12-20 10:24:10,918:INFO:Checking base model
2023-12-20 10:24:10,918:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 10:24:10,920:INFO:Declaring metric variables
2023-12-20 10:24:10,923:INFO:Defining Hyperparameters
2023-12-20 10:24:11,001:INFO:Tuning with n_jobs=-1
2023-12-20 10:24:11,001:INFO:Initializing RandomizedSearchCV
2023-12-20 10:24:22,430:INFO:best_params: {'actual_estimator__reg_lambda': 0.7, 'actual_estimator__reg_alpha': 0.3, 'actual_estimator__num_leaves': 256, 'actual_estimator__n_estimators': 270, 'actual_estimator__min_split_gain': 0.7, 'actual_estimator__min_child_samples': 31, 'actual_estimator__learning_rate': 0.15, 'actual_estimator__feature_fraction': 0.5, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 1.0}
2023-12-20 10:24:22,430:INFO:Hyperparameter search completed
2023-12-20 10:24:22,430:INFO:SubProcess create_model() called ==================================
2023-12-20 10:24:22,430:INFO:Initializing create_model()
2023-12-20 10:24:22,430:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001E4FFE60590>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.7, 'reg_alpha': 0.3, 'num_leaves': 256, 'n_estimators': 270, 'min_split_gain': 0.7, 'min_child_samples': 31, 'learning_rate': 0.15, 'feature_fraction': 0.5, 'bagging_freq': 2, 'bagging_fraction': 1.0})
2023-12-20 10:24:22,430:INFO:Checking exceptions
2023-12-20 10:24:22,430:INFO:Importing libraries
2023-12-20 10:24:22,430:INFO:Copying training dataset
2023-12-20 10:24:22,445:INFO:Defining folds
2023-12-20 10:24:22,445:INFO:Declaring metric variables
2023-12-20 10:24:22,445:INFO:Importing untrained model
2023-12-20 10:24:22,445:INFO:Declaring custom model
2023-12-20 10:24:22,457:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:24:22,458:INFO:Starting cross validation
2023-12-20 10:24:22,458:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:24:24,456:INFO:Calculating mean and std
2023-12-20 10:24:24,457:INFO:Creating metrics dataframe
2023-12-20 10:24:24,457:INFO:Finalizing model
2023-12-20 10:24:24,480:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-12-20 10:24:24,480:INFO:[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0
2023-12-20 10:24:24,480:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-12-20 10:24:24,495:INFO:[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5
2023-12-20 10:24:24,495:INFO:[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0
2023-12-20 10:24:24,495:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-12-20 10:24:24,495:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001938 seconds.
2023-12-20 10:24:24,495:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-20 10:24:24,495:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-20 10:24:24,495:INFO:[LightGBM] [Info] Total Bins 1798
2023-12-20 10:24:24,502:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 44
2023-12-20 10:24:24,502:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:24:24,502:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:24:24,502:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:24:24,522:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,540:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,566:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,581:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,589:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,615:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,639:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,656:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,677:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,715:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,738:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,745:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,776:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,789:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,895:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,923:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,956:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,965:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,973:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,974:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,976:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,976:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,979:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,980:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,981:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,981:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,982:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,983:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,984:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,984:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,985:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,986:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,988:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,988:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,989:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,991:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,993:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,993:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,994:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:24,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,006:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,015:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,024:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,025:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,027:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,028:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,056:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,057:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,073:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,073:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,074:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,074:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,075:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,076:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,076:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,076:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,077:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,078:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,078:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,079:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,079:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,080:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,081:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,081:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,082:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,082:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,083:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,083:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,084:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,085:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,086:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,086:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,086:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,087:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,088:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,089:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:24:25,091:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 10:24:25,107:INFO:Uploading results into container
2023-12-20 10:24:25,108:INFO:Uploading model into container now
2023-12-20 10:24:25,108:INFO:_master_model_container: 16
2023-12-20 10:24:25,109:INFO:_display_container: 3
2023-12-20 10:24:25,109:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=31, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=270, n_jobs=-1, num_leaves=256, objective=None,
               random_state=873, reg_alpha=0.3, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:24:25,109:INFO:create_model() successfully completed......................................
2023-12-20 10:24:25,215:INFO:SubProcess create_model() end ==================================
2023-12-20 10:24:25,215:INFO:choose_better activated
2023-12-20 10:24:25,215:INFO:SubProcess create_model() called ==================================
2023-12-20 10:24:25,225:INFO:Initializing create_model()
2023-12-20 10:24:25,225:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:24:25,225:INFO:Checking exceptions
2023-12-20 10:24:25,225:INFO:Importing libraries
2023-12-20 10:24:25,225:INFO:Copying training dataset
2023-12-20 10:24:25,225:INFO:Defining folds
2023-12-20 10:24:25,225:INFO:Declaring metric variables
2023-12-20 10:24:25,225:INFO:Importing untrained model
2023-12-20 10:24:25,225:INFO:Declaring custom model
2023-12-20 10:24:25,225:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:24:25,225:INFO:Starting cross validation
2023-12-20 10:24:25,225:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:24:28,525:INFO:Calculating mean and std
2023-12-20 10:24:28,525:INFO:Creating metrics dataframe
2023-12-20 10:24:28,525:INFO:Finalizing model
2023-12-20 10:24:28,563:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002133 seconds.
2023-12-20 10:24:28,563:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-20 10:24:28,563:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-20 10:24:28,563:INFO:[LightGBM] [Info] Total Bins 1798
2023-12-20 10:24:28,563:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 44
2023-12-20 10:24:28,575:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:24:28,575:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:24:28,575:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:24:28,957:INFO:Uploading results into container
2023-12-20 10:24:28,957:INFO:Uploading model into container now
2023-12-20 10:24:28,957:INFO:_master_model_container: 17
2023-12-20 10:24:28,957:INFO:_display_container: 4
2023-12-20 10:24:28,957:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:24:28,957:INFO:create_model() successfully completed......................................
2023-12-20 10:24:29,077:INFO:SubProcess create_model() end ==================================
2023-12-20 10:24:29,077:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=873, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8259
2023-12-20 10:24:29,077:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=31, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=270, n_jobs=-1, num_leaves=256, objective=None,
               random_state=873, reg_alpha=0.3, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8302
2023-12-20 10:24:29,077:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=31, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=270, n_jobs=-1, num_leaves=256, objective=None,
               random_state=873, reg_alpha=0.3, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 10:24:29,077:INFO:choose_better completed
2023-12-20 10:24:29,092:INFO:_master_model_container: 17
2023-12-20 10:24:29,092:INFO:_display_container: 3
2023-12-20 10:24:29,092:INFO:LGBMClassifier(bagging_fraction=1.0, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=31, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=270, n_jobs=-1, num_leaves=256, objective=None,
               random_state=873, reg_alpha=0.3, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:24:29,092:INFO:tune_model() successfully completed......................................
2023-12-20 10:24:29,201:INFO:Initializing evaluate_model()
2023-12-20 10:24:29,201:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=LGBMClassifier(bagging_fraction=1.0, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=31, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=270, n_jobs=-1, num_leaves=256, objective=None,
               random_state=873, reg_alpha=0.3, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 10:24:29,212:INFO:Initializing plot_model()
2023-12-20 10:24:29,212:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001E4FF7D5610>, estimator=LGBMClassifier(bagging_fraction=1.0, bagging_freq=2, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,
               importance_type='split', learning_rate=0.15, max_depth=-1,
               min_child_samples=31, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=270, n_jobs=-1, num_leaves=256, objective=None,
               random_state=873, reg_alpha=0.3, reg_lambda=0.7, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:24:29,212:INFO:Checking exceptions
2023-12-20 10:24:29,215:INFO:Preloading libraries
2023-12-20 10:24:29,227:INFO:Copying training dataset
2023-12-20 10:24:29,228:INFO:Plot type: pipeline
2023-12-20 10:24:29,357:INFO:Visual Rendered Successfully
2023-12-20 10:24:29,424:INFO:plot_model() successfully completed......................................
2023-12-20 10:28:19,453:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:28:19,453:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:28:19,453:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:28:19,453:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 10:28:22,704:INFO:PyCaret ClassificationExperiment
2023-12-20 10:28:22,704:INFO:Logging name: clf-default-name
2023-12-20 10:28:22,704:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 10:28:22,704:INFO:version 3.2.0
2023-12-20 10:28:22,704:INFO:Initializing setup()
2023-12-20 10:28:22,704:INFO:self.USI: 968d
2023-12-20 10:28:22,704:INFO:self._variable_keys: {'fold_generator', '_ml_usecase', 'logging_param', 'exp_name_log', 'X_test', 'y_test', 'data', 'X', 'html_param', 'is_multiclass', 'gpu_param', 'seed', '_available_plots', 'USI', 'fold_groups_param', 'y_train', 'idx', 'gpu_n_jobs_param', 'n_jobs_param', 'y', 'target_param', 'pipeline', 'fix_imbalance', 'exp_id', 'log_plots_param', 'fold_shuffle_param', 'X_train', 'memory'}
2023-12-20 10:28:22,704:INFO:Checking environment
2023-12-20 10:28:22,704:INFO:python_version: 3.11.5
2023-12-20 10:28:22,704:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 10:28:22,704:INFO:machine: AMD64
2023-12-20 10:28:22,704:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 10:28:22,704:INFO:Memory: svmem(total=16718413824, available=6467231744, percent=61.3, used=10251182080, free=6467231744)
2023-12-20 10:28:22,704:INFO:Physical Core: 12
2023-12-20 10:28:22,704:INFO:Logical Core: 16
2023-12-20 10:28:22,704:INFO:Checking libraries
2023-12-20 10:28:22,704:INFO:System:
2023-12-20 10:28:22,704:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 10:28:22,704:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 10:28:22,704:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 10:28:22,704:INFO:PyCaret required dependencies:
2023-12-20 10:28:23,274:INFO:                 pip: 23.2.1
2023-12-20 10:28:23,274:INFO:          setuptools: 68.0.0
2023-12-20 10:28:23,274:INFO:             pycaret: 3.2.0
2023-12-20 10:28:23,274:INFO:             IPython: 8.15.0
2023-12-20 10:28:23,274:INFO:          ipywidgets: 8.0.4
2023-12-20 10:28:23,274:INFO:                tqdm: 4.65.0
2023-12-20 10:28:23,274:INFO:               numpy: 1.24.3
2023-12-20 10:28:23,274:INFO:              pandas: 1.5.3
2023-12-20 10:28:23,274:INFO:              jinja2: 3.1.2
2023-12-20 10:28:23,274:INFO:               scipy: 1.10.1
2023-12-20 10:28:23,274:INFO:              joblib: 1.2.0
2023-12-20 10:28:23,274:INFO:             sklearn: 1.2.1
2023-12-20 10:28:23,274:INFO:                pyod: 1.1.2
2023-12-20 10:28:23,274:INFO:            imblearn: 0.11.0
2023-12-20 10:28:23,274:INFO:   category_encoders: 2.6.3
2023-12-20 10:28:23,274:INFO:            lightgbm: 4.1.0
2023-12-20 10:28:23,274:INFO:               numba: 0.57.1
2023-12-20 10:28:23,274:INFO:            requests: 2.31.0
2023-12-20 10:28:23,274:INFO:          matplotlib: 3.6.0
2023-12-20 10:28:23,274:INFO:          scikitplot: 0.3.7
2023-12-20 10:28:23,274:INFO:         yellowbrick: 1.5
2023-12-20 10:28:23,274:INFO:              plotly: 5.9.0
2023-12-20 10:28:23,274:INFO:    plotly-resampler: Not installed
2023-12-20 10:28:23,274:INFO:             kaleido: 0.2.1
2023-12-20 10:28:23,274:INFO:           schemdraw: 0.15
2023-12-20 10:28:23,274:INFO:         statsmodels: 0.14.0
2023-12-20 10:28:23,274:INFO:              sktime: 0.21.1
2023-12-20 10:28:23,274:INFO:               tbats: 1.1.3
2023-12-20 10:28:23,274:INFO:            pmdarima: 2.0.4
2023-12-20 10:28:23,274:INFO:              psutil: 5.9.0
2023-12-20 10:28:23,274:INFO:          markupsafe: 2.1.1
2023-12-20 10:28:23,274:INFO:             pickle5: Not installed
2023-12-20 10:28:23,274:INFO:         cloudpickle: 2.2.1
2023-12-20 10:28:23,274:INFO:         deprecation: 2.1.0
2023-12-20 10:28:23,274:INFO:              xxhash: 2.0.2
2023-12-20 10:28:23,274:INFO:           wurlitzer: Not installed
2023-12-20 10:28:23,274:INFO:PyCaret optional dependencies:
2023-12-20 10:28:23,410:INFO:                shap: Not installed
2023-12-20 10:28:23,410:INFO:           interpret: Not installed
2023-12-20 10:28:23,410:INFO:                umap: Not installed
2023-12-20 10:28:23,410:INFO:     ydata_profiling: Not installed
2023-12-20 10:28:23,410:INFO:  explainerdashboard: Not installed
2023-12-20 10:28:23,410:INFO:             autoviz: Not installed
2023-12-20 10:28:23,410:INFO:           fairlearn: Not installed
2023-12-20 10:28:23,410:INFO:          deepchecks: Not installed
2023-12-20 10:28:23,410:INFO:             xgboost: 2.0.2
2023-12-20 10:28:23,410:INFO:            catboost: Not installed
2023-12-20 10:28:23,410:INFO:              kmodes: Not installed
2023-12-20 10:28:23,410:INFO:             mlxtend: Not installed
2023-12-20 10:28:23,410:INFO:       statsforecast: Not installed
2023-12-20 10:28:23,410:INFO:        tune_sklearn: Not installed
2023-12-20 10:28:23,410:INFO:                 ray: Not installed
2023-12-20 10:28:23,410:INFO:            hyperopt: Not installed
2023-12-20 10:28:23,410:INFO:              optuna: Not installed
2023-12-20 10:28:23,410:INFO:               skopt: Not installed
2023-12-20 10:28:23,410:INFO:              mlflow: Not installed
2023-12-20 10:28:23,410:INFO:              gradio: Not installed
2023-12-20 10:28:23,410:INFO:             fastapi: Not installed
2023-12-20 10:28:23,410:INFO:             uvicorn: Not installed
2023-12-20 10:28:23,410:INFO:              m2cgen: Not installed
2023-12-20 10:28:23,410:INFO:           evidently: Not installed
2023-12-20 10:28:23,410:INFO:               fugue: Not installed
2023-12-20 10:28:23,410:INFO:           streamlit: Not installed
2023-12-20 10:28:23,410:INFO:             prophet: Not installed
2023-12-20 10:28:23,410:INFO:None
2023-12-20 10:28:23,410:INFO:Set up data.
2023-12-20 10:28:23,427:INFO:Set up folding strategy.
2023-12-20 10:28:23,427:INFO:Set up train/test split.
2023-12-20 10:28:23,427:INFO:Set up index.
2023-12-20 10:28:23,427:INFO:Assigning column types.
2023-12-20 10:28:23,439:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 10:28:23,461:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:28:23,469:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:28:23,487:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:28:23,487:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:28:23,512:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:28:23,519:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:28:23,520:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:28:23,535:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:28:23,535:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 10:28:23,553:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:28:23,570:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:28:23,570:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:28:23,602:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:28:23,619:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:28:23,620:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:28:23,620:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 10:28:23,654:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:28:23,654:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:28:23,702:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:28:23,703:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:28:23,703:INFO:Preparing preprocessing pipeline...
2023-12-20 10:28:23,703:INFO:Set up simple imputation.
2023-12-20 10:28:23,703:INFO:Set up column name cleaning.
2023-12-20 10:28:23,720:INFO:Finished creating preprocessing pipeline.
2023-12-20 10:28:23,720:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Platelets_c', 'Bilirubin_c',
                                             'Cholesterol_c', 'Albumin_c',
                                             'Copper_c', 'Alk_Pho...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 10:28:23,720:INFO:Creating final display dataframe.
2023-12-20 10:28:23,770:INFO:Setup _display_container:                     Description             Value
0                    Session id              8284
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 35)
4        Transformed data shape        (7905, 35)
5   Transformed train set shape        (6324, 35)
6    Transformed test set shape        (1581, 35)
7              Numeric features                34
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              968d
2023-12-20 10:28:23,824:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:28:23,824:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:28:23,870:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:28:23,872:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:28:23,874:INFO:setup() successfully completed in 1.32s...............
2023-12-20 10:28:23,895:INFO:Initializing compare_models()
2023-12-20 10:28:23,895:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 10:28:23,896:INFO:Checking exceptions
2023-12-20 10:28:23,898:INFO:Preparing display monitor
2023-12-20 10:28:23,915:INFO:Initializing Logistic Regression
2023-12-20 10:28:23,915:INFO:Total runtime is 0.0 minutes
2023-12-20 10:28:23,918:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:23,919:INFO:Initializing create_model()
2023-12-20 10:28:23,919:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:23,919:INFO:Checking exceptions
2023-12-20 10:28:23,919:INFO:Importing libraries
2023-12-20 10:28:23,919:INFO:Copying training dataset
2023-12-20 10:28:23,925:INFO:Defining folds
2023-12-20 10:28:23,925:INFO:Declaring metric variables
2023-12-20 10:28:23,926:INFO:Importing untrained model
2023-12-20 10:28:23,926:INFO:Logistic Regression Imported successfully
2023-12-20 10:28:23,926:INFO:Starting cross validation
2023-12-20 10:28:23,937:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:27,487:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:27,537:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:27,537:INFO:Calculating mean and std
2023-12-20 10:28:27,554:INFO:Creating metrics dataframe
2023-12-20 10:28:27,558:INFO:Uploading results into container
2023-12-20 10:28:27,558:INFO:Uploading model into container now
2023-12-20 10:28:27,558:INFO:_master_model_container: 1
2023-12-20 10:28:27,558:INFO:_display_container: 2
2023-12-20 10:28:27,558:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8284, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 10:28:27,558:INFO:create_model() successfully completed......................................
2023-12-20 10:28:27,637:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:27,637:INFO:Creating metrics dataframe
2023-12-20 10:28:27,637:INFO:Initializing K Neighbors Classifier
2023-12-20 10:28:27,637:INFO:Total runtime is 0.06203043858210246 minutes
2023-12-20 10:28:27,637:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:27,637:INFO:Initializing create_model()
2023-12-20 10:28:27,637:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:27,637:INFO:Checking exceptions
2023-12-20 10:28:27,637:INFO:Importing libraries
2023-12-20 10:28:27,637:INFO:Copying training dataset
2023-12-20 10:28:27,653:INFO:Defining folds
2023-12-20 10:28:27,653:INFO:Declaring metric variables
2023-12-20 10:28:27,658:INFO:Importing untrained model
2023-12-20 10:28:27,658:INFO:K Neighbors Classifier Imported successfully
2023-12-20 10:28:27,658:INFO:Starting cross validation
2023-12-20 10:28:27,658:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:30,646:INFO:Calculating mean and std
2023-12-20 10:28:30,648:INFO:Creating metrics dataframe
2023-12-20 10:28:30,651:INFO:Uploading results into container
2023-12-20 10:28:30,651:INFO:Uploading model into container now
2023-12-20 10:28:30,652:INFO:_master_model_container: 2
2023-12-20 10:28:30,652:INFO:_display_container: 2
2023-12-20 10:28:30,652:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 10:28:30,652:INFO:create_model() successfully completed......................................
2023-12-20 10:28:30,720:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:30,736:INFO:Creating metrics dataframe
2023-12-20 10:28:30,743:INFO:Initializing Naive Bayes
2023-12-20 10:28:30,743:INFO:Total runtime is 0.11379087766011556 minutes
2023-12-20 10:28:30,745:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:30,745:INFO:Initializing create_model()
2023-12-20 10:28:30,746:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:30,746:INFO:Checking exceptions
2023-12-20 10:28:30,746:INFO:Importing libraries
2023-12-20 10:28:30,746:INFO:Copying training dataset
2023-12-20 10:28:30,746:INFO:Defining folds
2023-12-20 10:28:30,746:INFO:Declaring metric variables
2023-12-20 10:28:30,746:INFO:Importing untrained model
2023-12-20 10:28:30,753:INFO:Naive Bayes Imported successfully
2023-12-20 10:28:30,753:INFO:Starting cross validation
2023-12-20 10:28:30,753:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:33,628:INFO:Calculating mean and std
2023-12-20 10:28:33,628:INFO:Creating metrics dataframe
2023-12-20 10:28:33,628:INFO:Uploading results into container
2023-12-20 10:28:33,628:INFO:Uploading model into container now
2023-12-20 10:28:33,628:INFO:_master_model_container: 3
2023-12-20 10:28:33,628:INFO:_display_container: 2
2023-12-20 10:28:33,628:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 10:28:33,635:INFO:create_model() successfully completed......................................
2023-12-20 10:28:33,708:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:33,708:INFO:Creating metrics dataframe
2023-12-20 10:28:33,711:INFO:Initializing Decision Tree Classifier
2023-12-20 10:28:33,711:INFO:Total runtime is 0.16325945059458413 minutes
2023-12-20 10:28:33,711:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:33,711:INFO:Initializing create_model()
2023-12-20 10:28:33,711:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:33,711:INFO:Checking exceptions
2023-12-20 10:28:33,711:INFO:Importing libraries
2023-12-20 10:28:33,711:INFO:Copying training dataset
2023-12-20 10:28:33,722:INFO:Defining folds
2023-12-20 10:28:33,722:INFO:Declaring metric variables
2023-12-20 10:28:33,725:INFO:Importing untrained model
2023-12-20 10:28:33,727:INFO:Decision Tree Classifier Imported successfully
2023-12-20 10:28:33,729:INFO:Starting cross validation
2023-12-20 10:28:33,729:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:36,019:INFO:Calculating mean and std
2023-12-20 10:28:36,019:INFO:Creating metrics dataframe
2023-12-20 10:28:36,019:INFO:Uploading results into container
2023-12-20 10:28:36,019:INFO:Uploading model into container now
2023-12-20 10:28:36,019:INFO:_master_model_container: 4
2023-12-20 10:28:36,019:INFO:_display_container: 2
2023-12-20 10:28:36,019:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8284, splitter='best')
2023-12-20 10:28:36,019:INFO:create_model() successfully completed......................................
2023-12-20 10:28:36,107:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:36,107:INFO:Creating metrics dataframe
2023-12-20 10:28:36,122:INFO:Initializing SVM - Linear Kernel
2023-12-20 10:28:36,122:INFO:Total runtime is 0.2034454782803853 minutes
2023-12-20 10:28:36,122:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:36,122:INFO:Initializing create_model()
2023-12-20 10:28:36,122:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:36,122:INFO:Checking exceptions
2023-12-20 10:28:36,122:INFO:Importing libraries
2023-12-20 10:28:36,122:INFO:Copying training dataset
2023-12-20 10:28:36,122:INFO:Defining folds
2023-12-20 10:28:36,122:INFO:Declaring metric variables
2023-12-20 10:28:36,138:INFO:Importing untrained model
2023-12-20 10:28:36,139:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 10:28:36,157:INFO:Starting cross validation
2023-12-20 10:28:36,157:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:36,279:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:28:36,285:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,285:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:28:36,285:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:28:36,285:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,300:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,304:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:28:36,307:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,311:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:28:36,314:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,317:INFO:Calculating mean and std
2023-12-20 10:28:36,318:INFO:Creating metrics dataframe
2023-12-20 10:28:36,319:INFO:Uploading results into container
2023-12-20 10:28:36,320:INFO:Uploading model into container now
2023-12-20 10:28:36,320:INFO:_master_model_container: 5
2023-12-20 10:28:36,320:INFO:_display_container: 2
2023-12-20 10:28:36,321:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8284, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 10:28:36,321:INFO:create_model() successfully completed......................................
2023-12-20 10:28:36,378:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:36,378:INFO:Creating metrics dataframe
2023-12-20 10:28:36,393:INFO:Initializing Ridge Classifier
2023-12-20 10:28:36,393:INFO:Total runtime is 0.2079552054405212 minutes
2023-12-20 10:28:36,393:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:36,393:INFO:Initializing create_model()
2023-12-20 10:28:36,393:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:36,393:INFO:Checking exceptions
2023-12-20 10:28:36,393:INFO:Importing libraries
2023-12-20 10:28:36,393:INFO:Copying training dataset
2023-12-20 10:28:36,393:INFO:Defining folds
2023-12-20 10:28:36,393:INFO:Declaring metric variables
2023-12-20 10:28:36,393:INFO:Importing untrained model
2023-12-20 10:28:36,407:INFO:Ridge Classifier Imported successfully
2023-12-20 10:28:36,409:INFO:Starting cross validation
2023-12-20 10:28:36,409:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:36,449:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:28:36,449:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,449:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:28:36,449:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:28:36,449:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:28:36,460:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,462:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,464:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,465:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:28:36,465:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:36,465:INFO:Calculating mean and std
2023-12-20 10:28:36,465:INFO:Creating metrics dataframe
2023-12-20 10:28:36,465:INFO:Uploading results into container
2023-12-20 10:28:36,465:INFO:Uploading model into container now
2023-12-20 10:28:36,465:INFO:_master_model_container: 6
2023-12-20 10:28:36,465:INFO:_display_container: 2
2023-12-20 10:28:36,465:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=8284, solver='auto',
                tol=0.0001)
2023-12-20 10:28:36,465:INFO:create_model() successfully completed......................................
2023-12-20 10:28:36,533:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:36,533:INFO:Creating metrics dataframe
2023-12-20 10:28:36,550:INFO:Initializing Random Forest Classifier
2023-12-20 10:28:36,550:INFO:Total runtime is 0.21058179537455238 minutes
2023-12-20 10:28:36,550:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:36,550:INFO:Initializing create_model()
2023-12-20 10:28:36,550:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:36,550:INFO:Checking exceptions
2023-12-20 10:28:36,550:INFO:Importing libraries
2023-12-20 10:28:36,550:INFO:Copying training dataset
2023-12-20 10:28:36,561:INFO:Defining folds
2023-12-20 10:28:36,561:INFO:Declaring metric variables
2023-12-20 10:28:36,563:INFO:Importing untrained model
2023-12-20 10:28:36,566:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:28:36,570:INFO:Starting cross validation
2023-12-20 10:28:36,571:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:37,062:INFO:Calculating mean and std
2023-12-20 10:28:37,062:INFO:Creating metrics dataframe
2023-12-20 10:28:37,062:INFO:Uploading results into container
2023-12-20 10:28:37,062:INFO:Uploading model into container now
2023-12-20 10:28:37,062:INFO:_master_model_container: 7
2023-12-20 10:28:37,062:INFO:_display_container: 2
2023-12-20 10:28:37,062:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8284, verbose=0, warm_start=False)
2023-12-20 10:28:37,062:INFO:create_model() successfully completed......................................
2023-12-20 10:28:37,130:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:37,130:INFO:Creating metrics dataframe
2023-12-20 10:28:37,130:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 10:28:37,130:INFO:Total runtime is 0.22024724483489988 minutes
2023-12-20 10:28:37,146:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:37,146:INFO:Initializing create_model()
2023-12-20 10:28:37,146:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:37,146:INFO:Checking exceptions
2023-12-20 10:28:37,146:INFO:Importing libraries
2023-12-20 10:28:37,146:INFO:Copying training dataset
2023-12-20 10:28:37,146:INFO:Defining folds
2023-12-20 10:28:37,146:INFO:Declaring metric variables
2023-12-20 10:28:37,153:INFO:Importing untrained model
2023-12-20 10:28:37,154:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 10:28:37,154:INFO:Starting cross validation
2023-12-20 10:28:37,154:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:37,198:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:28:37,198:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:28:37,199:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:28:37,199:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:28:37,201:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:28:37,215:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:37,224:INFO:Calculating mean and std
2023-12-20 10:28:37,225:INFO:Creating metrics dataframe
2023-12-20 10:28:37,226:INFO:Uploading results into container
2023-12-20 10:28:37,227:INFO:Uploading model into container now
2023-12-20 10:28:37,227:INFO:_master_model_container: 8
2023-12-20 10:28:37,227:INFO:_display_container: 2
2023-12-20 10:28:37,227:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 10:28:37,227:INFO:create_model() successfully completed......................................
2023-12-20 10:28:37,298:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:37,298:INFO:Creating metrics dataframe
2023-12-20 10:28:37,308:INFO:Initializing Ada Boost Classifier
2023-12-20 10:28:37,308:INFO:Total runtime is 0.22321175336837767 minutes
2023-12-20 10:28:37,308:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:37,311:INFO:Initializing create_model()
2023-12-20 10:28:37,311:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:37,311:INFO:Checking exceptions
2023-12-20 10:28:37,311:INFO:Importing libraries
2023-12-20 10:28:37,311:INFO:Copying training dataset
2023-12-20 10:28:37,313:INFO:Defining folds
2023-12-20 10:28:37,313:INFO:Declaring metric variables
2023-12-20 10:28:37,318:INFO:Importing untrained model
2023-12-20 10:28:37,319:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:28:37,319:INFO:Starting cross validation
2023-12-20 10:28:37,325:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:37,628:INFO:Calculating mean and std
2023-12-20 10:28:37,628:INFO:Creating metrics dataframe
2023-12-20 10:28:37,628:INFO:Uploading results into container
2023-12-20 10:28:37,628:INFO:Uploading model into container now
2023-12-20 10:28:37,628:INFO:_master_model_container: 9
2023-12-20 10:28:37,628:INFO:_display_container: 2
2023-12-20 10:28:37,628:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8284)
2023-12-20 10:28:37,628:INFO:create_model() successfully completed......................................
2023-12-20 10:28:37,695:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:37,695:INFO:Creating metrics dataframe
2023-12-20 10:28:37,714:INFO:Initializing Gradient Boosting Classifier
2023-12-20 10:28:37,714:INFO:Total runtime is 0.22997486193974812 minutes
2023-12-20 10:28:37,716:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:37,716:INFO:Initializing create_model()
2023-12-20 10:28:37,716:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:37,716:INFO:Checking exceptions
2023-12-20 10:28:37,716:INFO:Importing libraries
2023-12-20 10:28:37,716:INFO:Copying training dataset
2023-12-20 10:28:37,716:INFO:Defining folds
2023-12-20 10:28:37,716:INFO:Declaring metric variables
2023-12-20 10:28:37,716:INFO:Importing untrained model
2023-12-20 10:28:37,716:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:28:37,730:INFO:Starting cross validation
2023-12-20 10:28:37,731:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:40,823:INFO:Calculating mean and std
2023-12-20 10:28:40,824:INFO:Creating metrics dataframe
2023-12-20 10:28:40,827:INFO:Uploading results into container
2023-12-20 10:28:40,827:INFO:Uploading model into container now
2023-12-20 10:28:40,827:INFO:_master_model_container: 10
2023-12-20 10:28:40,827:INFO:_display_container: 2
2023-12-20 10:28:40,828:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:28:40,828:INFO:create_model() successfully completed......................................
2023-12-20 10:28:40,901:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:40,901:INFO:Creating metrics dataframe
2023-12-20 10:28:40,913:INFO:Initializing Linear Discriminant Analysis
2023-12-20 10:28:40,913:INFO:Total runtime is 0.28329863548278805 minutes
2023-12-20 10:28:40,913:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:40,913:INFO:Initializing create_model()
2023-12-20 10:28:40,913:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:40,913:INFO:Checking exceptions
2023-12-20 10:28:40,913:INFO:Importing libraries
2023-12-20 10:28:40,913:INFO:Copying training dataset
2023-12-20 10:28:40,913:INFO:Defining folds
2023-12-20 10:28:40,913:INFO:Declaring metric variables
2023-12-20 10:28:40,913:INFO:Importing untrained model
2023-12-20 10:28:40,913:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 10:28:40,930:INFO:Starting cross validation
2023-12-20 10:28:40,931:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:40,994:INFO:Calculating mean and std
2023-12-20 10:28:40,995:INFO:Creating metrics dataframe
2023-12-20 10:28:40,997:INFO:Uploading results into container
2023-12-20 10:28:40,997:INFO:Uploading model into container now
2023-12-20 10:28:40,998:INFO:_master_model_container: 11
2023-12-20 10:28:40,998:INFO:_display_container: 2
2023-12-20 10:28:40,998:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 10:28:40,998:INFO:create_model() successfully completed......................................
2023-12-20 10:28:41,062:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:41,062:INFO:Creating metrics dataframe
2023-12-20 10:28:41,079:INFO:Initializing Extra Trees Classifier
2023-12-20 10:28:41,079:INFO:Total runtime is 0.28606109619140624 minutes
2023-12-20 10:28:41,083:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:41,083:INFO:Initializing create_model()
2023-12-20 10:28:41,083:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:41,083:INFO:Checking exceptions
2023-12-20 10:28:41,083:INFO:Importing libraries
2023-12-20 10:28:41,083:INFO:Copying training dataset
2023-12-20 10:28:41,085:INFO:Defining folds
2023-12-20 10:28:41,085:INFO:Declaring metric variables
2023-12-20 10:28:41,085:INFO:Importing untrained model
2023-12-20 10:28:41,085:INFO:Extra Trees Classifier Imported successfully
2023-12-20 10:28:41,097:INFO:Starting cross validation
2023-12-20 10:28:41,098:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:41,495:INFO:Calculating mean and std
2023-12-20 10:28:41,495:INFO:Creating metrics dataframe
2023-12-20 10:28:41,510:INFO:Uploading results into container
2023-12-20 10:28:41,510:INFO:Uploading model into container now
2023-12-20 10:28:41,510:INFO:_master_model_container: 12
2023-12-20 10:28:41,510:INFO:_display_container: 2
2023-12-20 10:28:41,513:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8284, verbose=0, warm_start=False)
2023-12-20 10:28:41,513:INFO:create_model() successfully completed......................................
2023-12-20 10:28:41,579:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:41,579:INFO:Creating metrics dataframe
2023-12-20 10:28:41,579:INFO:Initializing Extreme Gradient Boosting
2023-12-20 10:28:41,579:INFO:Total runtime is 0.29439172744750974 minutes
2023-12-20 10:28:41,579:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:41,579:INFO:Initializing create_model()
2023-12-20 10:28:41,579:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:41,579:INFO:Checking exceptions
2023-12-20 10:28:41,579:INFO:Importing libraries
2023-12-20 10:28:41,579:INFO:Copying training dataset
2023-12-20 10:28:41,594:INFO:Defining folds
2023-12-20 10:28:41,594:INFO:Declaring metric variables
2023-12-20 10:28:41,594:INFO:Importing untrained model
2023-12-20 10:28:41,594:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:28:41,606:INFO:Starting cross validation
2023-12-20 10:28:41,607:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:42,228:INFO:Calculating mean and std
2023-12-20 10:28:42,228:INFO:Creating metrics dataframe
2023-12-20 10:28:42,228:INFO:Uploading results into container
2023-12-20 10:28:42,228:INFO:Uploading model into container now
2023-12-20 10:28:42,228:INFO:_master_model_container: 13
2023-12-20 10:28:42,228:INFO:_display_container: 2
2023-12-20 10:28:42,228:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 10:28:42,228:INFO:create_model() successfully completed......................................
2023-12-20 10:28:42,295:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:42,295:INFO:Creating metrics dataframe
2023-12-20 10:28:42,295:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 10:28:42,295:INFO:Total runtime is 0.3063259442647298 minutes
2023-12-20 10:28:42,308:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:42,310:INFO:Initializing create_model()
2023-12-20 10:28:42,310:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:42,310:INFO:Checking exceptions
2023-12-20 10:28:42,310:INFO:Importing libraries
2023-12-20 10:28:42,310:INFO:Copying training dataset
2023-12-20 10:28:42,312:INFO:Defining folds
2023-12-20 10:28:42,312:INFO:Declaring metric variables
2023-12-20 10:28:42,315:INFO:Importing untrained model
2023-12-20 10:28:42,315:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:28:42,315:INFO:Starting cross validation
2023-12-20 10:28:42,315:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:45,452:INFO:Calculating mean and std
2023-12-20 10:28:45,460:INFO:Creating metrics dataframe
2023-12-20 10:28:45,464:INFO:Uploading results into container
2023-12-20 10:28:45,465:INFO:Uploading model into container now
2023-12-20 10:28:45,466:INFO:_master_model_container: 14
2023-12-20 10:28:45,466:INFO:_display_container: 2
2023-12-20 10:28:45,467:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8284, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:28:45,467:INFO:create_model() successfully completed......................................
2023-12-20 10:28:45,577:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:45,578:INFO:Creating metrics dataframe
2023-12-20 10:28:45,578:INFO:Initializing Dummy Classifier
2023-12-20 10:28:45,578:INFO:Total runtime is 0.3610393047332764 minutes
2023-12-20 10:28:45,578:INFO:SubProcess create_model() called ==================================
2023-12-20 10:28:45,578:INFO:Initializing create_model()
2023-12-20 10:28:45,578:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F9EC3210>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:45,578:INFO:Checking exceptions
2023-12-20 10:28:45,578:INFO:Importing libraries
2023-12-20 10:28:45,578:INFO:Copying training dataset
2023-12-20 10:28:45,593:INFO:Defining folds
2023-12-20 10:28:45,593:INFO:Declaring metric variables
2023-12-20 10:28:45,595:INFO:Importing untrained model
2023-12-20 10:28:45,597:INFO:Dummy Classifier Imported successfully
2023-12-20 10:28:45,599:INFO:Starting cross validation
2023-12-20 10:28:45,599:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:28:45,629:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:45,630:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:45,632:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:45,638:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:45,638:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:28:45,645:INFO:Calculating mean and std
2023-12-20 10:28:45,645:INFO:Creating metrics dataframe
2023-12-20 10:28:45,647:INFO:Uploading results into container
2023-12-20 10:28:45,647:INFO:Uploading model into container now
2023-12-20 10:28:45,648:INFO:_master_model_container: 15
2023-12-20 10:28:45,648:INFO:_display_container: 2
2023-12-20 10:28:45,648:INFO:DummyClassifier(constant=None, random_state=8284, strategy='prior')
2023-12-20 10:28:45,648:INFO:create_model() successfully completed......................................
2023-12-20 10:28:45,716:INFO:SubProcess create_model() end ==================================
2023-12-20 10:28:45,716:INFO:Creating metrics dataframe
2023-12-20 10:28:45,732:INFO:Initializing create_model()
2023-12-20 10:28:45,732:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:45,732:INFO:Checking exceptions
2023-12-20 10:28:45,732:INFO:Importing libraries
2023-12-20 10:28:45,732:INFO:Copying training dataset
2023-12-20 10:28:45,736:INFO:Defining folds
2023-12-20 10:28:45,736:INFO:Declaring metric variables
2023-12-20 10:28:45,736:INFO:Importing untrained model
2023-12-20 10:28:45,736:INFO:Declaring custom model
2023-12-20 10:28:45,736:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:28:45,736:INFO:Cross validation set to False
2023-12-20 10:28:45,736:INFO:Fitting Model
2023-12-20 10:28:48,896:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:28:48,896:INFO:create_model() successfully completed......................................
2023-12-20 10:28:48,982:INFO:Initializing create_model()
2023-12-20 10:28:48,982:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:48,982:INFO:Checking exceptions
2023-12-20 10:28:48,987:INFO:Importing libraries
2023-12-20 10:28:48,987:INFO:Copying training dataset
2023-12-20 10:28:48,990:INFO:Defining folds
2023-12-20 10:28:48,990:INFO:Declaring metric variables
2023-12-20 10:28:48,990:INFO:Importing untrained model
2023-12-20 10:28:48,990:INFO:Declaring custom model
2023-12-20 10:28:48,990:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:28:48,994:INFO:Cross validation set to False
2023-12-20 10:28:48,994:INFO:Fitting Model
2023-12-20 10:28:49,278:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 10:28:49,278:INFO:create_model() successfully completed......................................
2023-12-20 10:28:49,361:INFO:Initializing create_model()
2023-12-20 10:28:49,361:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8284, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:49,361:INFO:Checking exceptions
2023-12-20 10:28:49,361:INFO:Importing libraries
2023-12-20 10:28:49,361:INFO:Copying training dataset
2023-12-20 10:28:49,378:INFO:Defining folds
2023-12-20 10:28:49,378:INFO:Declaring metric variables
2023-12-20 10:28:49,378:INFO:Importing untrained model
2023-12-20 10:28:49,378:INFO:Declaring custom model
2023-12-20 10:28:49,378:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:28:49,378:INFO:Cross validation set to False
2023-12-20 10:28:49,378:INFO:Fitting Model
2023-12-20 10:28:49,578:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8284, verbose=0, warm_start=False)
2023-12-20 10:28:49,578:INFO:create_model() successfully completed......................................
2023-12-20 10:28:49,660:INFO:Initializing create_model()
2023-12-20 10:28:49,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8284, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:49,660:INFO:Checking exceptions
2023-12-20 10:28:49,663:INFO:Importing libraries
2023-12-20 10:28:49,663:INFO:Copying training dataset
2023-12-20 10:28:49,664:INFO:Defining folds
2023-12-20 10:28:49,664:INFO:Declaring metric variables
2023-12-20 10:28:49,664:INFO:Importing untrained model
2023-12-20 10:28:49,664:INFO:Declaring custom model
2023-12-20 10:28:49,664:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:28:49,664:INFO:Cross validation set to False
2023-12-20 10:28:49,664:INFO:Fitting Model
2023-12-20 10:28:49,688:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000413 seconds.
2023-12-20 10:28:49,688:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-20 10:28:49,688:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-20 10:28:49,688:INFO:[LightGBM] [Info] Total Bins 1789
2023-12-20 10:28:49,688:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 10:28:49,689:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:28:49,689:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:28:49,689:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:28:49,973:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8284, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:28:49,973:INFO:create_model() successfully completed......................................
2023-12-20 10:28:50,081:INFO:Initializing create_model()
2023-12-20 10:28:50,081:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8284), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:28:50,081:INFO:Checking exceptions
2023-12-20 10:28:50,088:INFO:Importing libraries
2023-12-20 10:28:50,088:INFO:Copying training dataset
2023-12-20 10:28:50,089:INFO:Defining folds
2023-12-20 10:28:50,089:INFO:Declaring metric variables
2023-12-20 10:28:50,089:INFO:Importing untrained model
2023-12-20 10:28:50,089:INFO:Declaring custom model
2023-12-20 10:28:50,089:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:28:50,089:INFO:Cross validation set to False
2023-12-20 10:28:50,089:INFO:Fitting Model
2023-12-20 10:28:50,345:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8284)
2023-12-20 10:28:50,345:INFO:create_model() successfully completed......................................
2023-12-20 10:28:50,446:INFO:_master_model_container: 15
2023-12-20 10:28:50,446:INFO:_display_container: 2
2023-12-20 10:28:50,446:INFO:[GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8284, verbose=0, warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=8284, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8284)]
2023-12-20 10:28:50,446:INFO:compare_models() successfully completed......................................
2023-12-20 10:28:50,516:INFO:Initializing tune_model()
2023-12-20 10:28:50,517:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 10:28:50,517:INFO:Checking exceptions
2023-12-20 10:28:50,531:INFO:Copying training dataset
2023-12-20 10:28:50,535:INFO:Checking base model
2023-12-20 10:28:50,535:INFO:Base model : Gradient Boosting Classifier
2023-12-20 10:28:50,538:INFO:Declaring metric variables
2023-12-20 10:28:50,540:INFO:Defining Hyperparameters
2023-12-20 10:28:50,614:INFO:Tuning with n_jobs=-1
2023-12-20 10:28:50,614:INFO:Initializing RandomizedSearchCV
2023-12-20 10:29:02,228:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__n_estimators': 90, 'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 2, 'actual_estimator__min_impurity_decrease': 0.02, 'actual_estimator__max_features': 'log2', 'actual_estimator__max_depth': 8, 'actual_estimator__learning_rate': 0.1}
2023-12-20 10:29:02,228:INFO:Hyperparameter search completed
2023-12-20 10:29:02,228:INFO:SubProcess create_model() called ==================================
2023-12-20 10:29:02,243:INFO:Initializing create_model()
2023-12-20 10:29:02,243:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5FA4BAE50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.7, 'n_estimators': 90, 'min_samples_split': 7, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.02, 'max_features': 'log2', 'max_depth': 8, 'learning_rate': 0.1})
2023-12-20 10:29:02,243:INFO:Checking exceptions
2023-12-20 10:29:02,243:INFO:Importing libraries
2023-12-20 10:29:02,243:INFO:Copying training dataset
2023-12-20 10:29:02,248:INFO:Defining folds
2023-12-20 10:29:02,248:INFO:Declaring metric variables
2023-12-20 10:29:02,248:INFO:Importing untrained model
2023-12-20 10:29:02,248:INFO:Declaring custom model
2023-12-20 10:29:02,248:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:29:02,248:INFO:Starting cross validation
2023-12-20 10:29:02,248:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:29:03,794:INFO:Calculating mean and std
2023-12-20 10:29:03,794:INFO:Creating metrics dataframe
2023-12-20 10:29:03,794:INFO:Finalizing model
2023-12-20 10:29:05,195:INFO:Uploading results into container
2023-12-20 10:29:05,195:INFO:Uploading model into container now
2023-12-20 10:29:05,196:INFO:_master_model_container: 16
2023-12-20 10:29:05,197:INFO:_display_container: 3
2023-12-20 10:29:05,197:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=8,
                           max_features='log2', max_leaf_nodes=None,
                           min_impurity_decrease=0.02, min_samples_leaf=2,
                           min_samples_split=7, min_weight_fraction_leaf=0.0,
                           n_estimators=90, n_iter_no_change=None,
                           random_state=8284, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:29:05,197:INFO:create_model() successfully completed......................................
2023-12-20 10:29:05,273:INFO:SubProcess create_model() end ==================================
2023-12-20 10:29:05,273:INFO:choose_better activated
2023-12-20 10:29:05,278:INFO:SubProcess create_model() called ==================================
2023-12-20 10:29:05,278:INFO:Initializing create_model()
2023-12-20 10:29:05,278:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:29:05,278:INFO:Checking exceptions
2023-12-20 10:29:05,278:INFO:Importing libraries
2023-12-20 10:29:05,278:INFO:Copying training dataset
2023-12-20 10:29:05,278:INFO:Defining folds
2023-12-20 10:29:05,278:INFO:Declaring metric variables
2023-12-20 10:29:05,278:INFO:Importing untrained model
2023-12-20 10:29:05,278:INFO:Declaring custom model
2023-12-20 10:29:05,278:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:29:05,278:INFO:Starting cross validation
2023-12-20 10:29:05,278:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:29:08,536:INFO:Calculating mean and std
2023-12-20 10:29:08,536:INFO:Creating metrics dataframe
2023-12-20 10:29:08,537:INFO:Finalizing model
2023-12-20 10:29:11,780:INFO:Uploading results into container
2023-12-20 10:29:11,796:INFO:Uploading model into container now
2023-12-20 10:29:11,796:INFO:_master_model_container: 17
2023-12-20 10:29:11,796:INFO:_display_container: 4
2023-12-20 10:29:11,796:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:29:11,796:INFO:create_model() successfully completed......................................
2023-12-20 10:29:11,882:INFO:SubProcess create_model() end ==================================
2023-12-20 10:29:11,883:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8259
2023-12-20 10:29:11,883:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=8,
                           max_features='log2', max_leaf_nodes=None,
                           min_impurity_decrease=0.02, min_samples_leaf=2,
                           min_samples_split=7, min_weight_fraction_leaf=0.0,
                           n_estimators=90, n_iter_no_change=None,
                           random_state=8284, subsample=0.7, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8218
2023-12-20 10:29:11,884:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2023-12-20 10:29:11,884:INFO:choose_better completed
2023-12-20 10:29:11,884:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-12-20 10:29:11,887:INFO:_master_model_container: 17
2023-12-20 10:29:11,887:INFO:_display_container: 3
2023-12-20 10:29:11,887:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:29:11,887:INFO:tune_model() successfully completed......................................
2023-12-20 10:29:11,994:INFO:Initializing evaluate_model()
2023-12-20 10:29:11,994:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 10:29:12,021:INFO:Initializing plot_model()
2023-12-20 10:29:12,021:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5F8BAA110>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8284, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:29:12,021:INFO:Checking exceptions
2023-12-20 10:29:12,027:INFO:Preloading libraries
2023-12-20 10:29:12,039:INFO:Copying training dataset
2023-12-20 10:29:12,039:INFO:Plot type: pipeline
2023-12-20 10:29:12,130:INFO:Visual Rendered Successfully
2023-12-20 10:29:12,212:INFO:plot_model() successfully completed......................................
2023-12-20 10:57:46,505:INFO:PyCaret ClassificationExperiment
2023-12-20 10:57:46,505:INFO:Logging name: clf-default-name
2023-12-20 10:57:46,505:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 10:57:46,505:INFO:version 3.2.0
2023-12-20 10:57:46,505:INFO:Initializing setup()
2023-12-20 10:57:46,505:INFO:self.USI: 8275
2023-12-20 10:57:46,505:INFO:self._variable_keys: {'fold_generator', '_ml_usecase', 'logging_param', 'exp_name_log', 'X_test', 'y_test', 'data', 'X', 'html_param', 'is_multiclass', 'gpu_param', 'seed', '_available_plots', 'USI', 'fold_groups_param', 'y_train', 'idx', 'gpu_n_jobs_param', 'n_jobs_param', 'y', 'target_param', 'pipeline', 'fix_imbalance', 'exp_id', 'log_plots_param', 'fold_shuffle_param', 'X_train', 'memory'}
2023-12-20 10:57:46,505:INFO:Checking environment
2023-12-20 10:57:46,505:INFO:python_version: 3.11.5
2023-12-20 10:57:46,505:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 10:57:46,505:INFO:machine: AMD64
2023-12-20 10:57:46,505:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 10:57:46,505:INFO:Memory: svmem(total=16718413824, available=6737620992, percent=59.7, used=9980792832, free=6737620992)
2023-12-20 10:57:46,505:INFO:Physical Core: 12
2023-12-20 10:57:46,505:INFO:Logical Core: 16
2023-12-20 10:57:46,505:INFO:Checking libraries
2023-12-20 10:57:46,505:INFO:System:
2023-12-20 10:57:46,505:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 10:57:46,505:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 10:57:46,505:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 10:57:46,505:INFO:PyCaret required dependencies:
2023-12-20 10:57:46,505:INFO:                 pip: 23.2.1
2023-12-20 10:57:46,505:INFO:          setuptools: 68.0.0
2023-12-20 10:57:46,505:INFO:             pycaret: 3.2.0
2023-12-20 10:57:46,505:INFO:             IPython: 8.15.0
2023-12-20 10:57:46,505:INFO:          ipywidgets: 8.0.4
2023-12-20 10:57:46,505:INFO:                tqdm: 4.65.0
2023-12-20 10:57:46,505:INFO:               numpy: 1.24.3
2023-12-20 10:57:46,505:INFO:              pandas: 1.5.3
2023-12-20 10:57:46,505:INFO:              jinja2: 3.1.2
2023-12-20 10:57:46,505:INFO:               scipy: 1.10.1
2023-12-20 10:57:46,505:INFO:              joblib: 1.2.0
2023-12-20 10:57:46,505:INFO:             sklearn: 1.2.1
2023-12-20 10:57:46,505:INFO:                pyod: 1.1.2
2023-12-20 10:57:46,505:INFO:            imblearn: 0.11.0
2023-12-20 10:57:46,505:INFO:   category_encoders: 2.6.3
2023-12-20 10:57:46,505:INFO:            lightgbm: 4.1.0
2023-12-20 10:57:46,505:INFO:               numba: 0.57.1
2023-12-20 10:57:46,505:INFO:            requests: 2.31.0
2023-12-20 10:57:46,505:INFO:          matplotlib: 3.6.0
2023-12-20 10:57:46,505:INFO:          scikitplot: 0.3.7
2023-12-20 10:57:46,505:INFO:         yellowbrick: 1.5
2023-12-20 10:57:46,505:INFO:              plotly: 5.9.0
2023-12-20 10:57:46,505:INFO:    plotly-resampler: Not installed
2023-12-20 10:57:46,505:INFO:             kaleido: 0.2.1
2023-12-20 10:57:46,505:INFO:           schemdraw: 0.15
2023-12-20 10:57:46,505:INFO:         statsmodels: 0.14.0
2023-12-20 10:57:46,505:INFO:              sktime: 0.21.1
2023-12-20 10:57:46,505:INFO:               tbats: 1.1.3
2023-12-20 10:57:46,505:INFO:            pmdarima: 2.0.4
2023-12-20 10:57:46,505:INFO:              psutil: 5.9.0
2023-12-20 10:57:46,505:INFO:          markupsafe: 2.1.1
2023-12-20 10:57:46,505:INFO:             pickle5: Not installed
2023-12-20 10:57:46,505:INFO:         cloudpickle: 2.2.1
2023-12-20 10:57:46,505:INFO:         deprecation: 2.1.0
2023-12-20 10:57:46,505:INFO:              xxhash: 2.0.2
2023-12-20 10:57:46,505:INFO:           wurlitzer: Not installed
2023-12-20 10:57:46,505:INFO:PyCaret optional dependencies:
2023-12-20 10:57:46,505:INFO:                shap: Not installed
2023-12-20 10:57:46,505:INFO:           interpret: Not installed
2023-12-20 10:57:46,505:INFO:                umap: Not installed
2023-12-20 10:57:46,505:INFO:     ydata_profiling: Not installed
2023-12-20 10:57:46,505:INFO:  explainerdashboard: Not installed
2023-12-20 10:57:46,505:INFO:             autoviz: Not installed
2023-12-20 10:57:46,505:INFO:           fairlearn: Not installed
2023-12-20 10:57:46,505:INFO:          deepchecks: Not installed
2023-12-20 10:57:46,505:INFO:             xgboost: 2.0.2
2023-12-20 10:57:46,505:INFO:            catboost: Not installed
2023-12-20 10:57:46,505:INFO:              kmodes: Not installed
2023-12-20 10:57:46,505:INFO:             mlxtend: Not installed
2023-12-20 10:57:46,505:INFO:       statsforecast: Not installed
2023-12-20 10:57:46,505:INFO:        tune_sklearn: Not installed
2023-12-20 10:57:46,505:INFO:                 ray: Not installed
2023-12-20 10:57:46,505:INFO:            hyperopt: Not installed
2023-12-20 10:57:46,505:INFO:              optuna: Not installed
2023-12-20 10:57:46,505:INFO:               skopt: Not installed
2023-12-20 10:57:46,505:INFO:              mlflow: Not installed
2023-12-20 10:57:46,505:INFO:              gradio: Not installed
2023-12-20 10:57:46,505:INFO:             fastapi: Not installed
2023-12-20 10:57:46,505:INFO:             uvicorn: Not installed
2023-12-20 10:57:46,505:INFO:              m2cgen: Not installed
2023-12-20 10:57:46,505:INFO:           evidently: Not installed
2023-12-20 10:57:46,505:INFO:               fugue: Not installed
2023-12-20 10:57:46,505:INFO:           streamlit: Not installed
2023-12-20 10:57:46,505:INFO:             prophet: Not installed
2023-12-20 10:57:46,505:INFO:None
2023-12-20 10:57:46,505:INFO:Set up data.
2023-12-20 10:57:46,519:INFO:Set up folding strategy.
2023-12-20 10:57:46,519:INFO:Set up train/test split.
2023-12-20 10:57:46,519:INFO:Set up index.
2023-12-20 10:57:46,519:INFO:Assigning column types.
2023-12-20 10:57:46,519:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 10:57:46,554:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:57:46,554:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:57:46,565:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:57:46,565:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:57:46,594:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 10:57:46,594:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:57:46,610:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:57:46,610:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:57:46,610:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 10:57:46,635:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:57:46,651:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:57:46,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:57:46,669:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 10:57:46,693:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:57:46,693:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:57:46,693:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 10:57:46,741:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:57:46,742:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:57:46,767:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:57:46,783:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:57:46,783:INFO:Preparing preprocessing pipeline...
2023-12-20 10:57:46,783:INFO:Set up label encoding.
2023-12-20 10:57:46,783:INFO:Set up simple imputation.
2023-12-20 10:57:46,783:INFO:Set up column name cleaning.
2023-12-20 10:57:46,799:INFO:Finished creating preprocessing pipeline.
2023-12-20 10:57:46,815:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Plate...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 10:57:46,815:INFO:Creating final display dataframe.
2023-12-20 10:57:46,878:INFO:Setup _display_container:                     Description              Value
0                    Session id               7124
1                        Target             Status
2                   Target type         Multiclass
3                Target mapping  C: 0, CL: 1, D: 2
4           Original data shape         (7905, 35)
5        Transformed data shape         (7905, 35)
6   Transformed train set shape         (6324, 35)
7    Transformed test set shape         (1581, 35)
8              Numeric features                 34
9                    Preprocess               True
10              Imputation type             simple
11           Numeric imputation               mean
12       Categorical imputation               mode
13               Fold Generator    StratifiedKFold
14                  Fold Number                  5
15                     CPU Jobs                 -1
16                      Use GPU              False
17               Log Experiment              False
18              Experiment Name   clf-default-name
19                          USI               8275
2023-12-20 10:57:46,928:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:57:46,930:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:57:46,962:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 10:57:46,962:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 10:57:46,962:INFO:setup() successfully completed in 0.46s...............
2023-12-20 10:57:46,983:INFO:Initializing compare_models()
2023-12-20 10:57:46,983:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 10:57:46,983:INFO:Checking exceptions
2023-12-20 10:57:46,986:INFO:Preparing display monitor
2023-12-20 10:57:47,005:INFO:Initializing Logistic Regression
2023-12-20 10:57:47,005:INFO:Total runtime is 0.0 minutes
2023-12-20 10:57:47,008:INFO:SubProcess create_model() called ==================================
2023-12-20 10:57:47,008:INFO:Initializing create_model()
2023-12-20 10:57:47,008:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:57:47,008:INFO:Checking exceptions
2023-12-20 10:57:47,008:INFO:Importing libraries
2023-12-20 10:57:47,008:INFO:Copying training dataset
2023-12-20 10:57:47,008:INFO:Defining folds
2023-12-20 10:57:47,008:INFO:Declaring metric variables
2023-12-20 10:57:47,008:INFO:Importing untrained model
2023-12-20 10:57:47,008:INFO:Logistic Regression Imported successfully
2023-12-20 10:57:47,029:INFO:Starting cross validation
2023-12-20 10:57:47,030:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:57:50,629:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,635:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,638:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,640:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,646:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,652:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,653:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,662:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,663:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,666:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,672:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,674:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,674:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,674:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:50,695:INFO:Calculating mean and std
2023-12-20 10:57:50,695:INFO:Creating metrics dataframe
2023-12-20 10:57:50,711:INFO:Uploading results into container
2023-12-20 10:57:50,712:INFO:Uploading model into container now
2023-12-20 10:57:50,712:INFO:_master_model_container: 1
2023-12-20 10:57:50,712:INFO:_display_container: 2
2023-12-20 10:57:50,712:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=7124, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 10:57:50,712:INFO:create_model() successfully completed......................................
2023-12-20 10:57:50,811:INFO:SubProcess create_model() end ==================================
2023-12-20 10:57:50,811:INFO:Creating metrics dataframe
2023-12-20 10:57:50,811:INFO:Initializing K Neighbors Classifier
2023-12-20 10:57:50,811:INFO:Total runtime is 0.06342090368270874 minutes
2023-12-20 10:57:50,826:INFO:SubProcess create_model() called ==================================
2023-12-20 10:57:50,827:INFO:Initializing create_model()
2023-12-20 10:57:50,827:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:57:50,827:INFO:Checking exceptions
2023-12-20 10:57:50,827:INFO:Importing libraries
2023-12-20 10:57:50,827:INFO:Copying training dataset
2023-12-20 10:57:50,831:INFO:Defining folds
2023-12-20 10:57:50,831:INFO:Declaring metric variables
2023-12-20 10:57:50,831:INFO:Importing untrained model
2023-12-20 10:57:50,831:INFO:K Neighbors Classifier Imported successfully
2023-12-20 10:57:50,831:INFO:Starting cross validation
2023-12-20 10:57:50,831:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:57:53,757:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,757:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,757:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,757:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,757:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,773:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,773:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,773:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,773:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,789:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,798:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,798:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,798:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,798:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,814:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:53,829:INFO:Calculating mean and std
2023-12-20 10:57:53,829:INFO:Creating metrics dataframe
2023-12-20 10:57:53,829:INFO:Uploading results into container
2023-12-20 10:57:53,829:INFO:Uploading model into container now
2023-12-20 10:57:53,829:INFO:_master_model_container: 2
2023-12-20 10:57:53,829:INFO:_display_container: 2
2023-12-20 10:57:53,829:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 10:57:53,829:INFO:create_model() successfully completed......................................
2023-12-20 10:57:53,917:INFO:SubProcess create_model() end ==================================
2023-12-20 10:57:53,917:INFO:Creating metrics dataframe
2023-12-20 10:57:53,925:INFO:Initializing Naive Bayes
2023-12-20 10:57:53,925:INFO:Total runtime is 0.11532202164332073 minutes
2023-12-20 10:57:53,926:INFO:SubProcess create_model() called ==================================
2023-12-20 10:57:53,926:INFO:Initializing create_model()
2023-12-20 10:57:53,926:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:57:53,926:INFO:Checking exceptions
2023-12-20 10:57:53,926:INFO:Importing libraries
2023-12-20 10:57:53,926:INFO:Copying training dataset
2023-12-20 10:57:53,926:INFO:Defining folds
2023-12-20 10:57:53,926:INFO:Declaring metric variables
2023-12-20 10:57:53,926:INFO:Importing untrained model
2023-12-20 10:57:53,926:INFO:Naive Bayes Imported successfully
2023-12-20 10:57:53,942:INFO:Starting cross validation
2023-12-20 10:57:53,942:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:57:56,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,826:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,826:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,826:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,826:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,864:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,879:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,879:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:56,895:INFO:Calculating mean and std
2023-12-20 10:57:56,895:INFO:Creating metrics dataframe
2023-12-20 10:57:56,899:INFO:Uploading results into container
2023-12-20 10:57:56,899:INFO:Uploading model into container now
2023-12-20 10:57:56,899:INFO:_master_model_container: 3
2023-12-20 10:57:56,899:INFO:_display_container: 2
2023-12-20 10:57:56,899:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 10:57:56,899:INFO:create_model() successfully completed......................................
2023-12-20 10:57:56,964:INFO:SubProcess create_model() end ==================================
2023-12-20 10:57:56,964:INFO:Creating metrics dataframe
2023-12-20 10:57:56,980:INFO:Initializing Decision Tree Classifier
2023-12-20 10:57:56,980:INFO:Total runtime is 0.16623561382293703 minutes
2023-12-20 10:57:56,980:INFO:SubProcess create_model() called ==================================
2023-12-20 10:57:56,980:INFO:Initializing create_model()
2023-12-20 10:57:56,980:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:57:56,980:INFO:Checking exceptions
2023-12-20 10:57:56,980:INFO:Importing libraries
2023-12-20 10:57:56,980:INFO:Copying training dataset
2023-12-20 10:57:56,995:INFO:Defining folds
2023-12-20 10:57:56,995:INFO:Declaring metric variables
2023-12-20 10:57:56,995:INFO:Importing untrained model
2023-12-20 10:57:56,995:INFO:Decision Tree Classifier Imported successfully
2023-12-20 10:57:56,999:INFO:Starting cross validation
2023-12-20 10:57:57,004:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,075:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,093:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,094:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:57,097:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,233:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,234:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,234:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,234:INFO:Calculating mean and std
2023-12-20 10:57:59,250:INFO:Creating metrics dataframe
2023-12-20 10:57:59,250:INFO:Uploading results into container
2023-12-20 10:57:59,250:INFO:Uploading model into container now
2023-12-20 10:57:59,250:INFO:_master_model_container: 4
2023-12-20 10:57:59,250:INFO:_display_container: 2
2023-12-20 10:57:59,250:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=7124, splitter='best')
2023-12-20 10:57:59,250:INFO:create_model() successfully completed......................................
2023-12-20 10:57:59,326:INFO:SubProcess create_model() end ==================================
2023-12-20 10:57:59,326:INFO:Creating metrics dataframe
2023-12-20 10:57:59,342:INFO:Initializing SVM - Linear Kernel
2023-12-20 10:57:59,342:INFO:Total runtime is 0.2056073824564616 minutes
2023-12-20 10:57:59,342:INFO:SubProcess create_model() called ==================================
2023-12-20 10:57:59,342:INFO:Initializing create_model()
2023-12-20 10:57:59,342:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:57:59,342:INFO:Checking exceptions
2023-12-20 10:57:59,342:INFO:Importing libraries
2023-12-20 10:57:59,342:INFO:Copying training dataset
2023-12-20 10:57:59,342:INFO:Defining folds
2023-12-20 10:57:59,342:INFO:Declaring metric variables
2023-12-20 10:57:59,358:INFO:Importing untrained model
2023-12-20 10:57:59,361:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 10:57:59,365:INFO:Starting cross validation
2023-12-20 10:57:59,367:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:57:59,441:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:57:59,441:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,441:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:57:59,441:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,457:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,460:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:57:59,460:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,463:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,464:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:57:59,465:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,466:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,467:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,483:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,492:INFO:Calculating mean and std
2023-12-20 10:57:59,492:INFO:Creating metrics dataframe
2023-12-20 10:57:59,493:INFO:Uploading results into container
2023-12-20 10:57:59,493:INFO:Uploading model into container now
2023-12-20 10:57:59,493:INFO:_master_model_container: 5
2023-12-20 10:57:59,493:INFO:_display_container: 2
2023-12-20 10:57:59,493:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=7124, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 10:57:59,493:INFO:create_model() successfully completed......................................
2023-12-20 10:57:59,565:INFO:SubProcess create_model() end ==================================
2023-12-20 10:57:59,565:INFO:Creating metrics dataframe
2023-12-20 10:57:59,579:INFO:Initializing Ridge Classifier
2023-12-20 10:57:59,579:INFO:Total runtime is 0.20955208142598472 minutes
2023-12-20 10:57:59,591:INFO:SubProcess create_model() called ==================================
2023-12-20 10:57:59,591:INFO:Initializing create_model()
2023-12-20 10:57:59,591:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:57:59,592:INFO:Checking exceptions
2023-12-20 10:57:59,592:INFO:Importing libraries
2023-12-20 10:57:59,592:INFO:Copying training dataset
2023-12-20 10:57:59,601:INFO:Defining folds
2023-12-20 10:57:59,601:INFO:Declaring metric variables
2023-12-20 10:57:59,603:INFO:Importing untrained model
2023-12-20 10:57:59,603:INFO:Ridge Classifier Imported successfully
2023-12-20 10:57:59,612:INFO:Starting cross validation
2023-12-20 10:57:59,612:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:57:59,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:57:59,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:57:59,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:57:59,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:57:59,643:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,659:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,660:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,662:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:57:59,662:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,664:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,664:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,665:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:57:59,665:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 10:57:59,667:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,667:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:57:59,668:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,668:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,668:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,668:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:57:59,668:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,668:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,668:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:57:59,668:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:57:59,684:INFO:Calculating mean and std
2023-12-20 10:57:59,684:INFO:Creating metrics dataframe
2023-12-20 10:57:59,684:INFO:Uploading results into container
2023-12-20 10:57:59,684:INFO:Uploading model into container now
2023-12-20 10:57:59,684:INFO:_master_model_container: 6
2023-12-20 10:57:59,684:INFO:_display_container: 2
2023-12-20 10:57:59,684:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=7124, solver='auto',
                tol=0.0001)
2023-12-20 10:57:59,684:INFO:create_model() successfully completed......................................
2023-12-20 10:57:59,748:INFO:SubProcess create_model() end ==================================
2023-12-20 10:57:59,748:INFO:Creating metrics dataframe
2023-12-20 10:57:59,763:INFO:Initializing Random Forest Classifier
2023-12-20 10:57:59,763:INFO:Total runtime is 0.2126296639442444 minutes
2023-12-20 10:57:59,763:INFO:SubProcess create_model() called ==================================
2023-12-20 10:57:59,763:INFO:Initializing create_model()
2023-12-20 10:57:59,763:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:57:59,763:INFO:Checking exceptions
2023-12-20 10:57:59,763:INFO:Importing libraries
2023-12-20 10:57:59,763:INFO:Copying training dataset
2023-12-20 10:57:59,779:INFO:Defining folds
2023-12-20 10:57:59,779:INFO:Declaring metric variables
2023-12-20 10:57:59,779:INFO:Importing untrained model
2023-12-20 10:57:59,795:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:57:59,804:INFO:Starting cross validation
2023-12-20 10:57:59,810:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:00,261:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,261:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,276:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,292:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,292:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,305:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,308:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,308:INFO:Calculating mean and std
2023-12-20 10:58:00,308:INFO:Creating metrics dataframe
2023-12-20 10:58:00,308:INFO:Uploading results into container
2023-12-20 10:58:00,308:INFO:Uploading model into container now
2023-12-20 10:58:00,308:INFO:_master_model_container: 7
2023-12-20 10:58:00,308:INFO:_display_container: 2
2023-12-20 10:58:00,308:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7124, verbose=0, warm_start=False)
2023-12-20 10:58:00,308:INFO:create_model() successfully completed......................................
2023-12-20 10:58:00,375:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:00,375:INFO:Creating metrics dataframe
2023-12-20 10:58:00,390:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 10:58:00,390:INFO:Total runtime is 0.22307892243067426 minutes
2023-12-20 10:58:00,390:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:00,390:INFO:Initializing create_model()
2023-12-20 10:58:00,390:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:00,390:INFO:Checking exceptions
2023-12-20 10:58:00,390:INFO:Importing libraries
2023-12-20 10:58:00,390:INFO:Copying training dataset
2023-12-20 10:58:00,390:INFO:Defining folds
2023-12-20 10:58:00,390:INFO:Declaring metric variables
2023-12-20 10:58:00,406:INFO:Importing untrained model
2023-12-20 10:58:00,406:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 10:58:00,406:INFO:Starting cross validation
2023-12-20 10:58:00,406:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:00,437:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:58:00,437:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:58:00,437:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:58:00,437:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:58:00,453:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 10:58:00,453:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,453:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,453:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,469:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,484:INFO:Calculating mean and std
2023-12-20 10:58:00,484:INFO:Creating metrics dataframe
2023-12-20 10:58:00,484:INFO:Uploading results into container
2023-12-20 10:58:00,484:INFO:Uploading model into container now
2023-12-20 10:58:00,484:INFO:_master_model_container: 8
2023-12-20 10:58:00,484:INFO:_display_container: 2
2023-12-20 10:58:00,484:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 10:58:00,484:INFO:create_model() successfully completed......................................
2023-12-20 10:58:00,574:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:00,574:INFO:Creating metrics dataframe
2023-12-20 10:58:00,589:INFO:Initializing Ada Boost Classifier
2023-12-20 10:58:00,589:INFO:Total runtime is 0.2263936797777812 minutes
2023-12-20 10:58:00,589:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:00,589:INFO:Initializing create_model()
2023-12-20 10:58:00,589:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:00,589:INFO:Checking exceptions
2023-12-20 10:58:00,589:INFO:Importing libraries
2023-12-20 10:58:00,589:INFO:Copying training dataset
2023-12-20 10:58:00,589:INFO:Defining folds
2023-12-20 10:58:00,589:INFO:Declaring metric variables
2023-12-20 10:58:00,600:INFO:Importing untrained model
2023-12-20 10:58:00,601:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:58:00,605:INFO:Starting cross validation
2023-12-20 10:58:00,605:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:00,896:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,896:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,905:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,912:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,912:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,912:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,912:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,912:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,912:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,912:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,912:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,927:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,927:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,927:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,927:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:00,943:INFO:Calculating mean and std
2023-12-20 10:58:00,943:INFO:Creating metrics dataframe
2023-12-20 10:58:00,950:INFO:Uploading results into container
2023-12-20 10:58:00,950:INFO:Uploading model into container now
2023-12-20 10:58:00,951:INFO:_master_model_container: 9
2023-12-20 10:58:00,951:INFO:_display_container: 2
2023-12-20 10:58:00,951:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7124)
2023-12-20 10:58:00,951:INFO:create_model() successfully completed......................................
2023-12-20 10:58:01,025:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:01,025:INFO:Creating metrics dataframe
2023-12-20 10:58:01,027:INFO:Initializing Gradient Boosting Classifier
2023-12-20 10:58:01,027:INFO:Total runtime is 0.23369640509287518 minutes
2023-12-20 10:58:01,027:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:01,027:INFO:Initializing create_model()
2023-12-20 10:58:01,027:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:01,027:INFO:Checking exceptions
2023-12-20 10:58:01,027:INFO:Importing libraries
2023-12-20 10:58:01,027:INFO:Copying training dataset
2023-12-20 10:58:01,027:INFO:Defining folds
2023-12-20 10:58:01,027:INFO:Declaring metric variables
2023-12-20 10:58:01,027:INFO:Importing untrained model
2023-12-20 10:58:01,043:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:58:01,047:INFO:Starting cross validation
2023-12-20 10:58:01,048:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:04,110:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,110:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,110:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,125:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,141:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,141:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,162:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,166:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,169:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,171:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,173:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,176:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,176:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,176:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,176:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,191:INFO:Calculating mean and std
2023-12-20 10:58:04,191:INFO:Creating metrics dataframe
2023-12-20 10:58:04,191:INFO:Uploading results into container
2023-12-20 10:58:04,191:INFO:Uploading model into container now
2023-12-20 10:58:04,191:INFO:_master_model_container: 10
2023-12-20 10:58:04,191:INFO:_display_container: 2
2023-12-20 10:58:04,191:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7124, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:58:04,191:INFO:create_model() successfully completed......................................
2023-12-20 10:58:04,269:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:04,269:INFO:Creating metrics dataframe
2023-12-20 10:58:04,279:INFO:Initializing Linear Discriminant Analysis
2023-12-20 10:58:04,279:INFO:Total runtime is 0.28789285421371463 minutes
2023-12-20 10:58:04,291:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:04,291:INFO:Initializing create_model()
2023-12-20 10:58:04,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:04,292:INFO:Checking exceptions
2023-12-20 10:58:04,292:INFO:Importing libraries
2023-12-20 10:58:04,292:INFO:Copying training dataset
2023-12-20 10:58:04,294:INFO:Defining folds
2023-12-20 10:58:04,294:INFO:Declaring metric variables
2023-12-20 10:58:04,294:INFO:Importing untrained model
2023-12-20 10:58:04,294:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 10:58:04,294:INFO:Starting cross validation
2023-12-20 10:58:04,294:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:04,342:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,342:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,357:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,373:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,378:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,378:INFO:Calculating mean and std
2023-12-20 10:58:04,378:INFO:Creating metrics dataframe
2023-12-20 10:58:04,378:INFO:Uploading results into container
2023-12-20 10:58:04,378:INFO:Uploading model into container now
2023-12-20 10:58:04,378:INFO:_master_model_container: 11
2023-12-20 10:58:04,378:INFO:_display_container: 2
2023-12-20 10:58:04,378:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 10:58:04,378:INFO:create_model() successfully completed......................................
2023-12-20 10:58:04,452:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:04,452:INFO:Creating metrics dataframe
2023-12-20 10:58:04,468:INFO:Initializing Extra Trees Classifier
2023-12-20 10:58:04,468:INFO:Total runtime is 0.29103760321935024 minutes
2023-12-20 10:58:04,468:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:04,468:INFO:Initializing create_model()
2023-12-20 10:58:04,468:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:04,468:INFO:Checking exceptions
2023-12-20 10:58:04,468:INFO:Importing libraries
2023-12-20 10:58:04,468:INFO:Copying training dataset
2023-12-20 10:58:04,483:INFO:Defining folds
2023-12-20 10:58:04,483:INFO:Declaring metric variables
2023-12-20 10:58:04,489:INFO:Importing untrained model
2023-12-20 10:58:04,493:INFO:Extra Trees Classifier Imported successfully
2023-12-20 10:58:04,493:INFO:Starting cross validation
2023-12-20 10:58:04,493:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,861:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,877:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,877:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,877:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:04,893:INFO:Calculating mean and std
2023-12-20 10:58:04,893:INFO:Creating metrics dataframe
2023-12-20 10:58:04,893:INFO:Uploading results into container
2023-12-20 10:58:04,893:INFO:Uploading model into container now
2023-12-20 10:58:04,893:INFO:_master_model_container: 12
2023-12-20 10:58:04,893:INFO:_display_container: 2
2023-12-20 10:58:04,893:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=7124, verbose=0, warm_start=False)
2023-12-20 10:58:04,893:INFO:create_model() successfully completed......................................
2023-12-20 10:58:04,974:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:04,974:INFO:Creating metrics dataframe
2023-12-20 10:58:04,987:INFO:Initializing Extreme Gradient Boosting
2023-12-20 10:58:04,987:INFO:Total runtime is 0.29969158569971727 minutes
2023-12-20 10:58:04,992:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:04,993:INFO:Initializing create_model()
2023-12-20 10:58:04,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:04,993:INFO:Checking exceptions
2023-12-20 10:58:04,993:INFO:Importing libraries
2023-12-20 10:58:04,993:INFO:Copying training dataset
2023-12-20 10:58:04,997:INFO:Defining folds
2023-12-20 10:58:04,997:INFO:Declaring metric variables
2023-12-20 10:58:05,000:INFO:Importing untrained model
2023-12-20 10:58:05,000:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:58:05,000:INFO:Starting cross validation
2023-12-20 10:58:05,000:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:05,514:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,529:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,529:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,529:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,529:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,545:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,545:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,561:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,561:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,561:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,561:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,576:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,592:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,592:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,592:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:05,608:INFO:Calculating mean and std
2023-12-20 10:58:05,609:INFO:Creating metrics dataframe
2023-12-20 10:58:05,611:INFO:Uploading results into container
2023-12-20 10:58:05,612:INFO:Uploading model into container now
2023-12-20 10:58:05,612:INFO:_master_model_container: 13
2023-12-20 10:58:05,612:INFO:_display_container: 2
2023-12-20 10:58:05,612:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 10:58:05,612:INFO:create_model() successfully completed......................................
2023-12-20 10:58:05,674:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:05,674:INFO:Creating metrics dataframe
2023-12-20 10:58:05,690:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 10:58:05,690:INFO:Total runtime is 0.3114082574844361 minutes
2023-12-20 10:58:05,690:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:05,690:INFO:Initializing create_model()
2023-12-20 10:58:05,690:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:05,690:INFO:Checking exceptions
2023-12-20 10:58:05,690:INFO:Importing libraries
2023-12-20 10:58:05,690:INFO:Copying training dataset
2023-12-20 10:58:05,706:INFO:Defining folds
2023-12-20 10:58:05,706:INFO:Declaring metric variables
2023-12-20 10:58:05,713:INFO:Importing untrained model
2023-12-20 10:58:05,722:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:58:05,746:INFO:Starting cross validation
2023-12-20 10:58:05,746:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:08,525:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,525:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,541:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,541:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,541:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,541:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,541:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,556:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,556:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,885:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,901:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,901:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,901:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,901:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,918:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:08,933:INFO:Calculating mean and std
2023-12-20 10:58:08,933:INFO:Creating metrics dataframe
2023-12-20 10:58:08,933:INFO:Uploading results into container
2023-12-20 10:58:08,933:INFO:Uploading model into container now
2023-12-20 10:58:08,933:INFO:_master_model_container: 14
2023-12-20 10:58:08,933:INFO:_display_container: 2
2023-12-20 10:58:08,933:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:58:08,933:INFO:create_model() successfully completed......................................
2023-12-20 10:58:09,026:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:09,026:INFO:Creating metrics dataframe
2023-12-20 10:58:09,042:INFO:Initializing Dummy Classifier
2023-12-20 10:58:09,042:INFO:Total runtime is 0.36727309624354054 minutes
2023-12-20 10:58:09,042:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:09,042:INFO:Initializing create_model()
2023-12-20 10:58:09,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5F8C09290>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:09,042:INFO:Checking exceptions
2023-12-20 10:58:09,042:INFO:Importing libraries
2023-12-20 10:58:09,042:INFO:Copying training dataset
2023-12-20 10:58:09,042:INFO:Defining folds
2023-12-20 10:58:09,042:INFO:Declaring metric variables
2023-12-20 10:58:09,042:INFO:Importing untrained model
2023-12-20 10:58:09,042:INFO:Dummy Classifier Imported successfully
2023-12-20 10:58:09,060:INFO:Starting cross validation
2023-12-20 10:58:09,061:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,107:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:58:09,107:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,107:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,107:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 10:58:09,107:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:09,124:INFO:Calculating mean and std
2023-12-20 10:58:09,124:INFO:Creating metrics dataframe
2023-12-20 10:58:09,124:INFO:Uploading results into container
2023-12-20 10:58:09,124:INFO:Uploading model into container now
2023-12-20 10:58:09,124:INFO:_master_model_container: 15
2023-12-20 10:58:09,124:INFO:_display_container: 2
2023-12-20 10:58:09,124:INFO:DummyClassifier(constant=None, random_state=7124, strategy='prior')
2023-12-20 10:58:09,124:INFO:create_model() successfully completed......................................
2023-12-20 10:58:09,205:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:09,205:INFO:Creating metrics dataframe
2023-12-20 10:58:09,226:INFO:Initializing create_model()
2023-12-20 10:58:09,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:09,227:INFO:Checking exceptions
2023-12-20 10:58:09,227:INFO:Importing libraries
2023-12-20 10:58:09,227:INFO:Copying training dataset
2023-12-20 10:58:09,227:INFO:Defining folds
2023-12-20 10:58:09,227:INFO:Declaring metric variables
2023-12-20 10:58:09,227:INFO:Importing untrained model
2023-12-20 10:58:09,227:INFO:Declaring custom model
2023-12-20 10:58:09,227:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:58:09,227:INFO:Cross validation set to False
2023-12-20 10:58:09,227:INFO:Fitting Model
2023-12-20 10:58:09,243:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001235 seconds.
2023-12-20 10:58:09,243:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:58:09,243:INFO:[LightGBM] [Info] Total Bins 1783
2023-12-20 10:58:09,243:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 10:58:09,243:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:58:09,243:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:58:09,243:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:58:09,526:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:58:09,526:INFO:create_model() successfully completed......................................
2023-12-20 10:58:09,619:INFO:Initializing create_model()
2023-12-20 10:58:09,619:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7124, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:09,619:INFO:Checking exceptions
2023-12-20 10:58:09,621:INFO:Importing libraries
2023-12-20 10:58:09,621:INFO:Copying training dataset
2023-12-20 10:58:09,627:INFO:Defining folds
2023-12-20 10:58:09,627:INFO:Declaring metric variables
2023-12-20 10:58:09,627:INFO:Importing untrained model
2023-12-20 10:58:09,627:INFO:Declaring custom model
2023-12-20 10:58:09,627:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 10:58:09,627:INFO:Cross validation set to False
2023-12-20 10:58:09,627:INFO:Fitting Model
2023-12-20 10:58:12,793:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7124, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 10:58:12,793:INFO:create_model() successfully completed......................................
2023-12-20 10:58:12,888:INFO:Initializing create_model()
2023-12-20 10:58:12,888:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7124, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:12,888:INFO:Checking exceptions
2023-12-20 10:58:12,888:INFO:Importing libraries
2023-12-20 10:58:12,888:INFO:Copying training dataset
2023-12-20 10:58:12,903:INFO:Defining folds
2023-12-20 10:58:12,903:INFO:Declaring metric variables
2023-12-20 10:58:12,903:INFO:Importing untrained model
2023-12-20 10:58:12,903:INFO:Declaring custom model
2023-12-20 10:58:12,903:INFO:Random Forest Classifier Imported successfully
2023-12-20 10:58:12,903:INFO:Cross validation set to False
2023-12-20 10:58:12,903:INFO:Fitting Model
2023-12-20 10:58:13,126:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7124, verbose=0, warm_start=False)
2023-12-20 10:58:13,126:INFO:create_model() successfully completed......................................
2023-12-20 10:58:13,213:INFO:Initializing create_model()
2023-12-20 10:58:13,213:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:13,213:INFO:Checking exceptions
2023-12-20 10:58:13,213:INFO:Importing libraries
2023-12-20 10:58:13,213:INFO:Copying training dataset
2023-12-20 10:58:13,213:INFO:Defining folds
2023-12-20 10:58:13,213:INFO:Declaring metric variables
2023-12-20 10:58:13,213:INFO:Importing untrained model
2023-12-20 10:58:13,213:INFO:Declaring custom model
2023-12-20 10:58:13,213:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 10:58:13,213:INFO:Cross validation set to False
2023-12-20 10:58:13,213:INFO:Fitting Model
2023-12-20 10:58:13,507:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 10:58:13,507:INFO:create_model() successfully completed......................................
2023-12-20 10:58:13,628:INFO:Initializing create_model()
2023-12-20 10:58:13,628:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7124), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:13,628:INFO:Checking exceptions
2023-12-20 10:58:13,628:INFO:Importing libraries
2023-12-20 10:58:13,628:INFO:Copying training dataset
2023-12-20 10:58:13,628:INFO:Defining folds
2023-12-20 10:58:13,628:INFO:Declaring metric variables
2023-12-20 10:58:13,628:INFO:Importing untrained model
2023-12-20 10:58:13,628:INFO:Declaring custom model
2023-12-20 10:58:13,628:INFO:Ada Boost Classifier Imported successfully
2023-12-20 10:58:13,628:INFO:Cross validation set to False
2023-12-20 10:58:13,628:INFO:Fitting Model
2023-12-20 10:58:13,917:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7124)
2023-12-20 10:58:13,917:INFO:create_model() successfully completed......................................
2023-12-20 10:58:14,025:INFO:_master_model_container: 15
2023-12-20 10:58:14,025:INFO:_display_container: 2
2023-12-20 10:58:14,028:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7124, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7124, verbose=0, warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7124)]
2023-12-20 10:58:14,028:INFO:compare_models() successfully completed......................................
2023-12-20 10:58:14,045:INFO:Initializing tune_model()
2023-12-20 10:58:14,045:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 10:58:14,045:INFO:Checking exceptions
2023-12-20 10:58:14,084:INFO:Copying training dataset
2023-12-20 10:58:14,096:INFO:Checking base model
2023-12-20 10:58:14,097:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 10:58:14,104:INFO:Declaring metric variables
2023-12-20 10:58:14,111:INFO:Defining Hyperparameters
2023-12-20 10:58:14,192:INFO:Tuning with n_jobs=-1
2023-12-20 10:58:14,192:INFO:Initializing RandomizedSearchCV
2023-12-20 10:58:35,708:INFO:best_params: {'actual_estimator__reg_lambda': 4, 'actual_estimator__reg_alpha': 0.5, 'actual_estimator__num_leaves': 60, 'actual_estimator__n_estimators': 90, 'actual_estimator__min_split_gain': 0.8, 'actual_estimator__min_child_samples': 91, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 1.0, 'actual_estimator__bagging_freq': 3, 'actual_estimator__bagging_fraction': 0.8}
2023-12-20 10:58:35,708:INFO:Hyperparameter search completed
2023-12-20 10:58:35,708:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:35,724:INFO:Initializing create_model()
2023-12-20 10:58:35,724:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001C5FA06F610>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 4, 'reg_alpha': 0.5, 'num_leaves': 60, 'n_estimators': 90, 'min_split_gain': 0.8, 'min_child_samples': 91, 'learning_rate': 0.1, 'feature_fraction': 1.0, 'bagging_freq': 3, 'bagging_fraction': 0.8})
2023-12-20 10:58:35,724:INFO:Checking exceptions
2023-12-20 10:58:35,725:INFO:Importing libraries
2023-12-20 10:58:35,725:INFO:Copying training dataset
2023-12-20 10:58:35,730:INFO:Defining folds
2023-12-20 10:58:35,730:INFO:Declaring metric variables
2023-12-20 10:58:35,730:INFO:Importing untrained model
2023-12-20 10:58:35,730:INFO:Declaring custom model
2023-12-20 10:58:35,741:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:58:35,743:INFO:Starting cross validation
2023-12-20 10:58:35,743:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:37,142:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,159:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,159:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,159:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,177:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,183:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,259:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,259:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,278:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,281:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,281:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,281:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,292:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,292:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,292:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:37,309:INFO:Calculating mean and std
2023-12-20 10:58:37,309:INFO:Creating metrics dataframe
2023-12-20 10:58:37,324:INFO:Finalizing model
2023-12-20 10:58:37,345:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 10:58:37,345:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 10:58:37,345:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2023-12-20 10:58:37,353:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 10:58:37,353:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 10:58:37,353:INFO:[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3
2023-12-20 10:58:37,357:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002058 seconds.
2023-12-20 10:58:37,357:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:58:37,358:INFO:[LightGBM] [Info] Total Bins 1783
2023-12-20 10:58:37,358:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 10:58:37,359:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:58:37,359:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:58:37,360:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:58:37,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,362:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,376:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,383:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,390:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,425:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,440:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,442:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,458:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,475:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,493:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,495:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,508:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,558:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,575:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,590:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,592:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,607:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,625:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,640:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,642:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,675:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,692:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,707:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,708:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,710:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,711:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 10:58:37,739:INFO:Uploading results into container
2023-12-20 10:58:37,740:INFO:Uploading model into container now
2023-12-20 10:58:37,741:INFO:_master_model_container: 16
2023-12-20 10:58:37,741:INFO:_display_container: 3
2023-12-20 10:58:37,742:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=91, min_child_weight=0.001, min_split_gain=0.8,
               n_estimators=90, n_jobs=-1, num_leaves=60, objective=None,
               random_state=7124, reg_alpha=0.5, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:58:37,742:INFO:create_model() successfully completed......................................
2023-12-20 10:58:37,859:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:37,859:INFO:choose_better activated
2023-12-20 10:58:37,859:INFO:SubProcess create_model() called ==================================
2023-12-20 10:58:37,859:INFO:Initializing create_model()
2023-12-20 10:58:37,859:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 10:58:37,859:INFO:Checking exceptions
2023-12-20 10:58:37,859:INFO:Importing libraries
2023-12-20 10:58:37,859:INFO:Copying training dataset
2023-12-20 10:58:37,859:INFO:Defining folds
2023-12-20 10:58:37,859:INFO:Declaring metric variables
2023-12-20 10:58:37,859:INFO:Importing untrained model
2023-12-20 10:58:37,859:INFO:Declaring custom model
2023-12-20 10:58:37,859:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 10:58:37,859:INFO:Starting cross validation
2023-12-20 10:58:37,859:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 10:58:40,742:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:40,742:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:40,758:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:40,758:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:40,775:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:40,775:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:40,775:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:40,792:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:40,792:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:41,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:41,092:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:41,109:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:41,175:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:41,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:41,192:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 10:58:41,208:INFO:Calculating mean and std
2023-12-20 10:58:41,208:INFO:Creating metrics dataframe
2023-12-20 10:58:41,208:INFO:Finalizing model
2023-12-20 10:58:41,242:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001616 seconds.
2023-12-20 10:58:41,242:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 10:58:41,242:INFO:[LightGBM] [Info] Total Bins 1783
2023-12-20 10:58:41,242:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 10:58:41,242:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 10:58:41,242:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 10:58:41,242:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 10:58:41,581:INFO:Uploading results into container
2023-12-20 10:58:41,581:INFO:Uploading model into container now
2023-12-20 10:58:41,581:INFO:_master_model_container: 17
2023-12-20 10:58:41,581:INFO:_display_container: 4
2023-12-20 10:58:41,581:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:58:41,581:INFO:create_model() successfully completed......................................
2023-12-20 10:58:41,675:INFO:SubProcess create_model() end ==================================
2023-12-20 10:58:41,675:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8238
2023-12-20 10:58:41,675:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=3, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=91, min_child_weight=0.001, min_split_gain=0.8,
               n_estimators=90, n_jobs=-1, num_leaves=60, objective=None,
               random_state=7124, reg_alpha=0.5, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.821
2023-12-20 10:58:41,675:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 10:58:41,675:INFO:choose_better completed
2023-12-20 10:58:41,690:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-12-20 10:58:41,696:INFO:_master_model_container: 17
2023-12-20 10:58:41,696:INFO:_display_container: 3
2023-12-20 10:58:41,696:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 10:58:41,696:INFO:tune_model() successfully completed......................................
2023-12-20 10:58:41,789:INFO:Initializing evaluate_model()
2023-12-20 10:58:41,789:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 10:58:41,800:INFO:Initializing plot_model()
2023-12-20 10:58:41,800:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:58:41,800:INFO:Checking exceptions
2023-12-20 10:58:41,803:INFO:Preloading libraries
2023-12-20 10:58:41,814:INFO:Copying training dataset
2023-12-20 10:58:41,814:INFO:Plot type: pipeline
2023-12-20 10:58:41,953:INFO:Visual Rendered Successfully
2023-12-20 10:58:42,032:INFO:plot_model() successfully completed......................................
2023-12-20 10:59:33,033:INFO:Initializing plot_model()
2023-12-20 10:59:33,033:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001C5FA467210>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7124, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 10:59:33,033:INFO:Checking exceptions
2023-12-20 10:59:33,036:INFO:Preloading libraries
2023-12-20 10:59:33,043:INFO:Copying training dataset
2023-12-20 10:59:33,043:INFO:Plot type: auc
2023-12-20 10:59:33,133:INFO:Fitting Model
2023-12-20 10:59:33,133:INFO:Scoring test/hold-out set
2023-12-20 10:59:33,309:INFO:Visual Rendered Successfully
2023-12-20 10:59:33,393:INFO:plot_model() successfully completed......................................
2023-12-20 15:24:39,774:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 15:24:39,975:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 15:24:39,975:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 15:24:39,975:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 15:24:42,328:INFO:PyCaret ClassificationExperiment
2023-12-20 15:24:42,328:INFO:Logging name: clf-default-name
2023-12-20 15:24:42,328:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 15:24:42,328:INFO:version 3.2.0
2023-12-20 15:24:42,329:INFO:Initializing setup()
2023-12-20 15:24:42,329:INFO:self.USI: ecbb
2023-12-20 15:24:42,329:INFO:self._variable_keys: {'data', '_ml_usecase', 'fold_shuffle_param', 'X_train', 'fix_imbalance', 'y', 'n_jobs_param', 'gpu_n_jobs_param', 'y_train', 'exp_name_log', 'seed', 'X', 'X_test', 'fold_groups_param', 'memory', 'idx', 'is_multiclass', 'exp_id', '_available_plots', 'target_param', 'y_test', 'log_plots_param', 'fold_generator', 'html_param', 'USI', 'gpu_param', 'logging_param', 'pipeline'}
2023-12-20 15:24:42,329:INFO:Checking environment
2023-12-20 15:24:42,329:INFO:python_version: 3.11.5
2023-12-20 15:24:42,329:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 15:24:42,329:INFO:machine: AMD64
2023-12-20 15:24:42,329:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 15:24:42,329:INFO:Memory: svmem(total=16718413824, available=6268948480, percent=62.5, used=10449465344, free=6268948480)
2023-12-20 15:24:42,329:INFO:Physical Core: 12
2023-12-20 15:24:42,329:INFO:Logical Core: 16
2023-12-20 15:24:42,329:INFO:Checking libraries
2023-12-20 15:24:42,329:INFO:System:
2023-12-20 15:24:42,329:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 15:24:42,329:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 15:24:42,329:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 15:24:42,329:INFO:PyCaret required dependencies:
2023-12-20 15:24:42,866:INFO:                 pip: 23.2.1
2023-12-20 15:24:42,866:INFO:          setuptools: 68.0.0
2023-12-20 15:24:42,866:INFO:             pycaret: 3.2.0
2023-12-20 15:24:42,866:INFO:             IPython: 8.15.0
2023-12-20 15:24:42,866:INFO:          ipywidgets: 8.0.4
2023-12-20 15:24:42,866:INFO:                tqdm: 4.65.0
2023-12-20 15:24:42,866:INFO:               numpy: 1.24.3
2023-12-20 15:24:42,866:INFO:              pandas: 1.5.3
2023-12-20 15:24:42,867:INFO:              jinja2: 3.1.2
2023-12-20 15:24:42,867:INFO:               scipy: 1.10.1
2023-12-20 15:24:42,867:INFO:              joblib: 1.2.0
2023-12-20 15:24:42,867:INFO:             sklearn: 1.2.1
2023-12-20 15:24:42,867:INFO:                pyod: 1.1.2
2023-12-20 15:24:42,867:INFO:            imblearn: 0.11.0
2023-12-20 15:24:42,867:INFO:   category_encoders: 2.6.3
2023-12-20 15:24:42,867:INFO:            lightgbm: 4.1.0
2023-12-20 15:24:42,867:INFO:               numba: 0.57.1
2023-12-20 15:24:42,867:INFO:            requests: 2.31.0
2023-12-20 15:24:42,867:INFO:          matplotlib: 3.6.0
2023-12-20 15:24:42,867:INFO:          scikitplot: 0.3.7
2023-12-20 15:24:42,867:INFO:         yellowbrick: 1.5
2023-12-20 15:24:42,867:INFO:              plotly: 5.9.0
2023-12-20 15:24:42,867:INFO:    plotly-resampler: Not installed
2023-12-20 15:24:42,867:INFO:             kaleido: 0.2.1
2023-12-20 15:24:42,867:INFO:           schemdraw: 0.15
2023-12-20 15:24:42,867:INFO:         statsmodels: 0.14.0
2023-12-20 15:24:42,867:INFO:              sktime: 0.21.1
2023-12-20 15:24:42,867:INFO:               tbats: 1.1.3
2023-12-20 15:24:42,867:INFO:            pmdarima: 2.0.4
2023-12-20 15:24:42,867:INFO:              psutil: 5.9.0
2023-12-20 15:24:42,867:INFO:          markupsafe: 2.1.1
2023-12-20 15:24:42,867:INFO:             pickle5: Not installed
2023-12-20 15:24:42,867:INFO:         cloudpickle: 2.2.1
2023-12-20 15:24:42,867:INFO:         deprecation: 2.1.0
2023-12-20 15:24:42,867:INFO:              xxhash: 2.0.2
2023-12-20 15:24:42,867:INFO:           wurlitzer: Not installed
2023-12-20 15:24:42,867:INFO:PyCaret optional dependencies:
2023-12-20 15:24:42,978:INFO:                shap: Not installed
2023-12-20 15:24:42,978:INFO:           interpret: Not installed
2023-12-20 15:24:42,978:INFO:                umap: Not installed
2023-12-20 15:24:42,978:INFO:     ydata_profiling: Not installed
2023-12-20 15:24:42,978:INFO:  explainerdashboard: Not installed
2023-12-20 15:24:42,978:INFO:             autoviz: Not installed
2023-12-20 15:24:42,978:INFO:           fairlearn: Not installed
2023-12-20 15:24:42,978:INFO:          deepchecks: Not installed
2023-12-20 15:24:42,978:INFO:             xgboost: 2.0.2
2023-12-20 15:24:42,978:INFO:            catboost: Not installed
2023-12-20 15:24:42,978:INFO:              kmodes: Not installed
2023-12-20 15:24:42,978:INFO:             mlxtend: Not installed
2023-12-20 15:24:42,979:INFO:       statsforecast: Not installed
2023-12-20 15:24:42,979:INFO:        tune_sklearn: Not installed
2023-12-20 15:24:42,979:INFO:                 ray: Not installed
2023-12-20 15:24:42,979:INFO:            hyperopt: Not installed
2023-12-20 15:24:42,979:INFO:              optuna: Not installed
2023-12-20 15:24:42,979:INFO:               skopt: Not installed
2023-12-20 15:24:42,979:INFO:              mlflow: Not installed
2023-12-20 15:24:42,979:INFO:              gradio: Not installed
2023-12-20 15:24:42,979:INFO:             fastapi: Not installed
2023-12-20 15:24:42,979:INFO:             uvicorn: Not installed
2023-12-20 15:24:42,979:INFO:              m2cgen: Not installed
2023-12-20 15:24:42,979:INFO:           evidently: Not installed
2023-12-20 15:24:42,979:INFO:               fugue: Not installed
2023-12-20 15:24:42,979:INFO:           streamlit: Not installed
2023-12-20 15:24:42,979:INFO:             prophet: Not installed
2023-12-20 15:24:42,979:INFO:None
2023-12-20 15:24:42,979:INFO:Set up data.
2023-12-20 15:24:42,990:INFO:Set up folding strategy.
2023-12-20 15:24:42,991:INFO:Set up train/test split.
2023-12-20 15:24:42,997:INFO:Set up index.
2023-12-20 15:24:42,998:INFO:Assigning column types.
2023-12-20 15:24:43,000:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 15:24:43,025:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 15:24:43,028:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:24:43,048:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:24:43,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:24:43,074:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 15:24:43,075:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:24:43,089:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:24:43,089:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:24:43,089:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 15:24:43,106:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:24:43,132:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:24:43,134:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:24:43,160:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:24:43,175:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:24:43,177:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:24:43,177:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 15:24:43,207:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:24:43,207:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:24:43,261:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:24:43,263:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:24:43,264:INFO:Preparing preprocessing pipeline...
2023-12-20 15:24:43,265:INFO:Set up label encoding.
2023-12-20 15:24:43,265:INFO:Set up simple imputation.
2023-12-20 15:24:43,266:INFO:Set up column name cleaning.
2023-12-20 15:24:43,299:INFO:Finished creating preprocessing pipeline.
2023-12-20 15:24:43,304:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(exclude=None, include=None,
                                               transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Plate...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 15:24:43,304:INFO:Creating final display dataframe.
2023-12-20 15:24:43,385:INFO:Setup _display_container:                     Description              Value
0                    Session id               7139
1                        Target             Status
2                   Target type         Multiclass
3                Target mapping  C: 0, CL: 1, D: 2
4           Original data shape         (7905, 35)
5        Transformed data shape         (7905, 35)
6   Transformed train set shape         (6324, 35)
7    Transformed test set shape         (1581, 35)
8              Numeric features                 34
9                    Preprocess               True
10              Imputation type             simple
11           Numeric imputation               mean
12       Categorical imputation               mode
13               Fold Generator    StratifiedKFold
14                  Fold Number                  5
15                     CPU Jobs                 -1
16                      Use GPU              False
17               Log Experiment              False
18              Experiment Name   clf-default-name
19                          USI               ecbb
2023-12-20 15:24:43,443:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:24:43,445:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:24:43,486:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:24:43,487:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:24:43,489:INFO:setup() successfully completed in 1.41s...............
2023-12-20 15:24:43,495:INFO:Initializing compare_models()
2023-12-20 15:24:43,495:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 15:24:43,496:INFO:Checking exceptions
2023-12-20 15:24:43,499:INFO:Preparing display monitor
2023-12-20 15:24:43,519:INFO:Initializing Logistic Regression
2023-12-20 15:24:43,520:INFO:Total runtime is 1.662572224934896e-05 minutes
2023-12-20 15:24:43,522:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:43,523:INFO:Initializing create_model()
2023-12-20 15:24:43,523:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:43,523:INFO:Checking exceptions
2023-12-20 15:24:43,523:INFO:Importing libraries
2023-12-20 15:24:43,524:INFO:Copying training dataset
2023-12-20 15:24:43,528:INFO:Defining folds
2023-12-20 15:24:43,528:INFO:Declaring metric variables
2023-12-20 15:24:43,530:INFO:Importing untrained model
2023-12-20 15:24:43,532:INFO:Logistic Regression Imported successfully
2023-12-20 15:24:43,538:INFO:Starting cross validation
2023-12-20 15:24:43,540:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:47,266:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,271:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,272:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,275:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:47,277:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,280:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,287:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,288:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,292:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,293:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,298:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,300:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,304:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:47,308:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,318:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,326:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,331:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:47,339:INFO:Calculating mean and std
2023-12-20 15:24:47,341:INFO:Creating metrics dataframe
2023-12-20 15:24:47,344:INFO:Uploading results into container
2023-12-20 15:24:47,344:INFO:Uploading model into container now
2023-12-20 15:24:47,345:INFO:_master_model_container: 1
2023-12-20 15:24:47,345:INFO:_display_container: 2
2023-12-20 15:24:47,345:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=7139, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 15:24:47,345:INFO:create_model() successfully completed......................................
2023-12-20 15:24:47,421:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:47,422:INFO:Creating metrics dataframe
2023-12-20 15:24:47,432:INFO:Initializing K Neighbors Classifier
2023-12-20 15:24:47,433:INFO:Total runtime is 0.0652340571085612 minutes
2023-12-20 15:24:47,434:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:47,435:INFO:Initializing create_model()
2023-12-20 15:24:47,435:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:47,435:INFO:Checking exceptions
2023-12-20 15:24:47,435:INFO:Importing libraries
2023-12-20 15:24:47,435:INFO:Copying training dataset
2023-12-20 15:24:47,439:INFO:Defining folds
2023-12-20 15:24:47,439:INFO:Declaring metric variables
2023-12-20 15:24:47,442:INFO:Importing untrained model
2023-12-20 15:24:47,444:INFO:K Neighbors Classifier Imported successfully
2023-12-20 15:24:47,449:INFO:Starting cross validation
2023-12-20 15:24:47,450:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:50,758:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,767:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,769:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,772:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,776:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,778:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,782:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,783:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,785:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,788:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,791:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,797:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,832:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,837:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,842:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:50,849:INFO:Calculating mean and std
2023-12-20 15:24:50,850:INFO:Creating metrics dataframe
2023-12-20 15:24:50,853:INFO:Uploading results into container
2023-12-20 15:24:50,853:INFO:Uploading model into container now
2023-12-20 15:24:50,854:INFO:_master_model_container: 2
2023-12-20 15:24:50,854:INFO:_display_container: 2
2023-12-20 15:24:50,854:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 15:24:50,854:INFO:create_model() successfully completed......................................
2023-12-20 15:24:50,926:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:50,926:INFO:Creating metrics dataframe
2023-12-20 15:24:50,936:INFO:Initializing Naive Bayes
2023-12-20 15:24:50,936:INFO:Total runtime is 0.12362227042516072 minutes
2023-12-20 15:24:50,939:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:50,939:INFO:Initializing create_model()
2023-12-20 15:24:50,939:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:50,939:INFO:Checking exceptions
2023-12-20 15:24:50,939:INFO:Importing libraries
2023-12-20 15:24:50,940:INFO:Copying training dataset
2023-12-20 15:24:50,943:INFO:Defining folds
2023-12-20 15:24:50,943:INFO:Declaring metric variables
2023-12-20 15:24:50,946:INFO:Importing untrained model
2023-12-20 15:24:50,949:INFO:Naive Bayes Imported successfully
2023-12-20 15:24:50,954:INFO:Starting cross validation
2023-12-20 15:24:50,954:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:53,806:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,811:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,816:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,822:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,825:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,826:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,827:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,830:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,830:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,832:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,835:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,837:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,839:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,847:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:53,860:INFO:Calculating mean and std
2023-12-20 15:24:53,862:INFO:Creating metrics dataframe
2023-12-20 15:24:53,866:INFO:Uploading results into container
2023-12-20 15:24:53,866:INFO:Uploading model into container now
2023-12-20 15:24:53,867:INFO:_master_model_container: 3
2023-12-20 15:24:53,867:INFO:_display_container: 2
2023-12-20 15:24:53,867:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 15:24:53,867:INFO:create_model() successfully completed......................................
2023-12-20 15:24:53,944:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:53,944:INFO:Creating metrics dataframe
2023-12-20 15:24:53,953:INFO:Initializing Decision Tree Classifier
2023-12-20 15:24:53,953:INFO:Total runtime is 0.17390390237172443 minutes
2023-12-20 15:24:53,955:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:53,956:INFO:Initializing create_model()
2023-12-20 15:24:53,956:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:53,956:INFO:Checking exceptions
2023-12-20 15:24:53,956:INFO:Importing libraries
2023-12-20 15:24:53,956:INFO:Copying training dataset
2023-12-20 15:24:53,960:INFO:Defining folds
2023-12-20 15:24:53,960:INFO:Declaring metric variables
2023-12-20 15:24:53,963:INFO:Importing untrained model
2023-12-20 15:24:53,965:INFO:Decision Tree Classifier Imported successfully
2023-12-20 15:24:53,971:INFO:Starting cross validation
2023-12-20 15:24:53,972:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:54,040:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,040:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,040:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,056:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,057:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,058:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,059:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,060:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,063:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,063:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:54,067:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,207:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,208:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,208:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,222:INFO:Calculating mean and std
2023-12-20 15:24:56,222:INFO:Creating metrics dataframe
2023-12-20 15:24:56,222:INFO:Uploading results into container
2023-12-20 15:24:56,222:INFO:Uploading model into container now
2023-12-20 15:24:56,222:INFO:_master_model_container: 4
2023-12-20 15:24:56,222:INFO:_display_container: 2
2023-12-20 15:24:56,222:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=7139, splitter='best')
2023-12-20 15:24:56,222:INFO:create_model() successfully completed......................................
2023-12-20 15:24:56,302:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:56,302:INFO:Creating metrics dataframe
2023-12-20 15:24:56,310:INFO:Initializing SVM - Linear Kernel
2023-12-20 15:24:56,310:INFO:Total runtime is 0.2131864309310913 minutes
2023-12-20 15:24:56,312:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:56,312:INFO:Initializing create_model()
2023-12-20 15:24:56,312:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:56,312:INFO:Checking exceptions
2023-12-20 15:24:56,312:INFO:Importing libraries
2023-12-20 15:24:56,313:INFO:Copying training dataset
2023-12-20 15:24:56,316:INFO:Defining folds
2023-12-20 15:24:56,316:INFO:Declaring metric variables
2023-12-20 15:24:56,318:INFO:Importing untrained model
2023-12-20 15:24:56,321:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 15:24:56,325:INFO:Starting cross validation
2023-12-20 15:24:56,326:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:56,435:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:24:56,436:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:24:56,438:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,439:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,439:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:24:56,440:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:24:56,441:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,442:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,443:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,443:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,445:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:56,446:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,447:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,448:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,448:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,450:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,452:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:24:56,452:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,455:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,459:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,463:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,470:INFO:Calculating mean and std
2023-12-20 15:24:56,471:INFO:Creating metrics dataframe
2023-12-20 15:24:56,472:INFO:Uploading results into container
2023-12-20 15:24:56,472:INFO:Uploading model into container now
2023-12-20 15:24:56,472:INFO:_master_model_container: 5
2023-12-20 15:24:56,472:INFO:_display_container: 2
2023-12-20 15:24:56,472:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=7139, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 15:24:56,472:INFO:create_model() successfully completed......................................
2023-12-20 15:24:56,542:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:56,542:INFO:Creating metrics dataframe
2023-12-20 15:24:56,550:INFO:Initializing Ridge Classifier
2023-12-20 15:24:56,550:INFO:Total runtime is 0.21718393564224242 minutes
2023-12-20 15:24:56,552:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:56,552:INFO:Initializing create_model()
2023-12-20 15:24:56,552:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:56,552:INFO:Checking exceptions
2023-12-20 15:24:56,552:INFO:Importing libraries
2023-12-20 15:24:56,552:INFO:Copying training dataset
2023-12-20 15:24:56,556:INFO:Defining folds
2023-12-20 15:24:56,556:INFO:Declaring metric variables
2023-12-20 15:24:56,558:INFO:Importing untrained model
2023-12-20 15:24:56,560:INFO:Ridge Classifier Imported successfully
2023-12-20 15:24:56,563:INFO:Starting cross validation
2023-12-20 15:24:56,564:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:56,596:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:24:56,598:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,603:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,604:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:24:56,607:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:24:56,607:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,608:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:56,609:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,611:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,612:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:24:56,612:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,613:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,614:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,614:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:56,615:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:24:56,616:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:56,616:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,617:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,618:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,618:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,623:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:56,626:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,626:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,629:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:56,633:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:56,641:INFO:Calculating mean and std
2023-12-20 15:24:56,642:INFO:Creating metrics dataframe
2023-12-20 15:24:56,644:INFO:Uploading results into container
2023-12-20 15:24:56,644:INFO:Uploading model into container now
2023-12-20 15:24:56,645:INFO:_master_model_container: 6
2023-12-20 15:24:56,645:INFO:_display_container: 2
2023-12-20 15:24:56,645:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=7139, solver='auto',
                tol=0.0001)
2023-12-20 15:24:56,645:INFO:create_model() successfully completed......................................
2023-12-20 15:24:56,714:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:56,714:INFO:Creating metrics dataframe
2023-12-20 15:24:56,722:INFO:Initializing Random Forest Classifier
2023-12-20 15:24:56,722:INFO:Total runtime is 0.22005776961644488 minutes
2023-12-20 15:24:56,725:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:56,725:INFO:Initializing create_model()
2023-12-20 15:24:56,725:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:56,725:INFO:Checking exceptions
2023-12-20 15:24:56,725:INFO:Importing libraries
2023-12-20 15:24:56,725:INFO:Copying training dataset
2023-12-20 15:24:56,731:INFO:Defining folds
2023-12-20 15:24:56,731:INFO:Declaring metric variables
2023-12-20 15:24:56,733:INFO:Importing untrained model
2023-12-20 15:24:56,736:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:24:56,740:INFO:Starting cross validation
2023-12-20 15:24:56,741:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:57,215:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,216:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,217:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,218:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,220:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,221:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,222:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,223:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,223:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,225:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,226:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,227:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,228:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,232:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,237:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,245:INFO:Calculating mean and std
2023-12-20 15:24:57,246:INFO:Creating metrics dataframe
2023-12-20 15:24:57,248:INFO:Uploading results into container
2023-12-20 15:24:57,249:INFO:Uploading model into container now
2023-12-20 15:24:57,249:INFO:_master_model_container: 7
2023-12-20 15:24:57,249:INFO:_display_container: 2
2023-12-20 15:24:57,249:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False)
2023-12-20 15:24:57,250:INFO:create_model() successfully completed......................................
2023-12-20 15:24:57,320:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:57,320:INFO:Creating metrics dataframe
2023-12-20 15:24:57,329:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 15:24:57,329:INFO:Total runtime is 0.2301782965660095 minutes
2023-12-20 15:24:57,332:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:57,332:INFO:Initializing create_model()
2023-12-20 15:24:57,333:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:57,333:INFO:Checking exceptions
2023-12-20 15:24:57,333:INFO:Importing libraries
2023-12-20 15:24:57,333:INFO:Copying training dataset
2023-12-20 15:24:57,337:INFO:Defining folds
2023-12-20 15:24:57,337:INFO:Declaring metric variables
2023-12-20 15:24:57,339:INFO:Importing untrained model
2023-12-20 15:24:57,341:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 15:24:57,345:INFO:Starting cross validation
2023-12-20 15:24:57,346:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:57,378:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:24:57,381:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:24:57,381:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:24:57,382:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:24:57,385:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:24:57,393:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,396:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,397:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,397:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,398:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,401:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,401:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:24:57,402:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,402:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,403:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,405:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,406:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,406:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,406:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,406:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,406:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,423:INFO:Calculating mean and std
2023-12-20 15:24:57,423:INFO:Creating metrics dataframe
2023-12-20 15:24:57,423:INFO:Uploading results into container
2023-12-20 15:24:57,423:INFO:Uploading model into container now
2023-12-20 15:24:57,423:INFO:_master_model_container: 8
2023-12-20 15:24:57,423:INFO:_display_container: 2
2023-12-20 15:24:57,423:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 15:24:57,423:INFO:create_model() successfully completed......................................
2023-12-20 15:24:57,505:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:57,505:INFO:Creating metrics dataframe
2023-12-20 15:24:57,513:INFO:Initializing Ada Boost Classifier
2023-12-20 15:24:57,514:INFO:Total runtime is 0.233250351746877 minutes
2023-12-20 15:24:57,516:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:57,516:INFO:Initializing create_model()
2023-12-20 15:24:57,516:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:57,516:INFO:Checking exceptions
2023-12-20 15:24:57,516:INFO:Importing libraries
2023-12-20 15:24:57,516:INFO:Copying training dataset
2023-12-20 15:24:57,520:INFO:Defining folds
2023-12-20 15:24:57,520:INFO:Declaring metric variables
2023-12-20 15:24:57,522:INFO:Importing untrained model
2023-12-20 15:24:57,524:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:24:57,528:INFO:Starting cross validation
2023-12-20 15:24:57,529:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:24:57,823:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,826:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,828:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,831:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,833:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,836:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,844:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,845:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,849:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,849:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,853:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,854:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,855:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,860:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,864:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:24:57,871:INFO:Calculating mean and std
2023-12-20 15:24:57,872:INFO:Creating metrics dataframe
2023-12-20 15:24:57,874:INFO:Uploading results into container
2023-12-20 15:24:57,875:INFO:Uploading model into container now
2023-12-20 15:24:57,875:INFO:_master_model_container: 9
2023-12-20 15:24:57,875:INFO:_display_container: 2
2023-12-20 15:24:57,876:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139)
2023-12-20 15:24:57,876:INFO:create_model() successfully completed......................................
2023-12-20 15:24:57,948:INFO:SubProcess create_model() end ==================================
2023-12-20 15:24:57,948:INFO:Creating metrics dataframe
2023-12-20 15:24:57,958:INFO:Initializing Gradient Boosting Classifier
2023-12-20 15:24:57,958:INFO:Total runtime is 0.2406579772631327 minutes
2023-12-20 15:24:57,960:INFO:SubProcess create_model() called ==================================
2023-12-20 15:24:57,961:INFO:Initializing create_model()
2023-12-20 15:24:57,961:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:24:57,961:INFO:Checking exceptions
2023-12-20 15:24:57,961:INFO:Importing libraries
2023-12-20 15:24:57,961:INFO:Copying training dataset
2023-12-20 15:24:57,965:INFO:Defining folds
2023-12-20 15:24:57,965:INFO:Declaring metric variables
2023-12-20 15:24:57,968:INFO:Importing untrained model
2023-12-20 15:24:57,970:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:24:57,974:INFO:Starting cross validation
2023-12-20 15:24:57,975:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:25:01,131:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,135:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,139:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,140:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,140:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,145:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,146:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,149:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,153:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,178:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,182:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,186:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,210:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,210:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,210:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,222:INFO:Calculating mean and std
2023-12-20 15:25:01,222:INFO:Creating metrics dataframe
2023-12-20 15:25:01,222:INFO:Uploading results into container
2023-12-20 15:25:01,222:INFO:Uploading model into container now
2023-12-20 15:25:01,222:INFO:_master_model_container: 10
2023-12-20 15:25:01,222:INFO:_display_container: 2
2023-12-20 15:25:01,222:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:25:01,222:INFO:create_model() successfully completed......................................
2023-12-20 15:25:01,313:INFO:SubProcess create_model() end ==================================
2023-12-20 15:25:01,314:INFO:Creating metrics dataframe
2023-12-20 15:25:01,323:INFO:Initializing Linear Discriminant Analysis
2023-12-20 15:25:01,323:INFO:Total runtime is 0.296738056341807 minutes
2023-12-20 15:25:01,326:INFO:SubProcess create_model() called ==================================
2023-12-20 15:25:01,326:INFO:Initializing create_model()
2023-12-20 15:25:01,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:01,326:INFO:Checking exceptions
2023-12-20 15:25:01,326:INFO:Importing libraries
2023-12-20 15:25:01,326:INFO:Copying training dataset
2023-12-20 15:25:01,330:INFO:Defining folds
2023-12-20 15:25:01,331:INFO:Declaring metric variables
2023-12-20 15:25:01,333:INFO:Importing untrained model
2023-12-20 15:25:01,335:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 15:25:01,338:INFO:Starting cross validation
2023-12-20 15:25:01,339:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:25:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,401:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,402:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,403:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,406:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,407:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,407:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,410:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,410:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,415:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,415:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,415:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,419:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,420:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,425:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,433:INFO:Calculating mean and std
2023-12-20 15:25:01,433:INFO:Creating metrics dataframe
2023-12-20 15:25:01,436:INFO:Uploading results into container
2023-12-20 15:25:01,436:INFO:Uploading model into container now
2023-12-20 15:25:01,436:INFO:_master_model_container: 11
2023-12-20 15:25:01,436:INFO:_display_container: 2
2023-12-20 15:25:01,436:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 15:25:01,436:INFO:create_model() successfully completed......................................
2023-12-20 15:25:01,504:INFO:SubProcess create_model() end ==================================
2023-12-20 15:25:01,504:INFO:Creating metrics dataframe
2023-12-20 15:25:01,514:INFO:Initializing Extra Trees Classifier
2023-12-20 15:25:01,514:INFO:Total runtime is 0.29991927544275915 minutes
2023-12-20 15:25:01,516:INFO:SubProcess create_model() called ==================================
2023-12-20 15:25:01,516:INFO:Initializing create_model()
2023-12-20 15:25:01,516:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:01,516:INFO:Checking exceptions
2023-12-20 15:25:01,516:INFO:Importing libraries
2023-12-20 15:25:01,517:INFO:Copying training dataset
2023-12-20 15:25:01,520:INFO:Defining folds
2023-12-20 15:25:01,520:INFO:Declaring metric variables
2023-12-20 15:25:01,522:INFO:Importing untrained model
2023-12-20 15:25:01,524:INFO:Extra Trees Classifier Imported successfully
2023-12-20 15:25:01,528:INFO:Starting cross validation
2023-12-20 15:25:01,528:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:25:01,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,906:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,922:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,922:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,922:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,927:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,927:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,928:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,928:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,932:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,934:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,940:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,945:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:01,953:INFO:Calculating mean and std
2023-12-20 15:25:01,953:INFO:Creating metrics dataframe
2023-12-20 15:25:01,956:INFO:Uploading results into container
2023-12-20 15:25:01,956:INFO:Uploading model into container now
2023-12-20 15:25:01,956:INFO:_master_model_container: 12
2023-12-20 15:25:01,956:INFO:_display_container: 2
2023-12-20 15:25:01,957:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=7139, verbose=0, warm_start=False)
2023-12-20 15:25:01,957:INFO:create_model() successfully completed......................................
2023-12-20 15:25:02,035:INFO:SubProcess create_model() end ==================================
2023-12-20 15:25:02,035:INFO:Creating metrics dataframe
2023-12-20 15:25:02,045:INFO:Initializing Extreme Gradient Boosting
2023-12-20 15:25:02,045:INFO:Total runtime is 0.3087786515553792 minutes
2023-12-20 15:25:02,047:INFO:SubProcess create_model() called ==================================
2023-12-20 15:25:02,047:INFO:Initializing create_model()
2023-12-20 15:25:02,047:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:02,048:INFO:Checking exceptions
2023-12-20 15:25:02,048:INFO:Importing libraries
2023-12-20 15:25:02,048:INFO:Copying training dataset
2023-12-20 15:25:02,051:INFO:Defining folds
2023-12-20 15:25:02,051:INFO:Declaring metric variables
2023-12-20 15:25:02,053:INFO:Importing untrained model
2023-12-20 15:25:02,056:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:25:02,060:INFO:Starting cross validation
2023-12-20 15:25:02,060:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:25:02,631:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,636:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,637:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,641:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,642:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,647:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,647:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,649:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,652:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,654:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,656:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,659:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,692:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,700:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:02,706:INFO:Calculating mean and std
2023-12-20 15:25:02,707:INFO:Creating metrics dataframe
2023-12-20 15:25:02,710:INFO:Uploading results into container
2023-12-20 15:25:02,710:INFO:Uploading model into container now
2023-12-20 15:25:02,711:INFO:_master_model_container: 13
2023-12-20 15:25:02,711:INFO:_display_container: 2
2023-12-20 15:25:02,711:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 15:25:02,711:INFO:create_model() successfully completed......................................
2023-12-20 15:25:02,783:INFO:SubProcess create_model() end ==================================
2023-12-20 15:25:02,783:INFO:Creating metrics dataframe
2023-12-20 15:25:02,793:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 15:25:02,794:INFO:Total runtime is 0.32123823563257853 minutes
2023-12-20 15:25:02,796:INFO:SubProcess create_model() called ==================================
2023-12-20 15:25:02,796:INFO:Initializing create_model()
2023-12-20 15:25:02,797:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:02,797:INFO:Checking exceptions
2023-12-20 15:25:02,797:INFO:Importing libraries
2023-12-20 15:25:02,797:INFO:Copying training dataset
2023-12-20 15:25:02,800:INFO:Defining folds
2023-12-20 15:25:02,800:INFO:Declaring metric variables
2023-12-20 15:25:02,803:INFO:Importing untrained model
2023-12-20 15:25:02,805:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:25:02,810:INFO:Starting cross validation
2023-12-20 15:25:02,811:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:25:05,630:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,634:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,639:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,641:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,641:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,641:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,641:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,641:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,655:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,656:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,656:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,656:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,681:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,689:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,689:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,707:INFO:Calculating mean and std
2023-12-20 15:25:05,709:INFO:Creating metrics dataframe
2023-12-20 15:25:05,713:INFO:Uploading results into container
2023-12-20 15:25:05,713:INFO:Uploading model into container now
2023-12-20 15:25:05,714:INFO:_master_model_container: 14
2023-12-20 15:25:05,714:INFO:_display_container: 2
2023-12-20 15:25:05,715:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:25:05,715:INFO:create_model() successfully completed......................................
2023-12-20 15:25:05,806:INFO:SubProcess create_model() end ==================================
2023-12-20 15:25:05,806:INFO:Creating metrics dataframe
2023-12-20 15:25:05,806:INFO:Initializing Dummy Classifier
2023-12-20 15:25:05,806:INFO:Total runtime is 0.3714571237564087 minutes
2023-12-20 15:25:05,822:INFO:SubProcess create_model() called ==================================
2023-12-20 15:25:05,822:INFO:Initializing create_model()
2023-12-20 15:25:05,822:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B5A350>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:05,822:INFO:Checking exceptions
2023-12-20 15:25:05,822:INFO:Importing libraries
2023-12-20 15:25:05,822:INFO:Copying training dataset
2023-12-20 15:25:05,822:INFO:Defining folds
2023-12-20 15:25:05,822:INFO:Declaring metric variables
2023-12-20 15:25:05,828:INFO:Importing untrained model
2023-12-20 15:25:05,830:INFO:Dummy Classifier Imported successfully
2023-12-20 15:25:05,830:INFO:Starting cross validation
2023-12-20 15:25:05,830:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:25:05,859:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,859:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,859:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,859:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,874:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,874:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:25:05,876:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,876:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,876:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,877:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:25:05,879:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:25:05,879:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,881:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,881:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,884:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:25:05,886:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,886:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,891:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,892:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:25:05,892:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:05,902:INFO:Calculating mean and std
2023-12-20 15:25:05,902:INFO:Creating metrics dataframe
2023-12-20 15:25:05,905:INFO:Uploading results into container
2023-12-20 15:25:05,905:INFO:Uploading model into container now
2023-12-20 15:25:05,905:INFO:_master_model_container: 15
2023-12-20 15:25:05,905:INFO:_display_container: 2
2023-12-20 15:25:05,906:INFO:DummyClassifier(constant=None, random_state=7139, strategy='prior')
2023-12-20 15:25:05,906:INFO:create_model() successfully completed......................................
2023-12-20 15:25:05,972:INFO:SubProcess create_model() end ==================================
2023-12-20 15:25:05,972:INFO:Creating metrics dataframe
2023-12-20 15:25:05,987:INFO:Initializing create_model()
2023-12-20 15:25:05,987:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:05,988:INFO:Checking exceptions
2023-12-20 15:25:05,990:INFO:Importing libraries
2023-12-20 15:25:05,990:INFO:Copying training dataset
2023-12-20 15:25:05,993:INFO:Defining folds
2023-12-20 15:25:05,993:INFO:Declaring metric variables
2023-12-20 15:25:05,993:INFO:Importing untrained model
2023-12-20 15:25:05,993:INFO:Declaring custom model
2023-12-20 15:25:05,993:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:25:05,993:INFO:Cross validation set to False
2023-12-20 15:25:05,993:INFO:Fitting Model
2023-12-20 15:25:06,020:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001366 seconds.
2023-12-20 15:25:06,020:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 15:25:06,020:INFO:[LightGBM] [Info] Total Bins 1785
2023-12-20 15:25:06,021:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 15:25:06,021:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 15:25:06,021:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 15:25:06,022:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 15:25:06,568:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:25:06,568:INFO:create_model() successfully completed......................................
2023-12-20 15:25:06,684:INFO:Initializing create_model()
2023-12-20 15:25:06,684:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:06,684:INFO:Checking exceptions
2023-12-20 15:25:06,686:INFO:Importing libraries
2023-12-20 15:25:06,686:INFO:Copying training dataset
2023-12-20 15:25:06,690:INFO:Defining folds
2023-12-20 15:25:06,690:INFO:Declaring metric variables
2023-12-20 15:25:06,690:INFO:Importing untrained model
2023-12-20 15:25:06,690:INFO:Declaring custom model
2023-12-20 15:25:06,690:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:25:06,691:INFO:Cross validation set to False
2023-12-20 15:25:06,691:INFO:Fitting Model
2023-12-20 15:25:09,780:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:25:09,780:INFO:create_model() successfully completed......................................
2023-12-20 15:25:09,854:INFO:Initializing create_model()
2023-12-20 15:25:09,855:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:09,855:INFO:Checking exceptions
2023-12-20 15:25:09,856:INFO:Importing libraries
2023-12-20 15:25:09,856:INFO:Copying training dataset
2023-12-20 15:25:09,860:INFO:Defining folds
2023-12-20 15:25:09,860:INFO:Declaring metric variables
2023-12-20 15:25:09,860:INFO:Importing untrained model
2023-12-20 15:25:09,860:INFO:Declaring custom model
2023-12-20 15:25:09,861:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:25:09,861:INFO:Cross validation set to False
2023-12-20 15:25:09,861:INFO:Fitting Model
2023-12-20 15:25:10,176:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 15:25:10,176:INFO:create_model() successfully completed......................................
2023-12-20 15:25:10,293:INFO:Initializing create_model()
2023-12-20 15:25:10,293:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:10,294:INFO:Checking exceptions
2023-12-20 15:25:10,295:INFO:Importing libraries
2023-12-20 15:25:10,295:INFO:Copying training dataset
2023-12-20 15:25:10,299:INFO:Defining folds
2023-12-20 15:25:10,299:INFO:Declaring metric variables
2023-12-20 15:25:10,299:INFO:Importing untrained model
2023-12-20 15:25:10,299:INFO:Declaring custom model
2023-12-20 15:25:10,299:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:25:10,300:INFO:Cross validation set to False
2023-12-20 15:25:10,300:INFO:Fitting Model
2023-12-20 15:25:10,514:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False)
2023-12-20 15:25:10,514:INFO:create_model() successfully completed......................................
2023-12-20 15:25:10,589:INFO:Initializing create_model()
2023-12-20 15:25:10,589:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:10,589:INFO:Checking exceptions
2023-12-20 15:25:10,593:INFO:Importing libraries
2023-12-20 15:25:10,593:INFO:Copying training dataset
2023-12-20 15:25:10,598:INFO:Defining folds
2023-12-20 15:25:10,598:INFO:Declaring metric variables
2023-12-20 15:25:10,599:INFO:Importing untrained model
2023-12-20 15:25:10,599:INFO:Declaring custom model
2023-12-20 15:25:10,599:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:25:10,600:INFO:Cross validation set to False
2023-12-20 15:25:10,600:INFO:Fitting Model
2023-12-20 15:25:10,865:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139)
2023-12-20 15:25:10,865:INFO:create_model() successfully completed......................................
2023-12-20 15:25:10,969:INFO:_master_model_container: 15
2023-12-20 15:25:10,969:INFO:_display_container: 2
2023-12-20 15:25:10,970:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139)]
2023-12-20 15:25:10,971:INFO:compare_models() successfully completed......................................
2023-12-20 15:25:11,001:INFO:Initializing tune_model()
2023-12-20 15:25:11,001:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:25:11,001:INFO:Checking exceptions
2023-12-20 15:25:11,023:INFO:Copying training dataset
2023-12-20 15:25:11,029:INFO:Checking base model
2023-12-20 15:25:11,029:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 15:25:11,032:INFO:Declaring metric variables
2023-12-20 15:25:11,035:INFO:Defining Hyperparameters
2023-12-20 15:25:11,128:INFO:Tuning with n_jobs=-1
2023-12-20 15:25:11,129:INFO:Initializing RandomizedSearchCV
2023-12-20 15:25:29,042:INFO:best_params: {'actual_estimator__reg_lambda': 0.5, 'actual_estimator__reg_alpha': 0.005, 'actual_estimator__num_leaves': 50, 'actual_estimator__n_estimators': 210, 'actual_estimator__min_split_gain': 0.2, 'actual_estimator__min_child_samples': 76, 'actual_estimator__learning_rate': 0.05, 'actual_estimator__feature_fraction': 1.0, 'actual_estimator__bagging_freq': 5, 'actual_estimator__bagging_fraction': 0.8}
2023-12-20 15:25:29,043:INFO:Hyperparameter search completed
2023-12-20 15:25:29,043:INFO:SubProcess create_model() called ==================================
2023-12-20 15:25:29,044:INFO:Initializing create_model()
2023-12-20 15:25:29,044:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B380D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.5, 'reg_alpha': 0.005, 'num_leaves': 50, 'n_estimators': 210, 'min_split_gain': 0.2, 'min_child_samples': 76, 'learning_rate': 0.05, 'feature_fraction': 1.0, 'bagging_freq': 5, 'bagging_fraction': 0.8})
2023-12-20 15:25:29,044:INFO:Checking exceptions
2023-12-20 15:25:29,044:INFO:Importing libraries
2023-12-20 15:25:29,044:INFO:Copying training dataset
2023-12-20 15:25:29,052:INFO:Defining folds
2023-12-20 15:25:29,052:INFO:Declaring metric variables
2023-12-20 15:25:29,056:INFO:Importing untrained model
2023-12-20 15:25:29,056:INFO:Declaring custom model
2023-12-20 15:25:29,061:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:25:29,067:INFO:Starting cross validation
2023-12-20 15:25:29,068:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:25:35,346:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,353:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,362:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,440:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,448:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,456:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,707:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,710:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,716:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,717:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,726:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,726:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,756:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,763:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,770:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:35,782:INFO:Calculating mean and std
2023-12-20 15:25:35,783:INFO:Creating metrics dataframe
2023-12-20 15:25:35,790:INFO:Finalizing model
2023-12-20 15:25:35,811:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:25:35,811:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:25:35,812:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:25:35,818:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:25:35,818:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:25:35,819:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:25:35,822:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001490 seconds.
2023-12-20 15:25:35,822:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 15:25:35,822:INFO:[LightGBM] [Info] Total Bins 1785
2023-12-20 15:25:35,823:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 15:25:35,824:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 15:25:35,824:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 15:25:35,824:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 15:25:35,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,852:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:35,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,019:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,109:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,113:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,116:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,129:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,131:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,147:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,151:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,156:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,159:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,163:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,165:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,177:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,180:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,182:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,197:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,203:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,212:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,228:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,230:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,232:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,236:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,243:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,250:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,256:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,263:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,270:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,278:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,283:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,288:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,290:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,307:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,317:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,319:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,323:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,326:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,328:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,330:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,335:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,342:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,343:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,349:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,352:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,359:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,373:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,380:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,385:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,387:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,392:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,397:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,398:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,406:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,408:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,412:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,417:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,424:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,428:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,435:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,443:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,449:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,457:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,464:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,468:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,470:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,473:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,477:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,484:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,491:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,498:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,504:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,511:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,521:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,523:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,525:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,528:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,539:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,541:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,546:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,548:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,557:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,562:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,564:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,580:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,597:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,602:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,608:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,620:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,623:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,635:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,647:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,654:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,660:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,685:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,686:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,702:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,726:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,732:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,739:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,752:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,757:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,764:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,766:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,778:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,782:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,788:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:36,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,056:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,072:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,166:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,168:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,176:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,181:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,195:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:25:37,279:INFO:Uploading results into container
2023-12-20 15:25:37,280:INFO:Uploading model into container now
2023-12-20 15:25:37,280:INFO:_master_model_container: 16
2023-12-20 15:25:37,280:INFO:_display_container: 3
2023-12-20 15:25:37,281:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:25:37,281:INFO:create_model() successfully completed......................................
2023-12-20 15:25:37,389:INFO:SubProcess create_model() end ==================================
2023-12-20 15:25:37,389:INFO:choose_better activated
2023-12-20 15:25:37,392:INFO:SubProcess create_model() called ==================================
2023-12-20 15:25:37,392:INFO:Initializing create_model()
2023-12-20 15:25:37,392:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:25:37,392:INFO:Checking exceptions
2023-12-20 15:25:37,393:INFO:Importing libraries
2023-12-20 15:25:37,394:INFO:Copying training dataset
2023-12-20 15:25:37,397:INFO:Defining folds
2023-12-20 15:25:37,397:INFO:Declaring metric variables
2023-12-20 15:25:37,397:INFO:Importing untrained model
2023-12-20 15:25:37,397:INFO:Declaring custom model
2023-12-20 15:25:37,398:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:25:37,398:INFO:Starting cross validation
2023-12-20 15:25:37,398:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:25:40,317:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,326:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,335:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,338:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,347:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,351:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,355:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,358:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,366:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,375:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,383:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,390:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,573:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,581:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,588:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:25:40,600:INFO:Calculating mean and std
2023-12-20 15:25:40,601:INFO:Creating metrics dataframe
2023-12-20 15:25:40,604:INFO:Finalizing model
2023-12-20 15:25:40,636:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001758 seconds.
2023-12-20 15:25:40,636:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 15:25:40,636:INFO:[LightGBM] [Info] Total Bins 1785
2023-12-20 15:25:40,636:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 15:25:40,637:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 15:25:40,637:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 15:25:40,637:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 15:25:41,265:INFO:Uploading results into container
2023-12-20 15:25:41,265:INFO:Uploading model into container now
2023-12-20 15:25:41,266:INFO:_master_model_container: 17
2023-12-20 15:25:41,266:INFO:_display_container: 4
2023-12-20 15:25:41,266:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:25:41,266:INFO:create_model() successfully completed......................................
2023-12-20 15:25:41,352:INFO:SubProcess create_model() end ==================================
2023-12-20 15:25:41,353:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8257
2023-12-20 15:25:41,353:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8268
2023-12-20 15:25:41,354:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 15:25:41,354:INFO:choose_better completed
2023-12-20 15:25:41,364:INFO:_master_model_container: 17
2023-12-20 15:25:41,364:INFO:_display_container: 3
2023-12-20 15:25:41,365:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:25:41,365:INFO:tune_model() successfully completed......................................
2023-12-20 15:25:41,455:INFO:Initializing evaluate_model()
2023-12-20 15:25:41,455:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 15:25:41,469:INFO:Initializing plot_model()
2023-12-20 15:25:41,469:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 15:25:41,470:INFO:Checking exceptions
2023-12-20 15:25:41,473:INFO:Preloading libraries
2023-12-20 15:25:41,531:INFO:Copying training dataset
2023-12-20 15:25:41,531:INFO:Plot type: pipeline
2023-12-20 15:25:41,683:INFO:Visual Rendered Successfully
2023-12-20 15:25:41,762:INFO:plot_model() successfully completed......................................
2023-12-20 15:25:46,169:INFO:Initializing plot_model()
2023-12-20 15:25:46,169:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 15:25:46,169:INFO:Checking exceptions
2023-12-20 15:25:46,174:INFO:Preloading libraries
2023-12-20 15:25:46,236:INFO:Copying training dataset
2023-12-20 15:25:46,236:INFO:Plot type: auc
2023-12-20 15:25:46,325:INFO:Fitting Model
2023-12-20 15:25:46,326:INFO:Scoring test/hold-out set
2023-12-20 15:25:46,327:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:25:46,327:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:25:46,327:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:25:46,333:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:25:46,333:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:25:46,333:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:25:46,496:INFO:Visual Rendered Successfully
2023-12-20 15:25:46,563:INFO:plot_model() successfully completed......................................
2023-12-20 15:32:21,327:INFO:Initializing plot_model()
2023-12-20 15:32:21,328:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=parameter, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 15:32:21,328:INFO:Checking exceptions
2023-12-20 15:32:21,333:INFO:Preloading libraries
2023-12-20 15:32:21,389:INFO:Copying training dataset
2023-12-20 15:32:21,390:INFO:Plot type: parameter
2023-12-20 15:32:21,394:INFO:Visual Rendered Successfully
2023-12-20 15:32:21,508:INFO:plot_model() successfully completed......................................
2023-12-20 15:32:23,026:INFO:Initializing plot_model()
2023-12-20 15:32:23,026:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 15:32:23,027:INFO:Checking exceptions
2023-12-20 15:32:23,030:INFO:Preloading libraries
2023-12-20 15:32:23,084:INFO:Copying training dataset
2023-12-20 15:32:23,084:INFO:Plot type: auc
2023-12-20 15:32:23,194:INFO:Fitting Model
2023-12-20 15:32:23,195:INFO:Scoring test/hold-out set
2023-12-20 15:32:23,196:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:32:23,196:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:32:23,196:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:32:23,210:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:32:23,210:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:32:23,210:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:32:23,336:INFO:Visual Rendered Successfully
2023-12-20 15:32:23,406:INFO:plot_model() successfully completed......................................
2023-12-20 15:32:28,235:INFO:Initializing plot_model()
2023-12-20 15:32:28,236:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=calibration, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 15:32:28,236:INFO:Checking exceptions
2023-12-20 15:32:28,901:INFO:Initializing plot_model()
2023-12-20 15:32:28,902:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=manifold, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 15:32:28,902:INFO:Checking exceptions
2023-12-20 15:32:36,687:INFO:Initializing plot_model()
2023-12-20 15:32:36,687:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=auc, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 15:32:36,687:INFO:Checking exceptions
2023-12-20 15:32:36,689:INFO:Preloading libraries
2023-12-20 15:32:36,741:INFO:Copying training dataset
2023-12-20 15:32:36,741:INFO:Plot type: auc
2023-12-20 15:32:36,843:INFO:Fitting Model
2023-12-20 15:32:36,843:INFO:Scoring test/hold-out set
2023-12-20 15:32:36,844:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:32:36,844:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:32:36,844:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:32:36,858:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:32:36,858:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:32:36,858:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:32:37,016:INFO:Visual Rendered Successfully
2023-12-20 15:32:37,100:INFO:plot_model() successfully completed......................................
2023-12-20 15:45:18,980:INFO:Initializing tune_model()
2023-12-20 15:45:18,980:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:45:18,980:INFO:Checking exceptions
2023-12-20 15:45:19,003:INFO:Copying training dataset
2023-12-20 15:45:19,008:INFO:Checking base model
2023-12-20 15:45:19,009:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 15:45:19,011:INFO:Declaring metric variables
2023-12-20 15:45:19,013:INFO:Defining Hyperparameters
2023-12-20 15:45:19,127:INFO:Tuning with n_jobs=-1
2023-12-20 15:45:19,127:INFO:Initializing RandomizedSearchCV
2023-12-20 15:45:41,160:INFO:best_params: {'actual_estimator__reg_lambda': 0.5, 'actual_estimator__reg_alpha': 0.005, 'actual_estimator__num_leaves': 50, 'actual_estimator__n_estimators': 210, 'actual_estimator__min_split_gain': 0.2, 'actual_estimator__min_child_samples': 76, 'actual_estimator__learning_rate': 0.05, 'actual_estimator__feature_fraction': 1.0, 'actual_estimator__bagging_freq': 5, 'actual_estimator__bagging_fraction': 0.8}
2023-12-20 15:45:41,160:INFO:Hyperparameter search completed
2023-12-20 15:45:41,160:INFO:SubProcess create_model() called ==================================
2023-12-20 15:45:41,160:INFO:Initializing create_model()
2023-12-20 15:45:41,160:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026517775B90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.5, 'reg_alpha': 0.005, 'num_leaves': 50, 'n_estimators': 210, 'min_split_gain': 0.2, 'min_child_samples': 76, 'learning_rate': 0.05, 'feature_fraction': 1.0, 'bagging_freq': 5, 'bagging_fraction': 0.8})
2023-12-20 15:45:41,160:INFO:Checking exceptions
2023-12-20 15:45:41,160:INFO:Importing libraries
2023-12-20 15:45:41,160:INFO:Copying training dataset
2023-12-20 15:45:41,160:INFO:Defining folds
2023-12-20 15:45:41,160:INFO:Declaring metric variables
2023-12-20 15:45:41,176:INFO:Importing untrained model
2023-12-20 15:45:41,176:INFO:Declaring custom model
2023-12-20 15:45:41,180:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:45:41,182:INFO:Starting cross validation
2023-12-20 15:45:41,182:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:45:47,228:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,236:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,246:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,288:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,295:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,299:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,477:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,484:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,499:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,586:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,602:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,602:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,612:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,619:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,621:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:47,636:INFO:Calculating mean and std
2023-12-20 15:45:47,636:INFO:Creating metrics dataframe
2023-12-20 15:45:47,647:INFO:Finalizing model
2023-12-20 15:45:47,662:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:45:47,662:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:45:47,662:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:45:47,674:INFO:[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0
2023-12-20 15:45:47,675:INFO:[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8
2023-12-20 15:45:47,675:INFO:[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5
2023-12-20 15:45:47,677:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001770 seconds.
2023-12-20 15:45:47,677:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 15:45:47,678:INFO:[LightGBM] [Info] Total Bins 1785
2023-12-20 15:45:47,678:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 15:45:47,679:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 15:45:47,679:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 15:45:47,679:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 15:45:47,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,701:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,704:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,717:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,729:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,734:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,735:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,746:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,747:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,754:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,755:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,780:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,798:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,805:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,889:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,929:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:47,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,057:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,061:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,077:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,080:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,088:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,111:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,115:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,127:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,143:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,144:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,161:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,178:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,185:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,193:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,194:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,206:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,209:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,221:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,223:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,226:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,240:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,244:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,254:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,255:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,277:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,285:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,297:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,299:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,304:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,309:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,311:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,318:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,321:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,325:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,333:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,346:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,347:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,366:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,378:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,379:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,396:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,399:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,413:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,430:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,432:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,446:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,460:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,466:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,479:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,488:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,496:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,512:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,527:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,543:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,550:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,554:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,556:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,570:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,571:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,587:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,593:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,604:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,624:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,628:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,636:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,644:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,657:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,663:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,669:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,671:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,676:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,684:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,687:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,705:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,712:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,719:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,720:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,730:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,736:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,749:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,753:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,763:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,765:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,770:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,783:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,786:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,794:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,879:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,985:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:48,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,083:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,097:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,112:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,124:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,128:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:45:49,192:INFO:Uploading results into container
2023-12-20 15:45:49,193:INFO:Uploading model into container now
2023-12-20 15:45:49,193:INFO:_master_model_container: 18
2023-12-20 15:45:49,193:INFO:_display_container: 4
2023-12-20 15:45:49,194:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:45:49,194:INFO:create_model() successfully completed......................................
2023-12-20 15:45:49,318:INFO:SubProcess create_model() end ==================================
2023-12-20 15:45:49,318:INFO:choose_better activated
2023-12-20 15:45:49,318:INFO:SubProcess create_model() called ==================================
2023-12-20 15:45:49,318:INFO:Initializing create_model()
2023-12-20 15:45:49,318:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:45:49,318:INFO:Checking exceptions
2023-12-20 15:45:49,318:INFO:Importing libraries
2023-12-20 15:45:49,318:INFO:Copying training dataset
2023-12-20 15:45:49,329:INFO:Defining folds
2023-12-20 15:45:49,329:INFO:Declaring metric variables
2023-12-20 15:45:49,329:INFO:Importing untrained model
2023-12-20 15:45:49,329:INFO:Declaring custom model
2023-12-20 15:45:49,330:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:45:49,330:INFO:Starting cross validation
2023-12-20 15:45:49,331:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:45:52,194:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,206:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,215:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,223:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,228:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,228:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,254:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,262:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,268:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,269:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,275:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,283:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,457:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,472:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,472:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:45:52,495:INFO:Calculating mean and std
2023-12-20 15:45:52,495:INFO:Creating metrics dataframe
2023-12-20 15:45:52,495:INFO:Finalizing model
2023-12-20 15:45:52,528:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001152 seconds.
2023-12-20 15:45:52,528:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-20 15:45:52,528:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-20 15:45:52,528:INFO:[LightGBM] [Info] Total Bins 1785
2023-12-20 15:45:52,529:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 15:45:52,529:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 15:45:52,529:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 15:45:52,529:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 15:45:53,038:INFO:Uploading results into container
2023-12-20 15:45:53,038:INFO:Uploading model into container now
2023-12-20 15:45:53,038:INFO:_master_model_container: 19
2023-12-20 15:45:53,038:INFO:_display_container: 5
2023-12-20 15:45:53,038:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:45:53,038:INFO:create_model() successfully completed......................................
2023-12-20 15:45:53,155:INFO:SubProcess create_model() end ==================================
2023-12-20 15:45:53,155:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7139, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8257
2023-12-20 15:45:53,155:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8268
2023-12-20 15:45:53,169:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 15:45:53,169:INFO:choose_better completed
2023-12-20 15:45:53,172:INFO:_master_model_container: 19
2023-12-20 15:45:53,172:INFO:_display_container: 4
2023-12-20 15:45:53,172:INFO:LGBMClassifier(bagging_fraction=0.8, bagging_freq=5, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=1.0,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=76, min_child_weight=0.001, min_split_gain=0.2,
               n_estimators=210, n_jobs=-1, num_leaves=50, objective=None,
               random_state=7139, reg_alpha=0.005, reg_lambda=0.5,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:45:53,172:INFO:tune_model() successfully completed......................................
2023-12-20 15:45:53,287:INFO:Initializing tune_model()
2023-12-20 15:45:53,287:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:45:53,287:INFO:Checking exceptions
2023-12-20 15:45:53,296:INFO:Copying training dataset
2023-12-20 15:45:53,305:INFO:Checking base model
2023-12-20 15:45:53,306:INFO:Base model : Gradient Boosting Classifier
2023-12-20 15:45:53,308:INFO:Declaring metric variables
2023-12-20 15:45:53,310:INFO:Defining Hyperparameters
2023-12-20 15:45:53,434:INFO:Tuning with n_jobs=-1
2023-12-20 15:45:53,434:INFO:Initializing RandomizedSearchCV
2023-12-20 15:46:08,282:INFO:best_params: {'actual_estimator__subsample': 0.85, 'actual_estimator__n_estimators': 200, 'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.0001, 'actual_estimator__max_features': 'log2', 'actual_estimator__max_depth': 8, 'actual_estimator__learning_rate': 0.005}
2023-12-20 15:46:08,282:INFO:Hyperparameter search completed
2023-12-20 15:46:08,282:INFO:SubProcess create_model() called ==================================
2023-12-20 15:46:08,282:INFO:Initializing create_model()
2023-12-20 15:46:08,282:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265173D3C90>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.85, 'n_estimators': 200, 'min_samples_split': 7, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.0001, 'max_features': 'log2', 'max_depth': 8, 'learning_rate': 0.005})
2023-12-20 15:46:08,282:INFO:Checking exceptions
2023-12-20 15:46:08,282:INFO:Importing libraries
2023-12-20 15:46:08,282:INFO:Copying training dataset
2023-12-20 15:46:08,298:INFO:Defining folds
2023-12-20 15:46:08,298:INFO:Declaring metric variables
2023-12-20 15:46:08,299:INFO:Importing untrained model
2023-12-20 15:46:08,299:INFO:Declaring custom model
2023-12-20 15:46:08,299:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:46:08,299:INFO:Starting cross validation
2023-12-20 15:46:08,299:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:46:13,403:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,403:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,412:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:46:13,412:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,434:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,439:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,442:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:46:13,446:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,529:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,529:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,529:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:46:13,529:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,582:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,595:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,595:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,611:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,628:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,628:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:13,628:INFO:Calculating mean and std
2023-12-20 15:46:13,628:INFO:Creating metrics dataframe
2023-12-20 15:46:13,644:INFO:Finalizing model
2023-12-20 15:46:17,961:INFO:Uploading results into container
2023-12-20 15:46:17,961:INFO:Uploading model into container now
2023-12-20 15:46:17,971:INFO:_master_model_container: 20
2023-12-20 15:46:17,971:INFO:_display_container: 5
2023-12-20 15:46:17,971:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.005, loss='log_loss', max_depth=8,
                           max_features='log2', max_leaf_nodes=None,
                           min_impurity_decrease=0.0001, min_samples_leaf=3,
                           min_samples_split=7, min_weight_fraction_leaf=0.0,
                           n_estimators=200, n_iter_no_change=None,
                           random_state=7139, subsample=0.85, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:46:17,971:INFO:create_model() successfully completed......................................
2023-12-20 15:46:18,063:INFO:SubProcess create_model() end ==================================
2023-12-20 15:46:18,063:INFO:choose_better activated
2023-12-20 15:46:18,063:INFO:SubProcess create_model() called ==================================
2023-12-20 15:46:18,077:INFO:Initializing create_model()
2023-12-20 15:46:18,077:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:46:18,077:INFO:Checking exceptions
2023-12-20 15:46:18,078:INFO:Importing libraries
2023-12-20 15:46:18,078:INFO:Copying training dataset
2023-12-20 15:46:18,078:INFO:Defining folds
2023-12-20 15:46:18,078:INFO:Declaring metric variables
2023-12-20 15:46:18,078:INFO:Importing untrained model
2023-12-20 15:46:18,078:INFO:Declaring custom model
2023-12-20 15:46:18,078:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:46:18,078:INFO:Starting cross validation
2023-12-20 15:46:18,078:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:46:21,245:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,245:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,261:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,284:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,284:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,295:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,314:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,314:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,314:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,345:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,345:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,362:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,414:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,414:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,414:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:21,428:INFO:Calculating mean and std
2023-12-20 15:46:21,428:INFO:Creating metrics dataframe
2023-12-20 15:46:21,428:INFO:Finalizing model
2023-12-20 15:46:24,531:INFO:Uploading results into container
2023-12-20 15:46:24,531:INFO:Uploading model into container now
2023-12-20 15:46:24,531:INFO:_master_model_container: 21
2023-12-20 15:46:24,531:INFO:_display_container: 6
2023-12-20 15:46:24,531:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:46:24,531:INFO:create_model() successfully completed......................................
2023-12-20 15:46:24,615:INFO:SubProcess create_model() end ==================================
2023-12-20 15:46:24,615:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8227
2023-12-20 15:46:24,615:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.005, loss='log_loss', max_depth=8,
                           max_features='log2', max_leaf_nodes=None,
                           min_impurity_decrease=0.0001, min_samples_leaf=3,
                           min_samples_split=7, min_weight_fraction_leaf=0.0,
                           n_estimators=200, n_iter_no_change=None,
                           random_state=7139, subsample=0.85, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8164
2023-12-20 15:46:24,615:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2023-12-20 15:46:24,615:INFO:choose_better completed
2023-12-20 15:46:24,615:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-12-20 15:46:24,631:INFO:_master_model_container: 21
2023-12-20 15:46:24,631:INFO:_display_container: 5
2023-12-20 15:46:24,631:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7139, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:46:24,631:INFO:tune_model() successfully completed......................................
2023-12-20 15:46:24,744:INFO:Initializing tune_model()
2023-12-20 15:46:24,744:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:46:24,752:INFO:Checking exceptions
2023-12-20 15:46:24,762:INFO:Copying training dataset
2023-12-20 15:46:24,772:INFO:Checking base model
2023-12-20 15:46:24,772:INFO:Base model : Extreme Gradient Boosting
2023-12-20 15:46:24,775:INFO:Declaring metric variables
2023-12-20 15:46:24,777:INFO:Defining Hyperparameters
2023-12-20 15:46:24,873:INFO:Tuning with n_jobs=-1
2023-12-20 15:46:24,873:INFO:Initializing RandomizedSearchCV
2023-12-20 15:46:32,399:INFO:best_params: {'actual_estimator__subsample': 0.9, 'actual_estimator__scale_pos_weight': 14.700000000000003, 'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 0.3, 'actual_estimator__n_estimators': 160, 'actual_estimator__min_child_weight': 1, 'actual_estimator__max_depth': 8, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__colsample_bytree': 0.5}
2023-12-20 15:46:32,400:INFO:Hyperparameter search completed
2023-12-20 15:46:32,400:INFO:SubProcess create_model() called ==================================
2023-12-20 15:46:32,401:INFO:Initializing create_model()
2023-12-20 15:46:32,402:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B61750>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.9, 'scale_pos_weight': 14.700000000000003, 'reg_lambda': 3, 'reg_alpha': 0.3, 'n_estimators': 160, 'min_child_weight': 1, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.5})
2023-12-20 15:46:32,402:INFO:Checking exceptions
2023-12-20 15:46:32,402:INFO:Importing libraries
2023-12-20 15:46:32,402:INFO:Copying training dataset
2023-12-20 15:46:32,411:INFO:Defining folds
2023-12-20 15:46:32,411:INFO:Declaring metric variables
2023-12-20 15:46:32,414:INFO:Importing untrained model
2023-12-20 15:46:32,414:INFO:Declaring custom model
2023-12-20 15:46:32,419:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:46:32,426:INFO:Starting cross validation
2023-12-20 15:46:32,427:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:46:34,566:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,580:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,580:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,580:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,580:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,580:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,580:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,593:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,594:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,598:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,600:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,600:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,600:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,600:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,614:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:34,629:INFO:Calculating mean and std
2023-12-20 15:46:34,629:INFO:Creating metrics dataframe
2023-12-20 15:46:34,636:INFO:Finalizing model
2023-12-20 15:46:35,407:INFO:Uploading results into container
2023-12-20 15:46:35,408:INFO:Uploading model into container now
2023-12-20 15:46:35,409:INFO:_master_model_container: 22
2023-12-20 15:46:35,409:INFO:_display_container: 6
2023-12-20 15:46:35,410:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.5, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.1, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=8, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=160, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 15:46:35,410:INFO:create_model() successfully completed......................................
2023-12-20 15:46:35,536:INFO:SubProcess create_model() end ==================================
2023-12-20 15:46:35,536:INFO:choose_better activated
2023-12-20 15:46:35,539:INFO:SubProcess create_model() called ==================================
2023-12-20 15:46:35,539:INFO:Initializing create_model()
2023-12-20 15:46:35,539:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:46:35,539:INFO:Checking exceptions
2023-12-20 15:46:35,539:INFO:Importing libraries
2023-12-20 15:46:35,539:INFO:Copying training dataset
2023-12-20 15:46:35,539:INFO:Defining folds
2023-12-20 15:46:35,539:INFO:Declaring metric variables
2023-12-20 15:46:35,539:INFO:Importing untrained model
2023-12-20 15:46:35,539:INFO:Declaring custom model
2023-12-20 15:46:35,539:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:46:35,539:INFO:Starting cross validation
2023-12-20 15:46:35,539:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:46:36,527:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,548:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,551:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,551:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,551:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,562:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,564:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,564:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,564:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,578:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,579:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,579:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,594:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,599:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:36,615:INFO:Calculating mean and std
2023-12-20 15:46:36,616:INFO:Creating metrics dataframe
2023-12-20 15:46:36,616:INFO:Finalizing model
2023-12-20 15:46:37,037:INFO:Uploading results into container
2023-12-20 15:46:37,037:INFO:Uploading model into container now
2023-12-20 15:46:37,037:INFO:_master_model_container: 23
2023-12-20 15:46:37,037:INFO:_display_container: 7
2023-12-20 15:46:37,040:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 15:46:37,040:INFO:create_model() successfully completed......................................
2023-12-20 15:46:37,170:INFO:SubProcess create_model() end ==================================
2023-12-20 15:46:37,170:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8208
2023-12-20 15:46:37,170:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.5, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.1, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=8, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=160, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8264
2023-12-20 15:46:37,170:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.5, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.1, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=8, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=160, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2023-12-20 15:46:37,170:INFO:choose_better completed
2023-12-20 15:46:37,170:INFO:_master_model_container: 23
2023-12-20 15:46:37,170:INFO:_display_container: 6
2023-12-20 15:46:37,170:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.5, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.1, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=8, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=160, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 15:46:37,170:INFO:tune_model() successfully completed......................................
2023-12-20 15:46:37,311:INFO:Initializing tune_model()
2023-12-20 15:46:37,311:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:46:37,311:INFO:Checking exceptions
2023-12-20 15:46:37,321:INFO:Copying training dataset
2023-12-20 15:46:37,339:INFO:Checking base model
2023-12-20 15:46:37,340:INFO:Base model : Random Forest Classifier
2023-12-20 15:46:37,343:INFO:Declaring metric variables
2023-12-20 15:46:37,343:INFO:Defining Hyperparameters
2023-12-20 15:46:37,449:INFO:Tuning with n_jobs=-1
2023-12-20 15:46:37,449:INFO:Initializing RandomizedSearchCV
2023-12-20 15:46:42,314:INFO:best_params: {'actual_estimator__n_estimators': 300, 'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 3, 'actual_estimator__min_impurity_decrease': 0.002, 'actual_estimator__max_features': 'log2', 'actual_estimator__max_depth': 3, 'actual_estimator__criterion': 'gini', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': False}
2023-12-20 15:46:42,314:INFO:Hyperparameter search completed
2023-12-20 15:46:42,314:INFO:SubProcess create_model() called ==================================
2023-12-20 15:46:42,314:INFO:Initializing create_model()
2023-12-20 15:46:42,314:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000026518B8A950>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 300, 'min_samples_split': 9, 'min_samples_leaf': 3, 'min_impurity_decrease': 0.002, 'max_features': 'log2', 'max_depth': 3, 'criterion': 'gini', 'class_weight': {}, 'bootstrap': False})
2023-12-20 15:46:42,314:INFO:Checking exceptions
2023-12-20 15:46:42,314:INFO:Importing libraries
2023-12-20 15:46:42,314:INFO:Copying training dataset
2023-12-20 15:46:42,319:INFO:Defining folds
2023-12-20 15:46:42,319:INFO:Declaring metric variables
2023-12-20 15:46:42,319:INFO:Importing untrained model
2023-12-20 15:46:42,319:INFO:Declaring custom model
2023-12-20 15:46:42,319:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:46:42,330:INFO:Starting cross validation
2023-12-20 15:46:42,330:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:46:43,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:46:43,076:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,077:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,078:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:46:43,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:46:43,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:46:43,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,097:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,097:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,104:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:46:43,104:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:43,111:INFO:Calculating mean and std
2023-12-20 15:46:43,111:INFO:Creating metrics dataframe
2023-12-20 15:46:43,111:INFO:Finalizing model
2023-12-20 15:46:43,487:INFO:Uploading results into container
2023-12-20 15:46:43,488:INFO:Uploading model into container now
2023-12-20 15:46:43,489:INFO:_master_model_container: 24
2023-12-20 15:46:43,489:INFO:_display_container: 7
2023-12-20 15:46:43,489:INFO:RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=3, max_features='log2',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.002, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False)
2023-12-20 15:46:43,489:INFO:create_model() successfully completed......................................
2023-12-20 15:46:43,573:INFO:SubProcess create_model() end ==================================
2023-12-20 15:46:43,573:INFO:choose_better activated
2023-12-20 15:46:43,577:INFO:SubProcess create_model() called ==================================
2023-12-20 15:46:43,578:INFO:Initializing create_model()
2023-12-20 15:46:43,578:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:46:43,578:INFO:Checking exceptions
2023-12-20 15:46:43,579:INFO:Importing libraries
2023-12-20 15:46:43,579:INFO:Copying training dataset
2023-12-20 15:46:43,584:INFO:Defining folds
2023-12-20 15:46:43,584:INFO:Declaring metric variables
2023-12-20 15:46:43,584:INFO:Importing untrained model
2023-12-20 15:46:43,584:INFO:Declaring custom model
2023-12-20 15:46:43,585:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:46:43,585:INFO:Starting cross validation
2023-12-20 15:46:43,586:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,136:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,136:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,136:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,136:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,136:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:44,152:INFO:Calculating mean and std
2023-12-20 15:46:44,152:INFO:Creating metrics dataframe
2023-12-20 15:46:44,152:INFO:Finalizing model
2023-12-20 15:46:44,392:INFO:Uploading results into container
2023-12-20 15:46:44,392:INFO:Uploading model into container now
2023-12-20 15:46:44,392:INFO:_master_model_container: 25
2023-12-20 15:46:44,392:INFO:_display_container: 8
2023-12-20 15:46:44,392:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False)
2023-12-20 15:46:44,392:INFO:create_model() successfully completed......................................
2023-12-20 15:46:44,488:INFO:SubProcess create_model() end ==================================
2023-12-20 15:46:44,488:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False) result for Accuracy is 0.8193
2023-12-20 15:46:44,488:INFO:RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight={},
                       criterion='gini', max_depth=3, max_features='log2',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.002, min_samples_leaf=3,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       n_estimators=300, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False) result for Accuracy is 0.7995
2023-12-20 15:46:44,488:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False) is best model
2023-12-20 15:46:44,488:INFO:choose_better completed
2023-12-20 15:46:44,488:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-12-20 15:46:44,506:INFO:_master_model_container: 25
2023-12-20 15:46:44,506:INFO:_display_container: 7
2023-12-20 15:46:44,506:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7139, verbose=0, warm_start=False)
2023-12-20 15:46:44,506:INFO:tune_model() successfully completed......................................
2023-12-20 15:46:44,630:INFO:Initializing tune_model()
2023-12-20 15:46:44,631:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:46:44,631:INFO:Checking exceptions
2023-12-20 15:46:44,647:INFO:Copying training dataset
2023-12-20 15:46:44,647:INFO:Checking base model
2023-12-20 15:46:44,647:INFO:Base model : Ada Boost Classifier
2023-12-20 15:46:44,661:INFO:Declaring metric variables
2023-12-20 15:46:44,663:INFO:Defining Hyperparameters
2023-12-20 15:46:44,767:INFO:Tuning with n_jobs=-1
2023-12-20 15:46:44,767:INFO:Initializing RandomizedSearchCV
2023-12-20 15:46:49,727:INFO:best_params: {'actual_estimator__n_estimators': 200, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__algorithm': 'SAMME.R'}
2023-12-20 15:46:49,727:INFO:Hyperparameter search completed
2023-12-20 15:46:49,727:INFO:SubProcess create_model() called ==================================
2023-12-20 15:46:49,727:INFO:Initializing create_model()
2023-12-20 15:46:49,727:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000265173F3650>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 200, 'learning_rate': 0.2, 'algorithm': 'SAMME.R'})
2023-12-20 15:46:49,727:INFO:Checking exceptions
2023-12-20 15:46:49,727:INFO:Importing libraries
2023-12-20 15:46:49,727:INFO:Copying training dataset
2023-12-20 15:46:49,735:INFO:Defining folds
2023-12-20 15:46:49,735:INFO:Declaring metric variables
2023-12-20 15:46:49,735:INFO:Importing untrained model
2023-12-20 15:46:49,735:INFO:Declaring custom model
2023-12-20 15:46:49,735:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:46:49,743:INFO:Starting cross validation
2023-12-20 15:46:49,745:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:46:51,044:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,050:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,055:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,056:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,062:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,079:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,097:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,113:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,114:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,128:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,128:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,144:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:51,144:INFO:Calculating mean and std
2023-12-20 15:46:51,144:INFO:Creating metrics dataframe
2023-12-20 15:46:51,161:INFO:Finalizing model
2023-12-20 15:46:52,214:INFO:Uploading results into container
2023-12-20 15:46:52,214:INFO:Uploading model into container now
2023-12-20 15:46:52,216:INFO:_master_model_container: 26
2023-12-20 15:46:52,216:INFO:_display_container: 8
2023-12-20 15:46:52,216:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=0.2, n_estimators=200,
                   random_state=7139)
2023-12-20 15:46:52,216:INFO:create_model() successfully completed......................................
2023-12-20 15:46:52,330:INFO:SubProcess create_model() end ==================================
2023-12-20 15:46:52,331:INFO:choose_better activated
2023-12-20 15:46:52,335:INFO:SubProcess create_model() called ==================================
2023-12-20 15:46:52,336:INFO:Initializing create_model()
2023-12-20 15:46:52,336:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000026518AE03D0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:46:52,336:INFO:Checking exceptions
2023-12-20 15:46:52,336:INFO:Importing libraries
2023-12-20 15:46:52,336:INFO:Copying training dataset
2023-12-20 15:46:52,343:INFO:Defining folds
2023-12-20 15:46:52,343:INFO:Declaring metric variables
2023-12-20 15:46:52,343:INFO:Importing untrained model
2023-12-20 15:46:52,343:INFO:Declaring custom model
2023-12-20 15:46:52,343:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:46:52,344:INFO:Starting cross validation
2023-12-20 15:46:52,345:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:46:52,684:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,684:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,684:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,694:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,696:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,696:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,696:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,696:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,696:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,696:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,710:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,715:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,729:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,733:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,733:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1396: UserWarning: Note that pos_label (set to 'D') is ignored when average != 'binary' (got 'weighted'). You may use labels=[pos_label] to specify a single positive class.
  warnings.warn(

2023-12-20 15:46:52,745:INFO:Calculating mean and std
2023-12-20 15:46:52,745:INFO:Creating metrics dataframe
2023-12-20 15:46:52,745:INFO:Finalizing model
2023-12-20 15:46:53,023:INFO:Uploading results into container
2023-12-20 15:46:53,024:INFO:Uploading model into container now
2023-12-20 15:46:53,024:INFO:_master_model_container: 27
2023-12-20 15:46:53,024:INFO:_display_container: 9
2023-12-20 15:46:53,025:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139)
2023-12-20 15:46:53,025:INFO:create_model() successfully completed......................................
2023-12-20 15:46:53,137:INFO:SubProcess create_model() end ==================================
2023-12-20 15:46:53,138:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7139) result for Accuracy is 0.8066
2023-12-20 15:46:53,138:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=0.2, n_estimators=200,
                   random_state=7139) result for Accuracy is 0.8136
2023-12-20 15:46:53,139:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=0.2, n_estimators=200,
                   random_state=7139) is best model
2023-12-20 15:46:53,139:INFO:choose_better completed
2023-12-20 15:46:53,144:INFO:_master_model_container: 27
2023-12-20 15:46:53,144:INFO:_display_container: 8
2023-12-20 15:46:53,144:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=0.2, n_estimators=200,
                   random_state=7139)
2023-12-20 15:46:53,144:INFO:tune_model() successfully completed......................................
2023-12-20 15:55:13,084:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 15:55:13,084:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 15:55:13,084:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 15:55:13,084:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 15:55:17,153:INFO:PyCaret ClassificationExperiment
2023-12-20 15:55:17,153:INFO:Logging name: clf-default-name
2023-12-20 15:55:17,153:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 15:55:17,153:INFO:version 3.2.0
2023-12-20 15:55:17,153:INFO:Initializing setup()
2023-12-20 15:55:17,153:INFO:self.USI: 66a1
2023-12-20 15:55:17,153:INFO:self._variable_keys: {'html_param', 'seed', 'logging_param', 'X_train', '_available_plots', 'exp_id', 'USI', 'X_test', 'y_test', 'X', 'fold_generator', 'gpu_n_jobs_param', 'target_param', 'fold_shuffle_param', '_ml_usecase', 'y_train', 'memory', 'fix_imbalance', 'fold_groups_param', 'data', 'exp_name_log', 'log_plots_param', 'n_jobs_param', 'idx', 'pipeline', 'gpu_param', 'y', 'is_multiclass'}
2023-12-20 15:55:17,153:INFO:Checking environment
2023-12-20 15:55:17,153:INFO:python_version: 3.11.5
2023-12-20 15:55:17,153:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 15:55:17,153:INFO:machine: AMD64
2023-12-20 15:55:17,153:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 15:55:17,153:INFO:Memory: svmem(total=16718413824, available=7375507456, percent=55.9, used=9342906368, free=7375507456)
2023-12-20 15:55:17,153:INFO:Physical Core: 12
2023-12-20 15:55:17,153:INFO:Logical Core: 16
2023-12-20 15:55:17,167:INFO:Checking libraries
2023-12-20 15:55:17,167:INFO:System:
2023-12-20 15:55:17,167:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 15:55:17,167:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 15:55:17,167:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 15:55:17,167:INFO:PyCaret required dependencies:
2023-12-20 15:55:17,768:INFO:                 pip: 23.2.1
2023-12-20 15:55:17,768:INFO:          setuptools: 68.0.0
2023-12-20 15:55:17,768:INFO:             pycaret: 3.2.0
2023-12-20 15:55:17,768:INFO:             IPython: 8.15.0
2023-12-20 15:55:17,768:INFO:          ipywidgets: 8.0.4
2023-12-20 15:55:17,768:INFO:                tqdm: 4.65.0
2023-12-20 15:55:17,768:INFO:               numpy: 1.24.3
2023-12-20 15:55:17,768:INFO:              pandas: 1.5.3
2023-12-20 15:55:17,768:INFO:              jinja2: 3.1.2
2023-12-20 15:55:17,768:INFO:               scipy: 1.10.1
2023-12-20 15:55:17,768:INFO:              joblib: 1.2.0
2023-12-20 15:55:17,768:INFO:             sklearn: 1.2.1
2023-12-20 15:55:17,768:INFO:                pyod: 1.1.2
2023-12-20 15:55:17,768:INFO:            imblearn: 0.11.0
2023-12-20 15:55:17,768:INFO:   category_encoders: 2.6.3
2023-12-20 15:55:17,768:INFO:            lightgbm: 4.1.0
2023-12-20 15:55:17,768:INFO:               numba: 0.57.1
2023-12-20 15:55:17,768:INFO:            requests: 2.31.0
2023-12-20 15:55:17,768:INFO:          matplotlib: 3.6.0
2023-12-20 15:55:17,768:INFO:          scikitplot: 0.3.7
2023-12-20 15:55:17,768:INFO:         yellowbrick: 1.5
2023-12-20 15:55:17,768:INFO:              plotly: 5.9.0
2023-12-20 15:55:17,768:INFO:    plotly-resampler: Not installed
2023-12-20 15:55:17,768:INFO:             kaleido: 0.2.1
2023-12-20 15:55:17,768:INFO:           schemdraw: 0.15
2023-12-20 15:55:17,768:INFO:         statsmodels: 0.14.0
2023-12-20 15:55:17,768:INFO:              sktime: 0.21.1
2023-12-20 15:55:17,768:INFO:               tbats: 1.1.3
2023-12-20 15:55:17,768:INFO:            pmdarima: 2.0.4
2023-12-20 15:55:17,768:INFO:              psutil: 5.9.0
2023-12-20 15:55:17,768:INFO:          markupsafe: 2.1.1
2023-12-20 15:55:17,768:INFO:             pickle5: Not installed
2023-12-20 15:55:17,768:INFO:         cloudpickle: 2.2.1
2023-12-20 15:55:17,768:INFO:         deprecation: 2.1.0
2023-12-20 15:55:17,768:INFO:              xxhash: 2.0.2
2023-12-20 15:55:17,768:INFO:           wurlitzer: Not installed
2023-12-20 15:55:17,768:INFO:PyCaret optional dependencies:
2023-12-20 15:55:17,903:INFO:                shap: Not installed
2023-12-20 15:55:17,903:INFO:           interpret: Not installed
2023-12-20 15:55:17,903:INFO:                umap: Not installed
2023-12-20 15:55:17,903:INFO:     ydata_profiling: Not installed
2023-12-20 15:55:17,903:INFO:  explainerdashboard: Not installed
2023-12-20 15:55:17,903:INFO:             autoviz: Not installed
2023-12-20 15:55:17,903:INFO:           fairlearn: Not installed
2023-12-20 15:55:17,903:INFO:          deepchecks: Not installed
2023-12-20 15:55:17,903:INFO:             xgboost: 2.0.2
2023-12-20 15:55:17,903:INFO:            catboost: Not installed
2023-12-20 15:55:17,903:INFO:              kmodes: Not installed
2023-12-20 15:55:17,903:INFO:             mlxtend: Not installed
2023-12-20 15:55:17,903:INFO:       statsforecast: Not installed
2023-12-20 15:55:17,903:INFO:        tune_sklearn: Not installed
2023-12-20 15:55:17,903:INFO:                 ray: Not installed
2023-12-20 15:55:17,903:INFO:            hyperopt: Not installed
2023-12-20 15:55:17,903:INFO:              optuna: Not installed
2023-12-20 15:55:17,903:INFO:               skopt: Not installed
2023-12-20 15:55:17,903:INFO:              mlflow: Not installed
2023-12-20 15:55:17,903:INFO:              gradio: Not installed
2023-12-20 15:55:17,903:INFO:             fastapi: Not installed
2023-12-20 15:55:17,903:INFO:             uvicorn: Not installed
2023-12-20 15:55:17,903:INFO:              m2cgen: Not installed
2023-12-20 15:55:17,903:INFO:           evidently: Not installed
2023-12-20 15:55:17,903:INFO:               fugue: Not installed
2023-12-20 15:55:17,903:INFO:           streamlit: Not installed
2023-12-20 15:55:17,903:INFO:             prophet: Not installed
2023-12-20 15:55:17,903:INFO:None
2023-12-20 15:55:17,903:INFO:Set up data.
2023-12-20 15:55:17,903:INFO:Set up folding strategy.
2023-12-20 15:55:17,903:INFO:Set up train/test split.
2023-12-20 15:55:17,919:INFO:Set up index.
2023-12-20 15:55:17,920:INFO:Assigning column types.
2023-12-20 15:55:17,920:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 15:55:17,933:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 15:55:17,949:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:55:17,969:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:55:17,970:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:55:17,986:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 15:55:17,986:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:55:18,003:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:55:18,003:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:55:18,003:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 15:55:18,037:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:55:18,051:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:55:18,055:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:55:18,067:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:55:18,086:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:55:18,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:55:18,086:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 15:55:18,136:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:55:18,136:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:55:18,165:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:55:18,165:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:55:18,165:INFO:Preparing preprocessing pipeline...
2023-12-20 15:55:18,181:INFO:Set up simple imputation.
2023-12-20 15:55:18,181:INFO:Set up column name cleaning.
2023-12-20 15:55:18,197:INFO:Finished creating preprocessing pipeline.
2023-12-20 15:55:18,197:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Platelets_c', 'Bilirubin_c',
                                             'Cholesterol_c', 'Albumin_c',
                                             'Copper_c', 'Alk_Pho...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 15:55:18,197:INFO:Creating final display dataframe.
2023-12-20 15:55:18,241:INFO:Setup _display_container:                     Description             Value
0                    Session id              8461
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 35)
4        Transformed data shape        (7905, 35)
5   Transformed train set shape        (6324, 35)
6    Transformed test set shape        (1581, 35)
7              Numeric features                34
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              66a1
2023-12-20 15:55:18,287:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:55:18,302:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:55:18,341:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:55:18,342:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:55:18,342:INFO:setup() successfully completed in 1.4s...............
2023-12-20 15:55:18,356:INFO:Initializing compare_models()
2023-12-20 15:55:18,356:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 15:55:18,356:INFO:Checking exceptions
2023-12-20 15:55:18,356:INFO:Preparing display monitor
2023-12-20 15:55:18,381:INFO:Initializing Logistic Regression
2023-12-20 15:55:18,381:INFO:Total runtime is 0.0 minutes
2023-12-20 15:55:18,383:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:18,383:INFO:Initializing create_model()
2023-12-20 15:55:18,383:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:18,383:INFO:Checking exceptions
2023-12-20 15:55:18,383:INFO:Importing libraries
2023-12-20 15:55:18,383:INFO:Copying training dataset
2023-12-20 15:55:18,387:INFO:Defining folds
2023-12-20 15:55:18,387:INFO:Declaring metric variables
2023-12-20 15:55:18,390:INFO:Importing untrained model
2023-12-20 15:55:18,391:INFO:Logistic Regression Imported successfully
2023-12-20 15:55:18,397:INFO:Starting cross validation
2023-12-20 15:55:18,398:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:21,852:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:22,000:INFO:Calculating mean and std
2023-12-20 15:55:22,000:INFO:Creating metrics dataframe
2023-12-20 15:55:22,002:INFO:Uploading results into container
2023-12-20 15:55:22,002:INFO:Uploading model into container now
2023-12-20 15:55:22,002:INFO:_master_model_container: 1
2023-12-20 15:55:22,002:INFO:_display_container: 2
2023-12-20 15:55:22,002:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=8461, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 15:55:22,002:INFO:create_model() successfully completed......................................
2023-12-20 15:55:22,079:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:22,079:INFO:Creating metrics dataframe
2023-12-20 15:55:22,086:INFO:Initializing K Neighbors Classifier
2023-12-20 15:55:22,086:INFO:Total runtime is 0.06173464059829712 minutes
2023-12-20 15:55:22,086:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:22,086:INFO:Initializing create_model()
2023-12-20 15:55:22,086:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:22,086:INFO:Checking exceptions
2023-12-20 15:55:22,086:INFO:Importing libraries
2023-12-20 15:55:22,086:INFO:Copying training dataset
2023-12-20 15:55:22,093:INFO:Defining folds
2023-12-20 15:55:22,093:INFO:Declaring metric variables
2023-12-20 15:55:22,095:INFO:Importing untrained model
2023-12-20 15:55:22,095:INFO:K Neighbors Classifier Imported successfully
2023-12-20 15:55:22,103:INFO:Starting cross validation
2023-12-20 15:55:22,105:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:25,120:INFO:Calculating mean and std
2023-12-20 15:55:25,120:INFO:Creating metrics dataframe
2023-12-20 15:55:25,120:INFO:Uploading results into container
2023-12-20 15:55:25,120:INFO:Uploading model into container now
2023-12-20 15:55:25,120:INFO:_master_model_container: 2
2023-12-20 15:55:25,136:INFO:_display_container: 2
2023-12-20 15:55:25,136:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 15:55:25,136:INFO:create_model() successfully completed......................................
2023-12-20 15:55:25,210:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:25,210:INFO:Creating metrics dataframe
2023-12-20 15:55:25,227:INFO:Initializing Naive Bayes
2023-12-20 15:55:25,227:INFO:Total runtime is 0.11409856478373209 minutes
2023-12-20 15:55:25,227:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:25,227:INFO:Initializing create_model()
2023-12-20 15:55:25,227:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:25,227:INFO:Checking exceptions
2023-12-20 15:55:25,227:INFO:Importing libraries
2023-12-20 15:55:25,227:INFO:Copying training dataset
2023-12-20 15:55:25,227:INFO:Defining folds
2023-12-20 15:55:25,227:INFO:Declaring metric variables
2023-12-20 15:55:25,227:INFO:Importing untrained model
2023-12-20 15:55:25,243:INFO:Naive Bayes Imported successfully
2023-12-20 15:55:25,243:INFO:Starting cross validation
2023-12-20 15:55:25,243:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:28,240:INFO:Calculating mean and std
2023-12-20 15:55:28,240:INFO:Creating metrics dataframe
2023-12-20 15:55:28,254:INFO:Uploading results into container
2023-12-20 15:55:28,255:INFO:Uploading model into container now
2023-12-20 15:55:28,255:INFO:_master_model_container: 3
2023-12-20 15:55:28,255:INFO:_display_container: 2
2023-12-20 15:55:28,255:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 15:55:28,255:INFO:create_model() successfully completed......................................
2023-12-20 15:55:28,328:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:28,328:INFO:Creating metrics dataframe
2023-12-20 15:55:28,343:INFO:Initializing Decision Tree Classifier
2023-12-20 15:55:28,343:INFO:Total runtime is 0.16602160533269245 minutes
2023-12-20 15:55:28,354:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:28,355:INFO:Initializing create_model()
2023-12-20 15:55:28,355:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:28,355:INFO:Checking exceptions
2023-12-20 15:55:28,355:INFO:Importing libraries
2023-12-20 15:55:28,355:INFO:Copying training dataset
2023-12-20 15:55:28,363:INFO:Defining folds
2023-12-20 15:55:28,363:INFO:Declaring metric variables
2023-12-20 15:55:28,366:INFO:Importing untrained model
2023-12-20 15:55:28,366:INFO:Decision Tree Classifier Imported successfully
2023-12-20 15:55:28,385:INFO:Starting cross validation
2023-12-20 15:55:28,385:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:30,720:INFO:Calculating mean and std
2023-12-20 15:55:30,721:INFO:Creating metrics dataframe
2023-12-20 15:55:30,723:INFO:Uploading results into container
2023-12-20 15:55:30,723:INFO:Uploading model into container now
2023-12-20 15:55:30,725:INFO:_master_model_container: 4
2023-12-20 15:55:30,725:INFO:_display_container: 2
2023-12-20 15:55:30,725:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=8461, splitter='best')
2023-12-20 15:55:30,725:INFO:create_model() successfully completed......................................
2023-12-20 15:55:30,797:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:30,797:INFO:Creating metrics dataframe
2023-12-20 15:55:30,812:INFO:Initializing SVM - Linear Kernel
2023-12-20 15:55:30,812:INFO:Total runtime is 0.20717885096867877 minutes
2023-12-20 15:55:30,815:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:30,815:INFO:Initializing create_model()
2023-12-20 15:55:30,815:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:30,815:INFO:Checking exceptions
2023-12-20 15:55:30,815:INFO:Importing libraries
2023-12-20 15:55:30,815:INFO:Copying training dataset
2023-12-20 15:55:30,818:INFO:Defining folds
2023-12-20 15:55:30,818:INFO:Declaring metric variables
2023-12-20 15:55:30,818:INFO:Importing untrained model
2023-12-20 15:55:30,818:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 15:55:30,831:INFO:Starting cross validation
2023-12-20 15:55:30,832:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:30,936:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:55:30,936:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:55:30,936:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:30,936:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:55:30,936:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:55:30,936:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:55:30,948:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:30,948:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:30,948:INFO:Calculating mean and std
2023-12-20 15:55:30,948:INFO:Creating metrics dataframe
2023-12-20 15:55:30,948:INFO:Uploading results into container
2023-12-20 15:55:30,948:INFO:Uploading model into container now
2023-12-20 15:55:30,948:INFO:_master_model_container: 5
2023-12-20 15:55:30,948:INFO:_display_container: 2
2023-12-20 15:55:30,948:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=8461, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 15:55:30,948:INFO:create_model() successfully completed......................................
2023-12-20 15:55:31,028:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:31,028:INFO:Creating metrics dataframe
2023-12-20 15:55:31,046:INFO:Initializing Ridge Classifier
2023-12-20 15:55:31,046:INFO:Total runtime is 0.21108078559239704 minutes
2023-12-20 15:55:31,050:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:31,050:INFO:Initializing create_model()
2023-12-20 15:55:31,050:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:31,050:INFO:Checking exceptions
2023-12-20 15:55:31,050:INFO:Importing libraries
2023-12-20 15:55:31,050:INFO:Copying training dataset
2023-12-20 15:55:31,054:INFO:Defining folds
2023-12-20 15:55:31,055:INFO:Declaring metric variables
2023-12-20 15:55:31,057:INFO:Importing untrained model
2023-12-20 15:55:31,063:INFO:Ridge Classifier Imported successfully
2023-12-20 15:55:31,076:INFO:Starting cross validation
2023-12-20 15:55:31,079:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:31,120:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:55:31,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:31,121:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:55:31,127:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:55:31,129:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:31,130:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:55:31,130:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:31,133:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:31,134:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:55:31,137:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:31,140:INFO:Calculating mean and std
2023-12-20 15:55:31,140:INFO:Creating metrics dataframe
2023-12-20 15:55:31,142:INFO:Uploading results into container
2023-12-20 15:55:31,142:INFO:Uploading model into container now
2023-12-20 15:55:31,143:INFO:_master_model_container: 6
2023-12-20 15:55:31,143:INFO:_display_container: 2
2023-12-20 15:55:31,143:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=8461, solver='auto',
                tol=0.0001)
2023-12-20 15:55:31,143:INFO:create_model() successfully completed......................................
2023-12-20 15:55:31,227:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:31,227:INFO:Creating metrics dataframe
2023-12-20 15:55:31,230:INFO:Initializing Random Forest Classifier
2023-12-20 15:55:31,230:INFO:Total runtime is 0.214134140809377 minutes
2023-12-20 15:55:31,230:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:31,230:INFO:Initializing create_model()
2023-12-20 15:55:31,230:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:31,230:INFO:Checking exceptions
2023-12-20 15:55:31,230:INFO:Importing libraries
2023-12-20 15:55:31,230:INFO:Copying training dataset
2023-12-20 15:55:31,244:INFO:Defining folds
2023-12-20 15:55:31,244:INFO:Declaring metric variables
2023-12-20 15:55:31,247:INFO:Importing untrained model
2023-12-20 15:55:31,251:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:55:31,255:INFO:Starting cross validation
2023-12-20 15:55:31,256:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:31,729:INFO:Calculating mean and std
2023-12-20 15:55:31,729:INFO:Creating metrics dataframe
2023-12-20 15:55:31,729:INFO:Uploading results into container
2023-12-20 15:55:31,729:INFO:Uploading model into container now
2023-12-20 15:55:31,729:INFO:_master_model_container: 7
2023-12-20 15:55:31,729:INFO:_display_container: 2
2023-12-20 15:55:31,729:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=8461, verbose=0, warm_start=False)
2023-12-20 15:55:31,729:INFO:create_model() successfully completed......................................
2023-12-20 15:55:31,813:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:31,813:INFO:Creating metrics dataframe
2023-12-20 15:55:31,815:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 15:55:31,815:INFO:Total runtime is 0.22389454444249468 minutes
2023-12-20 15:55:31,815:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:31,815:INFO:Initializing create_model()
2023-12-20 15:55:31,815:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:31,815:INFO:Checking exceptions
2023-12-20 15:55:31,815:INFO:Importing libraries
2023-12-20 15:55:31,815:INFO:Copying training dataset
2023-12-20 15:55:31,828:INFO:Defining folds
2023-12-20 15:55:31,828:INFO:Declaring metric variables
2023-12-20 15:55:31,829:INFO:Importing untrained model
2023-12-20 15:55:31,829:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 15:55:31,829:INFO:Starting cross validation
2023-12-20 15:55:31,829:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:31,862:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:55:31,863:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:55:31,864:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:55:31,866:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:55:31,867:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:55:31,875:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:55:31,888:INFO:Calculating mean and std
2023-12-20 15:55:31,888:INFO:Creating metrics dataframe
2023-12-20 15:55:31,890:INFO:Uploading results into container
2023-12-20 15:55:31,891:INFO:Uploading model into container now
2023-12-20 15:55:31,891:INFO:_master_model_container: 8
2023-12-20 15:55:31,892:INFO:_display_container: 2
2023-12-20 15:55:31,892:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 15:55:31,892:INFO:create_model() successfully completed......................................
2023-12-20 15:55:31,973:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:31,973:INFO:Creating metrics dataframe
2023-12-20 15:55:31,979:INFO:Initializing Ada Boost Classifier
2023-12-20 15:55:31,979:INFO:Total runtime is 0.22663242816925047 minutes
2023-12-20 15:55:31,979:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:31,979:INFO:Initializing create_model()
2023-12-20 15:55:31,979:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:31,979:INFO:Checking exceptions
2023-12-20 15:55:31,979:INFO:Importing libraries
2023-12-20 15:55:31,979:INFO:Copying training dataset
2023-12-20 15:55:31,979:INFO:Defining folds
2023-12-20 15:55:31,979:INFO:Declaring metric variables
2023-12-20 15:55:31,994:INFO:Importing untrained model
2023-12-20 15:55:31,996:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:55:32,002:INFO:Starting cross validation
2023-12-20 15:55:32,003:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:32,328:INFO:Calculating mean and std
2023-12-20 15:55:32,328:INFO:Creating metrics dataframe
2023-12-20 15:55:32,328:INFO:Uploading results into container
2023-12-20 15:55:32,328:INFO:Uploading model into container now
2023-12-20 15:55:32,328:INFO:_master_model_container: 9
2023-12-20 15:55:32,328:INFO:_display_container: 2
2023-12-20 15:55:32,328:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=8461)
2023-12-20 15:55:32,328:INFO:create_model() successfully completed......................................
2023-12-20 15:55:32,417:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:32,417:INFO:Creating metrics dataframe
2023-12-20 15:55:32,429:INFO:Initializing Gradient Boosting Classifier
2023-12-20 15:55:32,429:INFO:Total runtime is 0.23411961396535236 minutes
2023-12-20 15:55:32,429:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:32,429:INFO:Initializing create_model()
2023-12-20 15:55:32,429:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:32,429:INFO:Checking exceptions
2023-12-20 15:55:32,429:INFO:Importing libraries
2023-12-20 15:55:32,429:INFO:Copying training dataset
2023-12-20 15:55:32,429:INFO:Defining folds
2023-12-20 15:55:32,429:INFO:Declaring metric variables
2023-12-20 15:55:32,429:INFO:Importing untrained model
2023-12-20 15:55:32,429:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:55:32,452:INFO:Starting cross validation
2023-12-20 15:55:32,452:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:35,605:INFO:Calculating mean and std
2023-12-20 15:55:35,605:INFO:Creating metrics dataframe
2023-12-20 15:55:35,605:INFO:Uploading results into container
2023-12-20 15:55:35,605:INFO:Uploading model into container now
2023-12-20 15:55:35,605:INFO:_master_model_container: 10
2023-12-20 15:55:35,605:INFO:_display_container: 2
2023-12-20 15:55:35,605:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=8461, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:55:35,605:INFO:create_model() successfully completed......................................
2023-12-20 15:55:35,692:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:35,692:INFO:Creating metrics dataframe
2023-12-20 15:55:35,703:INFO:Initializing Linear Discriminant Analysis
2023-12-20 15:55:35,703:INFO:Total runtime is 0.2886995991071065 minutes
2023-12-20 15:55:35,704:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:35,706:INFO:Initializing create_model()
2023-12-20 15:55:35,706:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:35,706:INFO:Checking exceptions
2023-12-20 15:55:35,706:INFO:Importing libraries
2023-12-20 15:55:35,706:INFO:Copying training dataset
2023-12-20 15:55:35,706:INFO:Defining folds
2023-12-20 15:55:35,706:INFO:Declaring metric variables
2023-12-20 15:55:35,706:INFO:Importing untrained model
2023-12-20 15:55:35,706:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 15:55:35,719:INFO:Starting cross validation
2023-12-20 15:55:35,720:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:35,794:INFO:Calculating mean and std
2023-12-20 15:55:35,794:INFO:Creating metrics dataframe
2023-12-20 15:55:35,794:INFO:Uploading results into container
2023-12-20 15:55:35,794:INFO:Uploading model into container now
2023-12-20 15:55:35,794:INFO:_master_model_container: 11
2023-12-20 15:55:35,794:INFO:_display_container: 2
2023-12-20 15:55:35,794:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 15:55:35,794:INFO:create_model() successfully completed......................................
2023-12-20 15:55:35,889:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:35,889:INFO:Creating metrics dataframe
2023-12-20 15:55:35,889:INFO:Initializing Extra Trees Classifier
2023-12-20 15:55:35,889:INFO:Total runtime is 0.2917865435282389 minutes
2023-12-20 15:55:35,903:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:35,903:INFO:Initializing create_model()
2023-12-20 15:55:35,903:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:35,903:INFO:Checking exceptions
2023-12-20 15:55:35,903:INFO:Importing libraries
2023-12-20 15:55:35,903:INFO:Copying training dataset
2023-12-20 15:55:35,907:INFO:Defining folds
2023-12-20 15:55:35,907:INFO:Declaring metric variables
2023-12-20 15:55:35,911:INFO:Importing untrained model
2023-12-20 15:55:35,912:INFO:Extra Trees Classifier Imported successfully
2023-12-20 15:55:35,918:INFO:Starting cross validation
2023-12-20 15:55:35,918:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:36,326:INFO:Calculating mean and std
2023-12-20 15:55:36,327:INFO:Creating metrics dataframe
2023-12-20 15:55:36,330:INFO:Uploading results into container
2023-12-20 15:55:36,330:INFO:Uploading model into container now
2023-12-20 15:55:36,331:INFO:_master_model_container: 12
2023-12-20 15:55:36,331:INFO:_display_container: 2
2023-12-20 15:55:36,331:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=8461, verbose=0, warm_start=False)
2023-12-20 15:55:36,331:INFO:create_model() successfully completed......................................
2023-12-20 15:55:36,410:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:36,410:INFO:Creating metrics dataframe
2023-12-20 15:55:36,418:INFO:Initializing Extreme Gradient Boosting
2023-12-20 15:55:36,418:INFO:Total runtime is 0.30061011711756386 minutes
2023-12-20 15:55:36,418:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:36,418:INFO:Initializing create_model()
2023-12-20 15:55:36,418:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:36,418:INFO:Checking exceptions
2023-12-20 15:55:36,418:INFO:Importing libraries
2023-12-20 15:55:36,418:INFO:Copying training dataset
2023-12-20 15:55:36,418:INFO:Defining folds
2023-12-20 15:55:36,418:INFO:Declaring metric variables
2023-12-20 15:55:36,428:INFO:Importing untrained model
2023-12-20 15:55:36,431:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:55:36,435:INFO:Starting cross validation
2023-12-20 15:55:36,435:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:55:37,036:INFO:Calculating mean and std
2023-12-20 15:55:37,036:INFO:Creating metrics dataframe
2023-12-20 15:55:37,044:INFO:Uploading results into container
2023-12-20 15:55:37,046:INFO:Uploading model into container now
2023-12-20 15:55:37,046:INFO:_master_model_container: 13
2023-12-20 15:55:37,046:INFO:_display_container: 2
2023-12-20 15:55:37,046:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 15:55:37,046:INFO:create_model() successfully completed......................................
2023-12-20 15:55:37,127:INFO:SubProcess create_model() end ==================================
2023-12-20 15:55:37,128:INFO:Creating metrics dataframe
2023-12-20 15:55:37,135:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 15:55:37,135:INFO:Total runtime is 0.3125615080197652 minutes
2023-12-20 15:55:37,145:INFO:SubProcess create_model() called ==================================
2023-12-20 15:55:37,145:INFO:Initializing create_model()
2023-12-20 15:55:37,145:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFF51A9190>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF5623010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:55:37,145:INFO:Checking exceptions
2023-12-20 15:55:37,145:INFO:Importing libraries
2023-12-20 15:55:37,145:INFO:Copying training dataset
2023-12-20 15:55:37,152:INFO:Defining folds
2023-12-20 15:55:37,152:INFO:Declaring metric variables
2023-12-20 15:55:37,152:INFO:Importing untrained model
2023-12-20 15:55:37,152:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:55:37,164:INFO:Starting cross validation
2023-12-20 15:55:37,165:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:56:47,852:INFO:PyCaret ClassificationExperiment
2023-12-20 15:56:47,852:INFO:Logging name: clf-default-name
2023-12-20 15:56:47,852:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 15:56:47,852:INFO:version 3.2.0
2023-12-20 15:56:47,852:INFO:Initializing setup()
2023-12-20 15:56:47,852:INFO:self.USI: 52f1
2023-12-20 15:56:47,852:INFO:self._variable_keys: {'html_param', 'seed', 'logging_param', 'X_train', '_available_plots', 'exp_id', 'USI', 'X_test', 'y_test', 'X', 'fold_generator', 'gpu_n_jobs_param', 'target_param', 'fold_shuffle_param', '_ml_usecase', 'y_train', 'memory', 'fix_imbalance', 'fold_groups_param', 'data', 'exp_name_log', 'log_plots_param', 'n_jobs_param', 'idx', 'pipeline', 'gpu_param', 'y', 'is_multiclass'}
2023-12-20 15:56:47,852:INFO:Checking environment
2023-12-20 15:56:47,852:INFO:python_version: 3.11.5
2023-12-20 15:56:47,852:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 15:56:47,852:INFO:machine: AMD64
2023-12-20 15:56:47,852:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 15:56:47,852:INFO:Memory: svmem(total=16718413824, available=7095476224, percent=57.6, used=9622937600, free=7095476224)
2023-12-20 15:56:47,852:INFO:Physical Core: 12
2023-12-20 15:56:47,852:INFO:Logical Core: 16
2023-12-20 15:56:47,852:INFO:Checking libraries
2023-12-20 15:56:47,852:INFO:System:
2023-12-20 15:56:47,852:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 15:56:47,852:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 15:56:47,852:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 15:56:47,852:INFO:PyCaret required dependencies:
2023-12-20 15:56:47,852:INFO:                 pip: 23.2.1
2023-12-20 15:56:47,852:INFO:          setuptools: 68.0.0
2023-12-20 15:56:47,852:INFO:             pycaret: 3.2.0
2023-12-20 15:56:47,852:INFO:             IPython: 8.15.0
2023-12-20 15:56:47,852:INFO:          ipywidgets: 8.0.4
2023-12-20 15:56:47,852:INFO:                tqdm: 4.65.0
2023-12-20 15:56:47,852:INFO:               numpy: 1.24.3
2023-12-20 15:56:47,852:INFO:              pandas: 1.5.3
2023-12-20 15:56:47,852:INFO:              jinja2: 3.1.2
2023-12-20 15:56:47,852:INFO:               scipy: 1.10.1
2023-12-20 15:56:47,852:INFO:              joblib: 1.2.0
2023-12-20 15:56:47,852:INFO:             sklearn: 1.2.1
2023-12-20 15:56:47,852:INFO:                pyod: 1.1.2
2023-12-20 15:56:47,852:INFO:            imblearn: 0.11.0
2023-12-20 15:56:47,852:INFO:   category_encoders: 2.6.3
2023-12-20 15:56:47,852:INFO:            lightgbm: 4.1.0
2023-12-20 15:56:47,852:INFO:               numba: 0.57.1
2023-12-20 15:56:47,852:INFO:            requests: 2.31.0
2023-12-20 15:56:47,852:INFO:          matplotlib: 3.6.0
2023-12-20 15:56:47,852:INFO:          scikitplot: 0.3.7
2023-12-20 15:56:47,852:INFO:         yellowbrick: 1.5
2023-12-20 15:56:47,852:INFO:              plotly: 5.9.0
2023-12-20 15:56:47,852:INFO:    plotly-resampler: Not installed
2023-12-20 15:56:47,852:INFO:             kaleido: 0.2.1
2023-12-20 15:56:47,852:INFO:           schemdraw: 0.15
2023-12-20 15:56:47,852:INFO:         statsmodels: 0.14.0
2023-12-20 15:56:47,852:INFO:              sktime: 0.21.1
2023-12-20 15:56:47,852:INFO:               tbats: 1.1.3
2023-12-20 15:56:47,852:INFO:            pmdarima: 2.0.4
2023-12-20 15:56:47,852:INFO:              psutil: 5.9.0
2023-12-20 15:56:47,852:INFO:          markupsafe: 2.1.1
2023-12-20 15:56:47,852:INFO:             pickle5: Not installed
2023-12-20 15:56:47,852:INFO:         cloudpickle: 2.2.1
2023-12-20 15:56:47,852:INFO:         deprecation: 2.1.0
2023-12-20 15:56:47,852:INFO:              xxhash: 2.0.2
2023-12-20 15:56:47,852:INFO:           wurlitzer: Not installed
2023-12-20 15:56:47,852:INFO:PyCaret optional dependencies:
2023-12-20 15:56:47,852:INFO:                shap: Not installed
2023-12-20 15:56:47,852:INFO:           interpret: Not installed
2023-12-20 15:56:47,852:INFO:                umap: Not installed
2023-12-20 15:56:47,852:INFO:     ydata_profiling: Not installed
2023-12-20 15:56:47,852:INFO:  explainerdashboard: Not installed
2023-12-20 15:56:47,852:INFO:             autoviz: Not installed
2023-12-20 15:56:47,852:INFO:           fairlearn: Not installed
2023-12-20 15:56:47,852:INFO:          deepchecks: Not installed
2023-12-20 15:56:47,852:INFO:             xgboost: 2.0.2
2023-12-20 15:56:47,852:INFO:            catboost: Not installed
2023-12-20 15:56:47,852:INFO:              kmodes: Not installed
2023-12-20 15:56:47,852:INFO:             mlxtend: Not installed
2023-12-20 15:56:47,852:INFO:       statsforecast: Not installed
2023-12-20 15:56:47,852:INFO:        tune_sklearn: Not installed
2023-12-20 15:56:47,852:INFO:                 ray: Not installed
2023-12-20 15:56:47,852:INFO:            hyperopt: Not installed
2023-12-20 15:56:47,852:INFO:              optuna: Not installed
2023-12-20 15:56:47,852:INFO:               skopt: Not installed
2023-12-20 15:56:47,852:INFO:              mlflow: Not installed
2023-12-20 15:56:47,852:INFO:              gradio: Not installed
2023-12-20 15:56:47,852:INFO:             fastapi: Not installed
2023-12-20 15:56:47,852:INFO:             uvicorn: Not installed
2023-12-20 15:56:47,852:INFO:              m2cgen: Not installed
2023-12-20 15:56:47,852:INFO:           evidently: Not installed
2023-12-20 15:56:47,852:INFO:               fugue: Not installed
2023-12-20 15:56:47,852:INFO:           streamlit: Not installed
2023-12-20 15:56:47,852:INFO:             prophet: Not installed
2023-12-20 15:56:47,852:INFO:None
2023-12-20 15:56:47,852:INFO:Set up data.
2023-12-20 15:56:47,866:INFO:Set up folding strategy.
2023-12-20 15:56:47,866:INFO:Set up train/test split.
2023-12-20 15:56:47,875:INFO:Set up index.
2023-12-20 15:56:47,875:INFO:Assigning column types.
2023-12-20 15:56:47,879:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 15:56:47,918:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 15:56:47,918:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:56:47,935:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:56:47,935:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:56:47,959:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 15:56:47,959:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:56:47,976:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:56:47,976:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:56:47,976:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 15:56:47,993:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:56:48,010:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:56:48,010:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:56:48,043:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 15:56:48,058:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:56:48,060:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:56:48,060:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 15:56:48,101:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:56:48,102:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:56:48,128:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:56:48,144:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:56:48,145:INFO:Preparing preprocessing pipeline...
2023-12-20 15:56:48,147:INFO:Set up simple imputation.
2023-12-20 15:56:48,147:INFO:Set up column name cleaning.
2023-12-20 15:56:48,161:INFO:Finished creating preprocessing pipeline.
2023-12-20 15:56:48,161:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Platelets_c', 'Bilirubin_c',
                                             'Cholesterol_c', 'Albumin_c',
                                             'Copper_c', 'Alk_Pho...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 15:56:48,161:INFO:Creating final display dataframe.
2023-12-20 15:56:48,210:INFO:Setup _display_container:                     Description             Value
0                    Session id               499
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 35)
4        Transformed data shape        (7905, 35)
5   Transformed train set shape        (6324, 35)
6    Transformed test set shape        (1581, 35)
7              Numeric features                34
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              52f1
2023-12-20 15:56:48,262:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:56:48,262:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:56:48,303:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 15:56:48,305:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 15:56:48,306:INFO:setup() successfully completed in 0.46s...............
2023-12-20 15:56:48,318:INFO:Initializing compare_models()
2023-12-20 15:56:48,318:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 15:56:48,318:INFO:Checking exceptions
2023-12-20 15:56:48,322:INFO:Preparing display monitor
2023-12-20 15:56:48,340:INFO:Initializing Logistic Regression
2023-12-20 15:56:48,340:INFO:Total runtime is 0.0 minutes
2023-12-20 15:56:48,344:INFO:SubProcess create_model() called ==================================
2023-12-20 15:56:48,344:INFO:Initializing create_model()
2023-12-20 15:56:48,344:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:56:48,344:INFO:Checking exceptions
2023-12-20 15:56:48,345:INFO:Importing libraries
2023-12-20 15:56:48,345:INFO:Copying training dataset
2023-12-20 15:56:48,349:INFO:Defining folds
2023-12-20 15:56:48,349:INFO:Declaring metric variables
2023-12-20 15:56:48,352:INFO:Importing untrained model
2023-12-20 15:56:48,354:INFO:Logistic Regression Imported successfully
2023-12-20 15:56:48,358:INFO:Starting cross validation
2023-12-20 15:56:48,360:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:56:52,086:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:56:52,134:INFO:Calculating mean and std
2023-12-20 15:56:52,136:INFO:Creating metrics dataframe
2023-12-20 15:56:52,141:INFO:Uploading results into container
2023-12-20 15:56:52,142:INFO:Uploading model into container now
2023-12-20 15:56:52,142:INFO:_master_model_container: 1
2023-12-20 15:56:52,142:INFO:_display_container: 2
2023-12-20 15:56:52,142:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=499, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 15:56:52,142:INFO:create_model() successfully completed......................................
2023-12-20 15:56:52,270:INFO:SubProcess create_model() end ==================================
2023-12-20 15:56:52,270:INFO:Creating metrics dataframe
2023-12-20 15:56:52,270:INFO:Initializing K Neighbors Classifier
2023-12-20 15:56:52,270:INFO:Total runtime is 0.06549795071283976 minutes
2023-12-20 15:56:52,270:INFO:SubProcess create_model() called ==================================
2023-12-20 15:56:52,270:INFO:Initializing create_model()
2023-12-20 15:56:52,270:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:56:52,270:INFO:Checking exceptions
2023-12-20 15:56:52,270:INFO:Importing libraries
2023-12-20 15:56:52,270:INFO:Copying training dataset
2023-12-20 15:56:52,287:INFO:Defining folds
2023-12-20 15:56:52,287:INFO:Declaring metric variables
2023-12-20 15:56:52,289:INFO:Importing untrained model
2023-12-20 15:56:52,291:INFO:K Neighbors Classifier Imported successfully
2023-12-20 15:56:52,291:INFO:Starting cross validation
2023-12-20 15:56:52,291:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:56:55,364:INFO:Calculating mean and std
2023-12-20 15:56:55,364:INFO:Creating metrics dataframe
2023-12-20 15:56:55,364:INFO:Uploading results into container
2023-12-20 15:56:55,364:INFO:Uploading model into container now
2023-12-20 15:56:55,364:INFO:_master_model_container: 2
2023-12-20 15:56:55,364:INFO:_display_container: 2
2023-12-20 15:56:55,380:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 15:56:55,380:INFO:create_model() successfully completed......................................
2023-12-20 15:56:55,501:INFO:SubProcess create_model() end ==================================
2023-12-20 15:56:55,501:INFO:Creating metrics dataframe
2023-12-20 15:56:55,502:INFO:Initializing Naive Bayes
2023-12-20 15:56:55,502:INFO:Total runtime is 0.11936620473861695 minutes
2023-12-20 15:56:55,502:INFO:SubProcess create_model() called ==================================
2023-12-20 15:56:55,502:INFO:Initializing create_model()
2023-12-20 15:56:55,502:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:56:55,502:INFO:Checking exceptions
2023-12-20 15:56:55,502:INFO:Importing libraries
2023-12-20 15:56:55,502:INFO:Copying training dataset
2023-12-20 15:56:55,520:INFO:Defining folds
2023-12-20 15:56:55,520:INFO:Declaring metric variables
2023-12-20 15:56:55,526:INFO:Importing untrained model
2023-12-20 15:56:55,527:INFO:Naive Bayes Imported successfully
2023-12-20 15:56:55,537:INFO:Starting cross validation
2023-12-20 15:56:55,538:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:56:58,410:INFO:Calculating mean and std
2023-12-20 15:56:58,425:INFO:Creating metrics dataframe
2023-12-20 15:56:58,429:INFO:Uploading results into container
2023-12-20 15:56:58,429:INFO:Uploading model into container now
2023-12-20 15:56:58,430:INFO:_master_model_container: 3
2023-12-20 15:56:58,430:INFO:_display_container: 2
2023-12-20 15:56:58,430:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 15:56:58,431:INFO:create_model() successfully completed......................................
2023-12-20 15:56:58,547:INFO:SubProcess create_model() end ==================================
2023-12-20 15:56:58,547:INFO:Creating metrics dataframe
2023-12-20 15:56:58,562:INFO:Initializing Decision Tree Classifier
2023-12-20 15:56:58,562:INFO:Total runtime is 0.17035545508066813 minutes
2023-12-20 15:56:58,568:INFO:SubProcess create_model() called ==================================
2023-12-20 15:56:58,568:INFO:Initializing create_model()
2023-12-20 15:56:58,568:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:56:58,576:INFO:Checking exceptions
2023-12-20 15:56:58,576:INFO:Importing libraries
2023-12-20 15:56:58,577:INFO:Copying training dataset
2023-12-20 15:56:58,600:INFO:Defining folds
2023-12-20 15:56:58,601:INFO:Declaring metric variables
2023-12-20 15:56:58,610:INFO:Importing untrained model
2023-12-20 15:56:58,617:INFO:Decision Tree Classifier Imported successfully
2023-12-20 15:56:58,627:INFO:Starting cross validation
2023-12-20 15:56:58,629:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:00,925:INFO:Calculating mean and std
2023-12-20 15:57:00,925:INFO:Creating metrics dataframe
2023-12-20 15:57:00,927:INFO:Uploading results into container
2023-12-20 15:57:00,929:INFO:Uploading model into container now
2023-12-20 15:57:00,929:INFO:_master_model_container: 4
2023-12-20 15:57:00,929:INFO:_display_container: 2
2023-12-20 15:57:00,929:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=499, splitter='best')
2023-12-20 15:57:00,929:INFO:create_model() successfully completed......................................
2023-12-20 15:57:01,061:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:01,061:INFO:Creating metrics dataframe
2023-12-20 15:57:01,061:INFO:Initializing SVM - Linear Kernel
2023-12-20 15:57:01,061:INFO:Total runtime is 0.21201019684473674 minutes
2023-12-20 15:57:01,061:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:01,061:INFO:Initializing create_model()
2023-12-20 15:57:01,061:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:01,061:INFO:Checking exceptions
2023-12-20 15:57:01,061:INFO:Importing libraries
2023-12-20 15:57:01,061:INFO:Copying training dataset
2023-12-20 15:57:01,076:INFO:Defining folds
2023-12-20 15:57:01,077:INFO:Declaring metric variables
2023-12-20 15:57:01,080:INFO:Importing untrained model
2023-12-20 15:57:01,080:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 15:57:01,080:INFO:Starting cross validation
2023-12-20 15:57:01,080:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:01,183:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:57:01,183:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:57:01,183:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:57:01,183:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:01,183:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:57:01,183:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 15:57:01,183:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:01,192:INFO:Calculating mean and std
2023-12-20 15:57:01,194:INFO:Creating metrics dataframe
2023-12-20 15:57:01,196:INFO:Uploading results into container
2023-12-20 15:57:01,197:INFO:Uploading model into container now
2023-12-20 15:57:01,197:INFO:_master_model_container: 5
2023-12-20 15:57:01,197:INFO:_display_container: 2
2023-12-20 15:57:01,197:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=499, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 15:57:01,197:INFO:create_model() successfully completed......................................
2023-12-20 15:57:01,311:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:01,311:INFO:Creating metrics dataframe
2023-12-20 15:57:01,329:INFO:Initializing Ridge Classifier
2023-12-20 15:57:01,329:INFO:Total runtime is 0.2164738138516744 minutes
2023-12-20 15:57:01,329:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:01,329:INFO:Initializing create_model()
2023-12-20 15:57:01,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:01,329:INFO:Checking exceptions
2023-12-20 15:57:01,329:INFO:Importing libraries
2023-12-20 15:57:01,329:INFO:Copying training dataset
2023-12-20 15:57:01,334:INFO:Defining folds
2023-12-20 15:57:01,334:INFO:Declaring metric variables
2023-12-20 15:57:01,334:INFO:Importing untrained model
2023-12-20 15:57:01,340:INFO:Ridge Classifier Imported successfully
2023-12-20 15:57:01,345:INFO:Starting cross validation
2023-12-20 15:57:01,346:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:01,381:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:57:01,386:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:57:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:57:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:57:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 15:57:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:01,399:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:01,411:INFO:Calculating mean and std
2023-12-20 15:57:01,412:INFO:Creating metrics dataframe
2023-12-20 15:57:01,414:INFO:Uploading results into container
2023-12-20 15:57:01,414:INFO:Uploading model into container now
2023-12-20 15:57:01,415:INFO:_master_model_container: 6
2023-12-20 15:57:01,415:INFO:_display_container: 2
2023-12-20 15:57:01,415:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=499, solver='auto',
                tol=0.0001)
2023-12-20 15:57:01,415:INFO:create_model() successfully completed......................................
2023-12-20 15:57:01,545:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:01,545:INFO:Creating metrics dataframe
2023-12-20 15:57:01,560:INFO:Initializing Random Forest Classifier
2023-12-20 15:57:01,560:INFO:Total runtime is 0.2203328092892965 minutes
2023-12-20 15:57:01,560:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:01,560:INFO:Initializing create_model()
2023-12-20 15:57:01,560:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:01,560:INFO:Checking exceptions
2023-12-20 15:57:01,560:INFO:Importing libraries
2023-12-20 15:57:01,560:INFO:Copying training dataset
2023-12-20 15:57:01,560:INFO:Defining folds
2023-12-20 15:57:01,560:INFO:Declaring metric variables
2023-12-20 15:57:01,570:INFO:Importing untrained model
2023-12-20 15:57:01,571:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:57:01,576:INFO:Starting cross validation
2023-12-20 15:57:01,577:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:02,060:INFO:Calculating mean and std
2023-12-20 15:57:02,060:INFO:Creating metrics dataframe
2023-12-20 15:57:02,060:INFO:Uploading results into container
2023-12-20 15:57:02,060:INFO:Uploading model into container now
2023-12-20 15:57:02,060:INFO:_master_model_container: 7
2023-12-20 15:57:02,060:INFO:_display_container: 2
2023-12-20 15:57:02,060:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False)
2023-12-20 15:57:02,060:INFO:create_model() successfully completed......................................
2023-12-20 15:57:02,196:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:02,196:INFO:Creating metrics dataframe
2023-12-20 15:57:02,196:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 15:57:02,196:INFO:Total runtime is 0.2309295694033305 minutes
2023-12-20 15:57:02,210:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:02,210:INFO:Initializing create_model()
2023-12-20 15:57:02,210:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:02,210:INFO:Checking exceptions
2023-12-20 15:57:02,210:INFO:Importing libraries
2023-12-20 15:57:02,210:INFO:Copying training dataset
2023-12-20 15:57:02,211:INFO:Defining folds
2023-12-20 15:57:02,211:INFO:Declaring metric variables
2023-12-20 15:57:02,220:INFO:Importing untrained model
2023-12-20 15:57:02,231:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 15:57:02,250:INFO:Starting cross validation
2023-12-20 15:57:02,253:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:02,294:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:57:02,300:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:57:02,300:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:57:02,300:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:57:02,300:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 15:57:02,316:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:02,321:INFO:Calculating mean and std
2023-12-20 15:57:02,322:INFO:Creating metrics dataframe
2023-12-20 15:57:02,324:INFO:Uploading results into container
2023-12-20 15:57:02,324:INFO:Uploading model into container now
2023-12-20 15:57:02,324:INFO:_master_model_container: 8
2023-12-20 15:57:02,325:INFO:_display_container: 2
2023-12-20 15:57:02,325:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 15:57:02,325:INFO:create_model() successfully completed......................................
2023-12-20 15:57:02,445:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:02,445:INFO:Creating metrics dataframe
2023-12-20 15:57:02,461:INFO:Initializing Ada Boost Classifier
2023-12-20 15:57:02,461:INFO:Total runtime is 0.23535021146138513 minutes
2023-12-20 15:57:02,461:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:02,461:INFO:Initializing create_model()
2023-12-20 15:57:02,461:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:02,471:INFO:Checking exceptions
2023-12-20 15:57:02,471:INFO:Importing libraries
2023-12-20 15:57:02,471:INFO:Copying training dataset
2023-12-20 15:57:02,475:INFO:Defining folds
2023-12-20 15:57:02,475:INFO:Declaring metric variables
2023-12-20 15:57:02,477:INFO:Importing untrained model
2023-12-20 15:57:02,482:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:57:02,482:INFO:Starting cross validation
2023-12-20 15:57:02,482:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:02,810:INFO:Calculating mean and std
2023-12-20 15:57:02,810:INFO:Creating metrics dataframe
2023-12-20 15:57:02,810:INFO:Uploading results into container
2023-12-20 15:57:02,810:INFO:Uploading model into container now
2023-12-20 15:57:02,810:INFO:_master_model_container: 9
2023-12-20 15:57:02,810:INFO:_display_container: 2
2023-12-20 15:57:02,810:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499)
2023-12-20 15:57:02,810:INFO:create_model() successfully completed......................................
2023-12-20 15:57:02,962:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:02,962:INFO:Creating metrics dataframe
2023-12-20 15:57:02,964:INFO:Initializing Gradient Boosting Classifier
2023-12-20 15:57:02,964:INFO:Total runtime is 0.2437234441439311 minutes
2023-12-20 15:57:02,964:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:02,964:INFO:Initializing create_model()
2023-12-20 15:57:02,964:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:02,964:INFO:Checking exceptions
2023-12-20 15:57:02,964:INFO:Importing libraries
2023-12-20 15:57:02,964:INFO:Copying training dataset
2023-12-20 15:57:02,977:INFO:Defining folds
2023-12-20 15:57:02,977:INFO:Declaring metric variables
2023-12-20 15:57:02,981:INFO:Importing untrained model
2023-12-20 15:57:02,984:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:57:02,984:INFO:Starting cross validation
2023-12-20 15:57:02,984:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:06,194:INFO:Calculating mean and std
2023-12-20 15:57:06,194:INFO:Creating metrics dataframe
2023-12-20 15:57:06,194:INFO:Uploading results into container
2023-12-20 15:57:06,194:INFO:Uploading model into container now
2023-12-20 15:57:06,198:INFO:_master_model_container: 10
2023-12-20 15:57:06,198:INFO:_display_container: 2
2023-12-20 15:57:06,198:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:57:06,198:INFO:create_model() successfully completed......................................
2023-12-20 15:57:06,342:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:06,342:INFO:Creating metrics dataframe
2023-12-20 15:57:06,344:INFO:Initializing Linear Discriminant Analysis
2023-12-20 15:57:06,344:INFO:Total runtime is 0.3000602761904399 minutes
2023-12-20 15:57:06,357:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:06,358:INFO:Initializing create_model()
2023-12-20 15:57:06,358:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:06,358:INFO:Checking exceptions
2023-12-20 15:57:06,358:INFO:Importing libraries
2023-12-20 15:57:06,358:INFO:Copying training dataset
2023-12-20 15:57:06,365:INFO:Defining folds
2023-12-20 15:57:06,365:INFO:Declaring metric variables
2023-12-20 15:57:06,368:INFO:Importing untrained model
2023-12-20 15:57:06,376:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 15:57:06,388:INFO:Starting cross validation
2023-12-20 15:57:06,390:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:06,467:INFO:Calculating mean and std
2023-12-20 15:57:06,468:INFO:Creating metrics dataframe
2023-12-20 15:57:06,470:INFO:Uploading results into container
2023-12-20 15:57:06,470:INFO:Uploading model into container now
2023-12-20 15:57:06,470:INFO:_master_model_container: 11
2023-12-20 15:57:06,470:INFO:_display_container: 2
2023-12-20 15:57:06,471:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 15:57:06,471:INFO:create_model() successfully completed......................................
2023-12-20 15:57:06,593:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:06,593:INFO:Creating metrics dataframe
2023-12-20 15:57:06,593:INFO:Initializing Extra Trees Classifier
2023-12-20 15:57:06,593:INFO:Total runtime is 0.30421869357426967 minutes
2023-12-20 15:57:06,610:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:06,610:INFO:Initializing create_model()
2023-12-20 15:57:06,611:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:06,611:INFO:Checking exceptions
2023-12-20 15:57:06,611:INFO:Importing libraries
2023-12-20 15:57:06,611:INFO:Copying training dataset
2023-12-20 15:57:06,611:INFO:Defining folds
2023-12-20 15:57:06,611:INFO:Declaring metric variables
2023-12-20 15:57:06,611:INFO:Importing untrained model
2023-12-20 15:57:06,619:INFO:Extra Trees Classifier Imported successfully
2023-12-20 15:57:06,620:INFO:Starting cross validation
2023-12-20 15:57:06,620:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:07,010:INFO:Calculating mean and std
2023-12-20 15:57:07,010:INFO:Creating metrics dataframe
2023-12-20 15:57:07,010:INFO:Uploading results into container
2023-12-20 15:57:07,010:INFO:Uploading model into container now
2023-12-20 15:57:07,010:INFO:_master_model_container: 12
2023-12-20 15:57:07,010:INFO:_display_container: 2
2023-12-20 15:57:07,010:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=499, verbose=0, warm_start=False)
2023-12-20 15:57:07,010:INFO:create_model() successfully completed......................................
2023-12-20 15:57:07,142:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:07,142:INFO:Creating metrics dataframe
2023-12-20 15:57:07,144:INFO:Initializing Extreme Gradient Boosting
2023-12-20 15:57:07,144:INFO:Total runtime is 0.3133922735850017 minutes
2023-12-20 15:57:07,144:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:07,144:INFO:Initializing create_model()
2023-12-20 15:57:07,144:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:07,144:INFO:Checking exceptions
2023-12-20 15:57:07,144:INFO:Importing libraries
2023-12-20 15:57:07,144:INFO:Copying training dataset
2023-12-20 15:57:07,156:INFO:Defining folds
2023-12-20 15:57:07,156:INFO:Declaring metric variables
2023-12-20 15:57:07,159:INFO:Importing untrained model
2023-12-20 15:57:07,163:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:57:07,167:INFO:Starting cross validation
2023-12-20 15:57:07,170:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:07,793:INFO:Calculating mean and std
2023-12-20 15:57:07,793:INFO:Creating metrics dataframe
2023-12-20 15:57:07,793:INFO:Uploading results into container
2023-12-20 15:57:07,793:INFO:Uploading model into container now
2023-12-20 15:57:07,793:INFO:_master_model_container: 13
2023-12-20 15:57:07,793:INFO:_display_container: 2
2023-12-20 15:57:07,793:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 15:57:07,793:INFO:create_model() successfully completed......................................
2023-12-20 15:57:07,926:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:07,926:INFO:Creating metrics dataframe
2023-12-20 15:57:07,928:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 15:57:07,928:INFO:Total runtime is 0.3264615933100383 minutes
2023-12-20 15:57:07,928:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:07,928:INFO:Initializing create_model()
2023-12-20 15:57:07,928:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:07,928:INFO:Checking exceptions
2023-12-20 15:57:07,928:INFO:Importing libraries
2023-12-20 15:57:07,928:INFO:Copying training dataset
2023-12-20 15:57:07,942:INFO:Defining folds
2023-12-20 15:57:07,942:INFO:Declaring metric variables
2023-12-20 15:57:07,945:INFO:Importing untrained model
2023-12-20 15:57:07,947:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:57:07,950:INFO:Starting cross validation
2023-12-20 15:57:07,952:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:11,050:INFO:Calculating mean and std
2023-12-20 15:57:11,053:INFO:Creating metrics dataframe
2023-12-20 15:57:11,056:INFO:Uploading results into container
2023-12-20 15:57:11,057:INFO:Uploading model into container now
2023-12-20 15:57:11,057:INFO:_master_model_container: 14
2023-12-20 15:57:11,057:INFO:_display_container: 2
2023-12-20 15:57:11,058:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:57:11,058:INFO:create_model() successfully completed......................................
2023-12-20 15:57:11,210:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:11,210:INFO:Creating metrics dataframe
2023-12-20 15:57:11,215:INFO:Initializing Dummy Classifier
2023-12-20 15:57:11,215:INFO:Total runtime is 0.3812534372011821 minutes
2023-12-20 15:57:11,215:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:11,215:INFO:Initializing create_model()
2023-12-20 15:57:11,215:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD349F10>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:11,215:INFO:Checking exceptions
2023-12-20 15:57:11,215:INFO:Importing libraries
2023-12-20 15:57:11,215:INFO:Copying training dataset
2023-12-20 15:57:11,227:INFO:Defining folds
2023-12-20 15:57:11,227:INFO:Declaring metric variables
2023-12-20 15:57:11,230:INFO:Importing untrained model
2023-12-20 15:57:11,232:INFO:Dummy Classifier Imported successfully
2023-12-20 15:57:11,232:INFO:Starting cross validation
2023-12-20 15:57:11,232:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:11,261:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:11,265:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:11,266:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:11,270:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:11,275:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:57:11,278:INFO:Calculating mean and std
2023-12-20 15:57:11,278:INFO:Creating metrics dataframe
2023-12-20 15:57:11,280:INFO:Uploading results into container
2023-12-20 15:57:11,281:INFO:Uploading model into container now
2023-12-20 15:57:11,281:INFO:_master_model_container: 15
2023-12-20 15:57:11,281:INFO:_display_container: 2
2023-12-20 15:57:11,281:INFO:DummyClassifier(constant=None, random_state=499, strategy='prior')
2023-12-20 15:57:11,281:INFO:create_model() successfully completed......................................
2023-12-20 15:57:11,395:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:11,395:INFO:Creating metrics dataframe
2023-12-20 15:57:11,419:INFO:Initializing create_model()
2023-12-20 15:57:11,419:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:11,419:INFO:Checking exceptions
2023-12-20 15:57:11,421:INFO:Importing libraries
2023-12-20 15:57:11,421:INFO:Copying training dataset
2023-12-20 15:57:11,421:INFO:Defining folds
2023-12-20 15:57:11,421:INFO:Declaring metric variables
2023-12-20 15:57:11,421:INFO:Importing untrained model
2023-12-20 15:57:11,421:INFO:Declaring custom model
2023-12-20 15:57:11,425:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:57:11,425:INFO:Cross validation set to False
2023-12-20 15:57:11,425:INFO:Fitting Model
2023-12-20 15:57:11,448:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000985 seconds.
2023-12-20 15:57:11,448:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 15:57:11,448:INFO:[LightGBM] [Info] Total Bins 1784
2023-12-20 15:57:11,448:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 15:57:11,449:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 15:57:11,449:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 15:57:11,449:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 15:57:11,960:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:57:11,960:INFO:create_model() successfully completed......................................
2023-12-20 15:57:12,094:INFO:Initializing create_model()
2023-12-20 15:57:12,094:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:12,094:INFO:Checking exceptions
2023-12-20 15:57:12,101:INFO:Importing libraries
2023-12-20 15:57:12,101:INFO:Copying training dataset
2023-12-20 15:57:12,103:INFO:Defining folds
2023-12-20 15:57:12,103:INFO:Declaring metric variables
2023-12-20 15:57:12,103:INFO:Importing untrained model
2023-12-20 15:57:12,103:INFO:Declaring custom model
2023-12-20 15:57:12,103:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:57:12,103:INFO:Cross validation set to False
2023-12-20 15:57:12,103:INFO:Fitting Model
2023-12-20 15:57:15,162:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:57:15,162:INFO:create_model() successfully completed......................................
2023-12-20 15:57:15,294:INFO:Initializing create_model()
2023-12-20 15:57:15,294:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:15,294:INFO:Checking exceptions
2023-12-20 15:57:15,296:INFO:Importing libraries
2023-12-20 15:57:15,297:INFO:Copying training dataset
2023-12-20 15:57:15,300:INFO:Defining folds
2023-12-20 15:57:15,300:INFO:Declaring metric variables
2023-12-20 15:57:15,300:INFO:Importing untrained model
2023-12-20 15:57:15,300:INFO:Declaring custom model
2023-12-20 15:57:15,300:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:57:15,300:INFO:Cross validation set to False
2023-12-20 15:57:15,300:INFO:Fitting Model
2023-12-20 15:57:15,510:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False)
2023-12-20 15:57:15,510:INFO:create_model() successfully completed......................................
2023-12-20 15:57:15,643:INFO:Initializing create_model()
2023-12-20 15:57:15,643:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:15,644:INFO:Checking exceptions
2023-12-20 15:57:15,645:INFO:Importing libraries
2023-12-20 15:57:15,645:INFO:Copying training dataset
2023-12-20 15:57:15,647:INFO:Defining folds
2023-12-20 15:57:15,647:INFO:Declaring metric variables
2023-12-20 15:57:15,647:INFO:Importing untrained model
2023-12-20 15:57:15,647:INFO:Declaring custom model
2023-12-20 15:57:15,647:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:57:15,647:INFO:Cross validation set to False
2023-12-20 15:57:15,647:INFO:Fitting Model
2023-12-20 15:57:15,980:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 15:57:15,980:INFO:create_model() successfully completed......................................
2023-12-20 15:57:16,144:INFO:Initializing create_model()
2023-12-20 15:57:16,144:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:16,144:INFO:Checking exceptions
2023-12-20 15:57:16,148:INFO:Importing libraries
2023-12-20 15:57:16,148:INFO:Copying training dataset
2023-12-20 15:57:16,150:INFO:Defining folds
2023-12-20 15:57:16,150:INFO:Declaring metric variables
2023-12-20 15:57:16,150:INFO:Importing untrained model
2023-12-20 15:57:16,150:INFO:Declaring custom model
2023-12-20 15:57:16,150:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:57:16,150:INFO:Cross validation set to False
2023-12-20 15:57:16,150:INFO:Fitting Model
2023-12-20 15:57:16,394:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499)
2023-12-20 15:57:16,394:INFO:create_model() successfully completed......................................
2023-12-20 15:57:16,566:INFO:_master_model_container: 15
2023-12-20 15:57:16,567:INFO:_display_container: 2
2023-12-20 15:57:16,568:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499)]
2023-12-20 15:57:16,570:INFO:compare_models() successfully completed......................................
2023-12-20 15:57:16,614:INFO:Initializing tune_model()
2023-12-20 15:57:16,615:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:57:16,616:INFO:Checking exceptions
2023-12-20 15:57:16,634:INFO:Copying training dataset
2023-12-20 15:57:16,637:INFO:Checking base model
2023-12-20 15:57:16,637:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 15:57:16,640:INFO:Declaring metric variables
2023-12-20 15:57:16,644:INFO:Defining Hyperparameters
2023-12-20 15:57:16,781:INFO:Tuning with n_jobs=-1
2023-12-20 15:57:16,782:INFO:Initializing RandomizedSearchCV
2023-12-20 15:57:37,995:INFO:best_params: {'actual_estimator__reg_lambda': 0.0001, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 256, 'actual_estimator__n_estimators': 130, 'actual_estimator__min_split_gain': 0.7, 'actual_estimator__min_child_samples': 21, 'actual_estimator__learning_rate': 0.05, 'actual_estimator__feature_fraction': 0.6, 'actual_estimator__bagging_freq': 6, 'actual_estimator__bagging_fraction': 0.4}
2023-12-20 15:57:37,995:INFO:Hyperparameter search completed
2023-12-20 15:57:37,995:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:37,995:INFO:Initializing create_model()
2023-12-20 15:57:37,995:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF558AF50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 0.0001, 'reg_alpha': 0.7, 'num_leaves': 256, 'n_estimators': 130, 'min_split_gain': 0.7, 'min_child_samples': 21, 'learning_rate': 0.05, 'feature_fraction': 0.6, 'bagging_freq': 6, 'bagging_fraction': 0.4})
2023-12-20 15:57:37,995:INFO:Checking exceptions
2023-12-20 15:57:37,995:INFO:Importing libraries
2023-12-20 15:57:37,999:INFO:Copying training dataset
2023-12-20 15:57:38,008:INFO:Defining folds
2023-12-20 15:57:38,008:INFO:Declaring metric variables
2023-12-20 15:57:38,012:INFO:Importing untrained model
2023-12-20 15:57:38,012:INFO:Declaring custom model
2023-12-20 15:57:38,014:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:57:38,025:INFO:Starting cross validation
2023-12-20 15:57:38,026:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:41,450:INFO:Calculating mean and std
2023-12-20 15:57:41,450:INFO:Creating metrics dataframe
2023-12-20 15:57:41,450:INFO:Finalizing model
2023-12-20 15:57:41,474:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2023-12-20 15:57:41,475:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2023-12-20 15:57:41,475:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2023-12-20 15:57:41,483:INFO:[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6
2023-12-20 15:57:41,483:INFO:[LightGBM] [Warning] bagging_fraction is set=0.4, subsample=1.0 will be ignored. Current value: bagging_fraction=0.4
2023-12-20 15:57:41,483:INFO:[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6
2023-12-20 15:57:41,484:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002184 seconds.
2023-12-20 15:57:41,484:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 15:57:41,484:INFO:[LightGBM] [Info] Total Bins 1784
2023-12-20 15:57:41,488:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 15:57:41,488:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 15:57:41,488:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 15:57:41,488:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 15:57:41,499:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,500:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,516:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,524:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,530:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,534:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,542:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,545:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,547:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,552:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,555:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,559:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,565:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,568:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,573:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,577:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,579:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,583:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,588:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,591:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,595:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,598:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,600:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,601:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,605:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,611:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,614:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,617:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,621:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,626:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,630:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,632:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,634:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,637:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,641:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,645:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,648:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,650:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,658:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,661:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,665:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,666:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,667:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,674:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,678:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,681:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,683:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,691:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,694:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,695:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,697:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,699:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,700:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,709:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,713:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,725:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,727:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,741:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,743:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,759:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,760:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,773:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,775:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,777:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,784:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,807:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,809:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,821:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,839:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,869:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,949:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:41,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,053:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,054:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,055:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,058:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,059:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,068:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,070:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,071:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,074:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,075:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,081:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,084:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,085:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,087:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,089:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,090:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,091:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,092:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,096:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,099:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,101:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,103:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,104:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,105:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,106:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,107:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,108:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,114:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,117:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,119:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,120:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,122:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,123:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,125:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,133:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,135:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,137:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,138:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,139:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,141:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,142:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,149:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,150:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,152:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,153:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,154:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,155:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,157:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,158:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,167:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,169:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,170:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,171:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,172:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,173:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,175:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,183:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,184:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,186:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,187:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,188:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,189:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,190:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,191:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,192:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,199:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,201:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,202:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,217:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,219:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,225:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 15:57:42,276:INFO:Uploading results into container
2023-12-20 15:57:42,277:INFO:Uploading model into container now
2023-12-20 15:57:42,278:INFO:_master_model_container: 16
2023-12-20 15:57:42,279:INFO:_display_container: 3
2023-12-20 15:57:42,279:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=130, n_jobs=-1, num_leaves=256, objective=None,
               random_state=499, reg_alpha=0.7, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:57:42,279:INFO:create_model() successfully completed......................................
2023-12-20 15:57:42,436:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:42,436:INFO:choose_better activated
2023-12-20 15:57:42,436:INFO:SubProcess create_model() called ==================================
2023-12-20 15:57:42,436:INFO:Initializing create_model()
2023-12-20 15:57:42,436:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:57:42,436:INFO:Checking exceptions
2023-12-20 15:57:42,436:INFO:Importing libraries
2023-12-20 15:57:42,436:INFO:Copying training dataset
2023-12-20 15:57:42,436:INFO:Defining folds
2023-12-20 15:57:42,436:INFO:Declaring metric variables
2023-12-20 15:57:42,436:INFO:Importing untrained model
2023-12-20 15:57:42,436:INFO:Declaring custom model
2023-12-20 15:57:42,436:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 15:57:42,436:INFO:Starting cross validation
2023-12-20 15:57:42,436:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:57:45,649:INFO:Calculating mean and std
2023-12-20 15:57:45,650:INFO:Creating metrics dataframe
2023-12-20 15:57:45,651:INFO:Finalizing model
2023-12-20 15:57:45,676:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001727 seconds.
2023-12-20 15:57:45,682:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 15:57:45,682:INFO:[LightGBM] [Info] Total Bins 1784
2023-12-20 15:57:45,682:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 15:57:45,682:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 15:57:45,683:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 15:57:45,683:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 15:57:46,297:INFO:Uploading results into container
2023-12-20 15:57:46,298:INFO:Uploading model into container now
2023-12-20 15:57:46,299:INFO:_master_model_container: 17
2023-12-20 15:57:46,299:INFO:_display_container: 4
2023-12-20 15:57:46,299:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:57:46,299:INFO:create_model() successfully completed......................................
2023-12-20 15:57:46,464:INFO:SubProcess create_model() end ==================================
2023-12-20 15:57:46,465:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=499, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8238
2023-12-20 15:57:46,465:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=130, n_jobs=-1, num_leaves=256, objective=None,
               random_state=499, reg_alpha=0.7, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8257
2023-12-20 15:57:46,465:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=130, n_jobs=-1, num_leaves=256, objective=None,
               random_state=499, reg_alpha=0.7, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 15:57:46,465:INFO:choose_better completed
2023-12-20 15:57:46,472:INFO:_master_model_container: 17
2023-12-20 15:57:46,472:INFO:_display_container: 3
2023-12-20 15:57:46,472:INFO:LGBMClassifier(bagging_fraction=0.4, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=130, n_jobs=-1, num_leaves=256, objective=None,
               random_state=499, reg_alpha=0.7, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)
2023-12-20 15:57:46,472:INFO:tune_model() successfully completed......................................
2023-12-20 15:57:46,639:INFO:Initializing tune_model()
2023-12-20 15:57:46,639:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:57:46,640:INFO:Checking exceptions
2023-12-20 15:57:46,652:INFO:Copying training dataset
2023-12-20 15:57:46,652:INFO:Checking base model
2023-12-20 15:57:46,652:INFO:Base model : Gradient Boosting Classifier
2023-12-20 15:57:46,652:INFO:Declaring metric variables
2023-12-20 15:57:46,664:INFO:Defining Hyperparameters
2023-12-20 15:57:46,831:INFO:Tuning with n_jobs=-1
2023-12-20 15:57:46,831:INFO:Initializing RandomizedSearchCV
2023-12-20 15:58:05,575:INFO:best_params: {'actual_estimator__subsample': 0.9, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.01, 'actual_estimator__max_features': 'log2', 'actual_estimator__max_depth': 3, 'actual_estimator__learning_rate': 0.15}
2023-12-20 15:58:05,575:INFO:Hyperparameter search completed
2023-12-20 15:58:05,575:INFO:SubProcess create_model() called ==================================
2023-12-20 15:58:05,575:INFO:Initializing create_model()
2023-12-20 15:58:05,575:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFB8DC410>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.9, 'n_estimators': 260, 'min_samples_split': 5, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.01, 'max_features': 'log2', 'max_depth': 3, 'learning_rate': 0.15})
2023-12-20 15:58:05,575:INFO:Checking exceptions
2023-12-20 15:58:05,575:INFO:Importing libraries
2023-12-20 15:58:05,575:INFO:Copying training dataset
2023-12-20 15:58:05,586:INFO:Defining folds
2023-12-20 15:58:05,586:INFO:Declaring metric variables
2023-12-20 15:58:05,586:INFO:Importing untrained model
2023-12-20 15:58:05,586:INFO:Declaring custom model
2023-12-20 15:58:05,591:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:58:05,594:INFO:Starting cross validation
2023-12-20 15:58:05,594:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:58:08,241:INFO:Calculating mean and std
2023-12-20 15:58:08,242:INFO:Creating metrics dataframe
2023-12-20 15:58:08,242:INFO:Finalizing model
2023-12-20 15:58:10,342:INFO:Uploading results into container
2023-12-20 15:58:10,342:INFO:Uploading model into container now
2023-12-20 15:58:10,357:INFO:_master_model_container: 18
2023-12-20 15:58:10,357:INFO:_display_container: 4
2023-12-20 15:58:10,358:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.15, loss='log_loss', max_depth=3,
                           max_features='log2', max_leaf_nodes=None,
                           min_impurity_decrease=0.01, min_samples_leaf=4,
                           min_samples_split=5, min_weight_fraction_leaf=0.0,
                           n_estimators=260, n_iter_no_change=None,
                           random_state=499, subsample=0.9, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:58:10,358:INFO:create_model() successfully completed......................................
2023-12-20 15:58:10,489:INFO:SubProcess create_model() end ==================================
2023-12-20 15:58:10,489:INFO:choose_better activated
2023-12-20 15:58:10,489:INFO:SubProcess create_model() called ==================================
2023-12-20 15:58:10,489:INFO:Initializing create_model()
2023-12-20 15:58:10,489:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:58:10,489:INFO:Checking exceptions
2023-12-20 15:58:10,504:INFO:Importing libraries
2023-12-20 15:58:10,504:INFO:Copying training dataset
2023-12-20 15:58:10,509:INFO:Defining folds
2023-12-20 15:58:10,509:INFO:Declaring metric variables
2023-12-20 15:58:10,509:INFO:Importing untrained model
2023-12-20 15:58:10,509:INFO:Declaring custom model
2023-12-20 15:58:10,510:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 15:58:10,510:INFO:Starting cross validation
2023-12-20 15:58:10,510:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:58:13,975:INFO:Calculating mean and std
2023-12-20 15:58:13,975:INFO:Creating metrics dataframe
2023-12-20 15:58:13,975:INFO:Finalizing model
2023-12-20 15:58:17,192:INFO:Uploading results into container
2023-12-20 15:58:17,192:INFO:Uploading model into container now
2023-12-20 15:58:17,192:INFO:_master_model_container: 19
2023-12-20 15:58:17,192:INFO:_display_container: 5
2023-12-20 15:58:17,192:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:58:17,192:INFO:create_model() successfully completed......................................
2023-12-20 15:58:17,345:INFO:SubProcess create_model() end ==================================
2023-12-20 15:58:17,345:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=499, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8221
2023-12-20 15:58:17,345:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.15, loss='log_loss', max_depth=3,
                           max_features='log2', max_leaf_nodes=None,
                           min_impurity_decrease=0.01, min_samples_leaf=4,
                           min_samples_split=5, min_weight_fraction_leaf=0.0,
                           n_estimators=260, n_iter_no_change=None,
                           random_state=499, subsample=0.9, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8278
2023-12-20 15:58:17,345:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.15, loss='log_loss', max_depth=3,
                           max_features='log2', max_leaf_nodes=None,
                           min_impurity_decrease=0.01, min_samples_leaf=4,
                           min_samples_split=5, min_weight_fraction_leaf=0.0,
                           n_estimators=260, n_iter_no_change=None,
                           random_state=499, subsample=0.9, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2023-12-20 15:58:17,345:INFO:choose_better completed
2023-12-20 15:58:17,360:INFO:_master_model_container: 19
2023-12-20 15:58:17,360:INFO:_display_container: 4
2023-12-20 15:58:17,361:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.15, loss='log_loss', max_depth=3,
                           max_features='log2', max_leaf_nodes=None,
                           min_impurity_decrease=0.01, min_samples_leaf=4,
                           min_samples_split=5, min_weight_fraction_leaf=0.0,
                           n_estimators=260, n_iter_no_change=None,
                           random_state=499, subsample=0.9, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 15:58:17,361:INFO:tune_model() successfully completed......................................
2023-12-20 15:58:17,531:INFO:Initializing tune_model()
2023-12-20 15:58:17,531:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:58:17,531:INFO:Checking exceptions
2023-12-20 15:58:17,548:INFO:Copying training dataset
2023-12-20 15:58:17,552:INFO:Checking base model
2023-12-20 15:58:17,552:INFO:Base model : Random Forest Classifier
2023-12-20 15:58:17,554:INFO:Declaring metric variables
2023-12-20 15:58:17,557:INFO:Defining Hyperparameters
2023-12-20 15:58:17,730:INFO:Tuning with n_jobs=-1
2023-12-20 15:58:17,730:INFO:Initializing RandomizedSearchCV
2023-12-20 15:58:24,522:INFO:best_params: {'actual_estimator__n_estimators': 160, 'actual_estimator__min_samples_split': 9, 'actual_estimator__min_samples_leaf': 4, 'actual_estimator__min_impurity_decrease': 0.001, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 6, 'actual_estimator__criterion': 'entropy', 'actual_estimator__class_weight': {}, 'actual_estimator__bootstrap': False}
2023-12-20 15:58:24,522:INFO:Hyperparameter search completed
2023-12-20 15:58:24,522:INFO:SubProcess create_model() called ==================================
2023-12-20 15:58:24,522:INFO:Initializing create_model()
2023-12-20 15:58:24,522:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFCD8DF50>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 160, 'min_samples_split': 9, 'min_samples_leaf': 4, 'min_impurity_decrease': 0.001, 'max_features': 'sqrt', 'max_depth': 6, 'criterion': 'entropy', 'class_weight': {}, 'bootstrap': False})
2023-12-20 15:58:24,522:INFO:Checking exceptions
2023-12-20 15:58:24,522:INFO:Importing libraries
2023-12-20 15:58:24,522:INFO:Copying training dataset
2023-12-20 15:58:24,534:INFO:Defining folds
2023-12-20 15:58:24,534:INFO:Declaring metric variables
2023-12-20 15:58:24,534:INFO:Importing untrained model
2023-12-20 15:58:24,534:INFO:Declaring custom model
2023-12-20 15:58:24,541:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:58:24,543:INFO:Starting cross validation
2023-12-20 15:58:24,543:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:58:25,110:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:58:25,110:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:58:25,110:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:58:25,110:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:58:25,132:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 15:58:25,132:INFO:Calculating mean and std
2023-12-20 15:58:25,132:INFO:Creating metrics dataframe
2023-12-20 15:58:25,142:INFO:Finalizing model
2023-12-20 15:58:25,399:INFO:Uploading results into container
2023-12-20 15:58:25,400:INFO:Uploading model into container now
2023-12-20 15:58:25,401:INFO:_master_model_container: 20
2023-12-20 15:58:25,401:INFO:_display_container: 5
2023-12-20 15:58:25,401:INFO:RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=6, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.001, min_samples_leaf=4,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       n_estimators=160, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False)
2023-12-20 15:58:25,401:INFO:create_model() successfully completed......................................
2023-12-20 15:58:25,533:INFO:SubProcess create_model() end ==================================
2023-12-20 15:58:25,533:INFO:choose_better activated
2023-12-20 15:58:25,533:INFO:SubProcess create_model() called ==================================
2023-12-20 15:58:25,533:INFO:Initializing create_model()
2023-12-20 15:58:25,533:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:58:25,533:INFO:Checking exceptions
2023-12-20 15:58:25,538:INFO:Importing libraries
2023-12-20 15:58:25,538:INFO:Copying training dataset
2023-12-20 15:58:25,544:INFO:Defining folds
2023-12-20 15:58:25,544:INFO:Declaring metric variables
2023-12-20 15:58:25,544:INFO:Importing untrained model
2023-12-20 15:58:25,544:INFO:Declaring custom model
2023-12-20 15:58:25,544:INFO:Random Forest Classifier Imported successfully
2023-12-20 15:58:25,544:INFO:Starting cross validation
2023-12-20 15:58:25,544:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:58:26,078:INFO:Calculating mean and std
2023-12-20 15:58:26,078:INFO:Creating metrics dataframe
2023-12-20 15:58:26,078:INFO:Finalizing model
2023-12-20 15:58:26,310:INFO:Uploading results into container
2023-12-20 15:58:26,310:INFO:Uploading model into container now
2023-12-20 15:58:26,310:INFO:_master_model_container: 21
2023-12-20 15:58:26,310:INFO:_display_container: 6
2023-12-20 15:58:26,310:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False)
2023-12-20 15:58:26,310:INFO:create_model() successfully completed......................................
2023-12-20 15:58:26,441:INFO:SubProcess create_model() end ==================================
2023-12-20 15:58:26,441:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False) result for Accuracy is 0.82
2023-12-20 15:58:26,457:INFO:RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight={},
                       criterion='entropy', max_depth=6, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.001, min_samples_leaf=4,
                       min_samples_split=9, min_weight_fraction_leaf=0.0,
                       n_estimators=160, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False) result for Accuracy is 0.8123
2023-12-20 15:58:26,457:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False) is best model
2023-12-20 15:58:26,457:INFO:choose_better completed
2023-12-20 15:58:26,457:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-12-20 15:58:26,457:INFO:_master_model_container: 21
2023-12-20 15:58:26,457:INFO:_display_container: 5
2023-12-20 15:58:26,457:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=499, verbose=0, warm_start=False)
2023-12-20 15:58:26,457:INFO:tune_model() successfully completed......................................
2023-12-20 15:58:26,633:INFO:Initializing tune_model()
2023-12-20 15:58:26,635:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:58:26,635:INFO:Checking exceptions
2023-12-20 15:58:26,664:INFO:Copying training dataset
2023-12-20 15:58:26,671:INFO:Checking base model
2023-12-20 15:58:26,671:INFO:Base model : Extreme Gradient Boosting
2023-12-20 15:58:26,677:INFO:Declaring metric variables
2023-12-20 15:58:26,680:INFO:Defining Hyperparameters
2023-12-20 15:58:26,816:INFO:Tuning with n_jobs=-1
2023-12-20 15:58:26,817:INFO:Initializing RandomizedSearchCV
2023-12-20 15:58:42,217:INFO:best_params: {'actual_estimator__subsample': 0.9, 'actual_estimator__scale_pos_weight': 32.9, 'actual_estimator__reg_lambda': 0.0001, 'actual_estimator__reg_alpha': 0.01, 'actual_estimator__n_estimators': 240, 'actual_estimator__min_child_weight': 1, 'actual_estimator__max_depth': 8, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__colsample_bytree': 0.7}
2023-12-20 15:58:42,217:INFO:Hyperparameter search completed
2023-12-20 15:58:42,217:INFO:SubProcess create_model() called ==================================
2023-12-20 15:58:42,217:INFO:Initializing create_model()
2023-12-20 15:58:42,217:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFFD5F3010>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.9, 'scale_pos_weight': 32.9, 'reg_lambda': 0.0001, 'reg_alpha': 0.01, 'n_estimators': 240, 'min_child_weight': 1, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.7})
2023-12-20 15:58:42,217:INFO:Checking exceptions
2023-12-20 15:58:42,217:INFO:Importing libraries
2023-12-20 15:58:42,217:INFO:Copying training dataset
2023-12-20 15:58:42,232:INFO:Defining folds
2023-12-20 15:58:42,232:INFO:Declaring metric variables
2023-12-20 15:58:42,234:INFO:Importing untrained model
2023-12-20 15:58:42,234:INFO:Declaring custom model
2023-12-20 15:58:42,234:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:58:42,244:INFO:Starting cross validation
2023-12-20 15:58:42,244:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:58:45,624:INFO:Calculating mean and std
2023-12-20 15:58:45,625:INFO:Creating metrics dataframe
2023-12-20 15:58:45,631:INFO:Finalizing model
2023-12-20 15:58:46,837:INFO:Uploading results into container
2023-12-20 15:58:46,837:INFO:Uploading model into container now
2023-12-20 15:58:46,839:INFO:_master_model_container: 22
2023-12-20 15:58:46,839:INFO:_display_container: 6
2023-12-20 15:58:46,840:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.1, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=8, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=240, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 15:58:46,840:INFO:create_model() successfully completed......................................
2023-12-20 15:58:47,000:INFO:SubProcess create_model() end ==================================
2023-12-20 15:58:47,000:INFO:choose_better activated
2023-12-20 15:58:47,000:INFO:SubProcess create_model() called ==================================
2023-12-20 15:58:47,000:INFO:Initializing create_model()
2023-12-20 15:58:47,000:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:58:47,000:INFO:Checking exceptions
2023-12-20 15:58:47,000:INFO:Importing libraries
2023-12-20 15:58:47,000:INFO:Copying training dataset
2023-12-20 15:58:47,000:INFO:Defining folds
2023-12-20 15:58:47,000:INFO:Declaring metric variables
2023-12-20 15:58:47,000:INFO:Importing untrained model
2023-12-20 15:58:47,000:INFO:Declaring custom model
2023-12-20 15:58:47,000:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 15:58:47,000:INFO:Starting cross validation
2023-12-20 15:58:47,000:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:58:48,086:INFO:Calculating mean and std
2023-12-20 15:58:48,086:INFO:Creating metrics dataframe
2023-12-20 15:58:48,086:INFO:Finalizing model
2023-12-20 15:58:48,508:INFO:Uploading results into container
2023-12-20 15:58:48,508:INFO:Uploading model into container now
2023-12-20 15:58:48,508:INFO:_master_model_container: 23
2023-12-20 15:58:48,508:INFO:_display_container: 7
2023-12-20 15:58:48,508:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 15:58:48,508:INFO:create_model() successfully completed......................................
2023-12-20 15:58:48,683:INFO:SubProcess create_model() end ==================================
2023-12-20 15:58:48,683:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8166
2023-12-20 15:58:48,683:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.1, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=8, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=240, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) result for Accuracy is 0.8253
2023-12-20 15:58:48,683:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.1, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=8, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=240, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...) is best model
2023-12-20 15:58:48,683:INFO:choose_better completed
2023-12-20 15:58:48,683:INFO:_master_model_container: 23
2023-12-20 15:58:48,683:INFO:_display_container: 6
2023-12-20 15:58:48,683:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=0.7, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=0.1, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=8, max_leaves=None,
              min_child_weight=1, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=240, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 15:58:48,683:INFO:tune_model() successfully completed......................................
2023-12-20 15:58:48,868:INFO:Initializing tune_model()
2023-12-20 15:58:48,868:INFO:tune_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={})
2023-12-20 15:58:48,868:INFO:Checking exceptions
2023-12-20 15:58:48,882:INFO:Copying training dataset
2023-12-20 15:58:48,882:INFO:Checking base model
2023-12-20 15:58:48,882:INFO:Base model : Ada Boost Classifier
2023-12-20 15:58:48,892:INFO:Declaring metric variables
2023-12-20 15:58:48,894:INFO:Defining Hyperparameters
2023-12-20 15:58:49,062:INFO:Tuning with n_jobs=-1
2023-12-20 15:58:49,062:INFO:Initializing RandomizedSearchCV
2023-12-20 15:58:52,941:INFO:best_params: {'actual_estimator__n_estimators': 160, 'actual_estimator__learning_rate': 0.3, 'actual_estimator__algorithm': 'SAMME.R'}
2023-12-20 15:58:52,941:INFO:Hyperparameter search completed
2023-12-20 15:58:52,941:INFO:SubProcess create_model() called ==================================
2023-12-20 15:58:52,941:INFO:Initializing create_model()
2023-12-20 15:58:52,941:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001FFF557FED0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'n_estimators': 160, 'learning_rate': 0.3, 'algorithm': 'SAMME.R'})
2023-12-20 15:58:52,941:INFO:Checking exceptions
2023-12-20 15:58:52,941:INFO:Importing libraries
2023-12-20 15:58:52,941:INFO:Copying training dataset
2023-12-20 15:58:52,948:INFO:Defining folds
2023-12-20 15:58:52,948:INFO:Declaring metric variables
2023-12-20 15:58:52,948:INFO:Importing untrained model
2023-12-20 15:58:52,948:INFO:Declaring custom model
2023-12-20 15:58:52,948:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:58:52,957:INFO:Starting cross validation
2023-12-20 15:58:52,958:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:58:54,110:INFO:Calculating mean and std
2023-12-20 15:58:54,110:INFO:Creating metrics dataframe
2023-12-20 15:58:54,110:INFO:Finalizing model
2023-12-20 15:58:54,982:INFO:Uploading results into container
2023-12-20 15:58:54,982:INFO:Uploading model into container now
2023-12-20 15:58:54,982:INFO:_master_model_container: 24
2023-12-20 15:58:54,982:INFO:_display_container: 7
2023-12-20 15:58:54,983:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=0.3, n_estimators=160,
                   random_state=499)
2023-12-20 15:58:54,983:INFO:create_model() successfully completed......................................
2023-12-20 15:58:55,128:INFO:SubProcess create_model() end ==================================
2023-12-20 15:58:55,128:INFO:choose_better activated
2023-12-20 15:58:55,142:INFO:SubProcess create_model() called ==================================
2023-12-20 15:58:55,143:INFO:Initializing create_model()
2023-12-20 15:58:55,143:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 15:58:55,143:INFO:Checking exceptions
2023-12-20 15:58:55,145:INFO:Importing libraries
2023-12-20 15:58:55,145:INFO:Copying training dataset
2023-12-20 15:58:55,145:INFO:Defining folds
2023-12-20 15:58:55,145:INFO:Declaring metric variables
2023-12-20 15:58:55,145:INFO:Importing untrained model
2023-12-20 15:58:55,145:INFO:Declaring custom model
2023-12-20 15:58:55,145:INFO:Ada Boost Classifier Imported successfully
2023-12-20 15:58:55,145:INFO:Starting cross validation
2023-12-20 15:58:55,145:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 15:58:55,529:INFO:Calculating mean and std
2023-12-20 15:58:55,529:INFO:Creating metrics dataframe
2023-12-20 15:58:55,529:INFO:Finalizing model
2023-12-20 15:58:55,813:INFO:Uploading results into container
2023-12-20 15:58:55,823:INFO:Uploading model into container now
2023-12-20 15:58:55,823:INFO:_master_model_container: 25
2023-12-20 15:58:55,823:INFO:_display_container: 8
2023-12-20 15:58:55,823:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499)
2023-12-20 15:58:55,823:INFO:create_model() successfully completed......................................
2023-12-20 15:58:55,994:INFO:SubProcess create_model() end ==================================
2023-12-20 15:58:55,995:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=499) result for Accuracy is 0.8101
2023-12-20 15:58:55,996:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=0.3, n_estimators=160,
                   random_state=499) result for Accuracy is 0.8191
2023-12-20 15:58:55,996:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=0.3, n_estimators=160,
                   random_state=499) is best model
2023-12-20 15:58:55,996:INFO:choose_better completed
2023-12-20 15:58:55,997:INFO:_master_model_container: 25
2023-12-20 15:58:55,997:INFO:_display_container: 7
2023-12-20 15:58:56,007:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=0.3, n_estimators=160,
                   random_state=499)
2023-12-20 15:58:56,007:INFO:tune_model() successfully completed......................................
2023-12-20 15:58:56,229:INFO:Initializing evaluate_model()
2023-12-20 15:58:56,229:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=130, n_jobs=-1, num_leaves=256, objective=None,
               random_state=499, reg_alpha=0.7, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 15:58:56,247:INFO:Initializing plot_model()
2023-12-20 15:58:56,247:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001FFFCE007D0>, estimator=LGBMClassifier(bagging_fraction=0.4, bagging_freq=6, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.6,
               importance_type='split', learning_rate=0.05, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.7,
               n_estimators=130, n_jobs=-1, num_leaves=256, objective=None,
               random_state=499, reg_alpha=0.7, reg_lambda=0.0001,
               subsample=1.0, subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 15:58:56,247:INFO:Checking exceptions
2023-12-20 15:58:56,251:INFO:Preloading libraries
2023-12-20 15:58:56,286:INFO:Copying training dataset
2023-12-20 15:58:56,286:INFO:Plot type: pipeline
2023-12-20 15:58:56,399:INFO:Visual Rendered Successfully
2023-12-20 15:58:56,545:INFO:plot_model() successfully completed......................................
2023-12-20 16:16:27,710:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 16:16:27,710:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 16:16:27,710:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 16:16:27,710:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 16:16:31,168:INFO:PyCaret ClassificationExperiment
2023-12-20 16:16:31,168:INFO:Logging name: clf-default-name
2023-12-20 16:16:31,170:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 16:16:31,170:INFO:version 3.2.0
2023-12-20 16:16:31,170:INFO:Initializing setup()
2023-12-20 16:16:31,170:INFO:self.USI: 178b
2023-12-20 16:16:31,170:INFO:self._variable_keys: {'_available_plots', 'fold_shuffle_param', 'USI', 'data', 'html_param', 'log_plots_param', 'seed', 'target_param', 'X_test', 'logging_param', 'fold_groups_param', 'exp_name_log', 'pipeline', '_ml_usecase', 'X', 'is_multiclass', 'exp_id', 'y', 'y_test', 'X_train', 'fix_imbalance', 'fold_generator', 'idx', 'memory', 'n_jobs_param', 'gpu_n_jobs_param', 'gpu_param', 'y_train'}
2023-12-20 16:16:31,170:INFO:Checking environment
2023-12-20 16:16:31,170:INFO:python_version: 3.11.5
2023-12-20 16:16:31,171:INFO:python_build: ('main', 'Sep 11 2023 13:26:23')
2023-12-20 16:16:31,171:INFO:machine: AMD64
2023-12-20 16:16:31,171:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 16:16:31,171:INFO:Memory: svmem(total=16718413824, available=8305655808, percent=50.3, used=8412758016, free=8305655808)
2023-12-20 16:16:31,171:INFO:Physical Core: 12
2023-12-20 16:16:31,171:INFO:Logical Core: 16
2023-12-20 16:16:31,171:INFO:Checking libraries
2023-12-20 16:16:31,171:INFO:System:
2023-12-20 16:16:31,171:INFO:    python: 3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]
2023-12-20 16:16:31,171:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 16:16:31,171:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 16:16:31,171:INFO:PyCaret required dependencies:
2023-12-20 16:16:31,736:INFO:                 pip: 23.2.1
2023-12-20 16:16:31,736:INFO:          setuptools: 68.0.0
2023-12-20 16:16:31,736:INFO:             pycaret: 3.2.0
2023-12-20 16:16:31,736:INFO:             IPython: 8.15.0
2023-12-20 16:16:31,736:INFO:          ipywidgets: 8.0.4
2023-12-20 16:16:31,738:INFO:                tqdm: 4.65.0
2023-12-20 16:16:31,738:INFO:               numpy: 1.24.3
2023-12-20 16:16:31,738:INFO:              pandas: 1.5.3
2023-12-20 16:16:31,738:INFO:              jinja2: 3.1.2
2023-12-20 16:16:31,738:INFO:               scipy: 1.10.1
2023-12-20 16:16:31,738:INFO:              joblib: 1.2.0
2023-12-20 16:16:31,738:INFO:             sklearn: 1.2.1
2023-12-20 16:16:31,738:INFO:                pyod: 1.1.2
2023-12-20 16:16:31,738:INFO:            imblearn: 0.11.0
2023-12-20 16:16:31,738:INFO:   category_encoders: 2.6.3
2023-12-20 16:16:31,738:INFO:            lightgbm: 4.1.0
2023-12-20 16:16:31,738:INFO:               numba: 0.57.1
2023-12-20 16:16:31,738:INFO:            requests: 2.31.0
2023-12-20 16:16:31,738:INFO:          matplotlib: 3.6.0
2023-12-20 16:16:31,738:INFO:          scikitplot: 0.3.7
2023-12-20 16:16:31,738:INFO:         yellowbrick: 1.5
2023-12-20 16:16:31,738:INFO:              plotly: 5.9.0
2023-12-20 16:16:31,738:INFO:    plotly-resampler: Not installed
2023-12-20 16:16:31,738:INFO:             kaleido: 0.2.1
2023-12-20 16:16:31,738:INFO:           schemdraw: 0.15
2023-12-20 16:16:31,738:INFO:         statsmodels: 0.14.0
2023-12-20 16:16:31,738:INFO:              sktime: 0.21.1
2023-12-20 16:16:31,738:INFO:               tbats: 1.1.3
2023-12-20 16:16:31,738:INFO:            pmdarima: 2.0.4
2023-12-20 16:16:31,738:INFO:              psutil: 5.9.0
2023-12-20 16:16:31,738:INFO:          markupsafe: 2.1.1
2023-12-20 16:16:31,738:INFO:             pickle5: Not installed
2023-12-20 16:16:31,738:INFO:         cloudpickle: 2.2.1
2023-12-20 16:16:31,738:INFO:         deprecation: 2.1.0
2023-12-20 16:16:31,738:INFO:              xxhash: 2.0.2
2023-12-20 16:16:31,738:INFO:           wurlitzer: Not installed
2023-12-20 16:16:31,738:INFO:PyCaret optional dependencies:
2023-12-20 16:16:31,847:INFO:                shap: Not installed
2023-12-20 16:16:31,847:INFO:           interpret: Not installed
2023-12-20 16:16:31,847:INFO:                umap: Not installed
2023-12-20 16:16:31,847:INFO:     ydata_profiling: Not installed
2023-12-20 16:16:31,847:INFO:  explainerdashboard: Not installed
2023-12-20 16:16:31,847:INFO:             autoviz: Not installed
2023-12-20 16:16:31,847:INFO:           fairlearn: Not installed
2023-12-20 16:16:31,847:INFO:          deepchecks: Not installed
2023-12-20 16:16:31,847:INFO:             xgboost: 2.0.2
2023-12-20 16:16:31,847:INFO:            catboost: Not installed
2023-12-20 16:16:31,847:INFO:              kmodes: Not installed
2023-12-20 16:16:31,847:INFO:             mlxtend: Not installed
2023-12-20 16:16:31,847:INFO:       statsforecast: Not installed
2023-12-20 16:16:31,847:INFO:        tune_sklearn: Not installed
2023-12-20 16:16:31,847:INFO:                 ray: Not installed
2023-12-20 16:16:31,847:INFO:            hyperopt: Not installed
2023-12-20 16:16:31,847:INFO:              optuna: Not installed
2023-12-20 16:16:31,847:INFO:               skopt: Not installed
2023-12-20 16:16:31,847:INFO:              mlflow: Not installed
2023-12-20 16:16:31,847:INFO:              gradio: Not installed
2023-12-20 16:16:31,847:INFO:             fastapi: Not installed
2023-12-20 16:16:31,847:INFO:             uvicorn: Not installed
2023-12-20 16:16:31,847:INFO:              m2cgen: Not installed
2023-12-20 16:16:31,847:INFO:           evidently: Not installed
2023-12-20 16:16:31,847:INFO:               fugue: Not installed
2023-12-20 16:16:31,847:INFO:           streamlit: Not installed
2023-12-20 16:16:31,847:INFO:             prophet: Not installed
2023-12-20 16:16:31,847:INFO:None
2023-12-20 16:16:31,847:INFO:Set up data.
2023-12-20 16:16:31,867:INFO:Set up folding strategy.
2023-12-20 16:16:31,867:INFO:Set up train/test split.
2023-12-20 16:16:31,869:INFO:Set up index.
2023-12-20 16:16:31,869:INFO:Assigning column types.
2023-12-20 16:16:31,869:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 16:16:31,884:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 16:16:31,899:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 16:16:31,916:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 16:16:31,916:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 16:16:31,949:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 16:16:31,950:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 16:16:31,950:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 16:16:31,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 16:16:31,966:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 16:16:31,991:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 16:16:32,007:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 16:16:32,008:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 16:16:32,034:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 16:16:32,048:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 16:16:32,051:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 16:16:32,051:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 16:16:32,086:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 16:16:32,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 16:16:32,132:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 16:16:32,133:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 16:16:32,133:INFO:Preparing preprocessing pipeline...
2023-12-20 16:16:32,133:INFO:Set up simple imputation.
2023-12-20 16:16:32,133:INFO:Set up column name cleaning.
2023-12-20 16:16:32,150:INFO:Finished creating preprocessing pipeline.
2023-12-20 16:16:32,150:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Platelets_c', 'Bilirubin_c',
                                             'Cholesterol_c', 'Albumin_c',
                                             'Copper_c', 'Alk_Pho...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 16:16:32,150:INFO:Creating final display dataframe.
2023-12-20 16:16:32,209:INFO:Setup _display_container:                     Description             Value
0                    Session id              7449
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 35)
4        Transformed data shape        (7905, 35)
5   Transformed train set shape        (6324, 35)
6    Transformed test set shape        (1581, 35)
7              Numeric features                34
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              178b
2023-12-20 16:16:32,254:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 16:16:32,257:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 16:16:32,299:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 16:16:32,301:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 16:16:32,304:INFO:setup() successfully completed in 1.37s...............
2023-12-20 16:16:32,317:INFO:Initializing compare_models()
2023-12-20 16:16:32,317:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2023-12-20 16:16:32,317:INFO:Checking exceptions
2023-12-20 16:16:32,320:INFO:Preparing display monitor
2023-12-20 16:16:32,338:INFO:Initializing Logistic Regression
2023-12-20 16:16:32,338:INFO:Total runtime is 0.0 minutes
2023-12-20 16:16:32,338:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:32,349:INFO:Initializing create_model()
2023-12-20 16:16:32,349:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:32,349:INFO:Checking exceptions
2023-12-20 16:16:32,349:INFO:Importing libraries
2023-12-20 16:16:32,349:INFO:Copying training dataset
2023-12-20 16:16:32,355:INFO:Defining folds
2023-12-20 16:16:32,355:INFO:Declaring metric variables
2023-12-20 16:16:32,358:INFO:Importing untrained model
2023-12-20 16:16:32,362:INFO:Logistic Regression Imported successfully
2023-12-20 16:16:32,370:INFO:Starting cross validation
2023-12-20 16:16:32,371:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:35,900:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:35,950:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:35,950:INFO:Calculating mean and std
2023-12-20 16:16:35,950:INFO:Creating metrics dataframe
2023-12-20 16:16:35,962:INFO:Uploading results into container
2023-12-20 16:16:35,963:INFO:Uploading model into container now
2023-12-20 16:16:35,963:INFO:_master_model_container: 1
2023-12-20 16:16:35,963:INFO:_display_container: 2
2023-12-20 16:16:35,963:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=7449, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 16:16:35,963:INFO:create_model() successfully completed......................................
2023-12-20 16:16:36,040:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:36,040:INFO:Creating metrics dataframe
2023-12-20 16:16:36,055:INFO:Initializing K Neighbors Classifier
2023-12-20 16:16:36,055:INFO:Total runtime is 0.06193430821100871 minutes
2023-12-20 16:16:36,057:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:36,057:INFO:Initializing create_model()
2023-12-20 16:16:36,057:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:36,057:INFO:Checking exceptions
2023-12-20 16:16:36,058:INFO:Importing libraries
2023-12-20 16:16:36,058:INFO:Copying training dataset
2023-12-20 16:16:36,059:INFO:Defining folds
2023-12-20 16:16:36,059:INFO:Declaring metric variables
2023-12-20 16:16:36,066:INFO:Importing untrained model
2023-12-20 16:16:36,067:INFO:K Neighbors Classifier Imported successfully
2023-12-20 16:16:36,067:INFO:Starting cross validation
2023-12-20 16:16:36,067:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:39,051:INFO:Calculating mean and std
2023-12-20 16:16:39,053:INFO:Creating metrics dataframe
2023-12-20 16:16:39,053:INFO:Uploading results into container
2023-12-20 16:16:39,053:INFO:Uploading model into container now
2023-12-20 16:16:39,053:INFO:_master_model_container: 2
2023-12-20 16:16:39,053:INFO:_display_container: 2
2023-12-20 16:16:39,053:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 16:16:39,053:INFO:create_model() successfully completed......................................
2023-12-20 16:16:39,136:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:39,136:INFO:Creating metrics dataframe
2023-12-20 16:16:39,142:INFO:Initializing Naive Bayes
2023-12-20 16:16:39,142:INFO:Total runtime is 0.11339617570241292 minutes
2023-12-20 16:16:39,142:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:39,148:INFO:Initializing create_model()
2023-12-20 16:16:39,148:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:39,148:INFO:Checking exceptions
2023-12-20 16:16:39,148:INFO:Importing libraries
2023-12-20 16:16:39,148:INFO:Copying training dataset
2023-12-20 16:16:39,152:INFO:Defining folds
2023-12-20 16:16:39,152:INFO:Declaring metric variables
2023-12-20 16:16:39,152:INFO:Importing untrained model
2023-12-20 16:16:39,152:INFO:Naive Bayes Imported successfully
2023-12-20 16:16:39,152:INFO:Starting cross validation
2023-12-20 16:16:39,152:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:41,950:INFO:Calculating mean and std
2023-12-20 16:16:41,950:INFO:Creating metrics dataframe
2023-12-20 16:16:41,967:INFO:Uploading results into container
2023-12-20 16:16:41,968:INFO:Uploading model into container now
2023-12-20 16:16:41,968:INFO:_master_model_container: 3
2023-12-20 16:16:41,968:INFO:_display_container: 2
2023-12-20 16:16:41,968:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 16:16:41,968:INFO:create_model() successfully completed......................................
2023-12-20 16:16:42,046:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:42,046:INFO:Creating metrics dataframe
2023-12-20 16:16:42,052:INFO:Initializing Decision Tree Classifier
2023-12-20 16:16:42,052:INFO:Total runtime is 0.16188980340957643 minutes
2023-12-20 16:16:42,065:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:42,065:INFO:Initializing create_model()
2023-12-20 16:16:42,066:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:42,066:INFO:Checking exceptions
2023-12-20 16:16:42,066:INFO:Importing libraries
2023-12-20 16:16:42,067:INFO:Copying training dataset
2023-12-20 16:16:42,073:INFO:Defining folds
2023-12-20 16:16:42,073:INFO:Declaring metric variables
2023-12-20 16:16:42,073:INFO:Importing untrained model
2023-12-20 16:16:42,073:INFO:Decision Tree Classifier Imported successfully
2023-12-20 16:16:42,085:INFO:Starting cross validation
2023-12-20 16:16:42,086:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:44,316:INFO:Calculating mean and std
2023-12-20 16:16:44,316:INFO:Creating metrics dataframe
2023-12-20 16:16:44,316:INFO:Uploading results into container
2023-12-20 16:16:44,316:INFO:Uploading model into container now
2023-12-20 16:16:44,316:INFO:_master_model_container: 4
2023-12-20 16:16:44,316:INFO:_display_container: 2
2023-12-20 16:16:44,316:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=7449, splitter='best')
2023-12-20 16:16:44,316:INFO:create_model() successfully completed......................................
2023-12-20 16:16:44,399:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:44,399:INFO:Creating metrics dataframe
2023-12-20 16:16:44,399:INFO:Initializing SVM - Linear Kernel
2023-12-20 16:16:44,399:INFO:Total runtime is 0.20101530949274699 minutes
2023-12-20 16:16:44,399:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:44,399:INFO:Initializing create_model()
2023-12-20 16:16:44,399:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:44,399:INFO:Checking exceptions
2023-12-20 16:16:44,399:INFO:Importing libraries
2023-12-20 16:16:44,399:INFO:Copying training dataset
2023-12-20 16:16:44,399:INFO:Defining folds
2023-12-20 16:16:44,399:INFO:Declaring metric variables
2023-12-20 16:16:44,415:INFO:Importing untrained model
2023-12-20 16:16:44,415:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 16:16:44,421:INFO:Starting cross validation
2023-12-20 16:16:44,423:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:44,505:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 16:16:44,520:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 16:16:44,520:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 16:16:44,520:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:44,520:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 16:16:44,520:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 16:16:44,520:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:44,520:INFO:Calculating mean and std
2023-12-20 16:16:44,520:INFO:Creating metrics dataframe
2023-12-20 16:16:44,532:INFO:Uploading results into container
2023-12-20 16:16:44,534:INFO:Uploading model into container now
2023-12-20 16:16:44,534:INFO:_master_model_container: 5
2023-12-20 16:16:44,534:INFO:_display_container: 2
2023-12-20 16:16:44,534:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=7449, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 16:16:44,534:INFO:create_model() successfully completed......................................
2023-12-20 16:16:44,602:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:44,602:INFO:Creating metrics dataframe
2023-12-20 16:16:44,616:INFO:Initializing Ridge Classifier
2023-12-20 16:16:44,616:INFO:Total runtime is 0.20463025569915771 minutes
2023-12-20 16:16:44,616:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:44,616:INFO:Initializing create_model()
2023-12-20 16:16:44,616:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:44,616:INFO:Checking exceptions
2023-12-20 16:16:44,616:INFO:Importing libraries
2023-12-20 16:16:44,616:INFO:Copying training dataset
2023-12-20 16:16:44,623:INFO:Defining folds
2023-12-20 16:16:44,623:INFO:Declaring metric variables
2023-12-20 16:16:44,623:INFO:Importing untrained model
2023-12-20 16:16:44,632:INFO:Ridge Classifier Imported successfully
2023-12-20 16:16:44,635:INFO:Starting cross validation
2023-12-20 16:16:44,635:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:44,674:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 16:16:44,678:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:44,685:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 16:16:44,687:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 16:16:44,689:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:44,690:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:44,692:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 16:16:44,692:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
           ~~~~~^^^^^^^^
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\yjg10\anaconda3\Lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 16:16:44,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:44,695:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:44,699:INFO:Calculating mean and std
2023-12-20 16:16:44,700:INFO:Creating metrics dataframe
2023-12-20 16:16:44,702:INFO:Uploading results into container
2023-12-20 16:16:44,703:INFO:Uploading model into container now
2023-12-20 16:16:44,703:INFO:_master_model_container: 6
2023-12-20 16:16:44,703:INFO:_display_container: 2
2023-12-20 16:16:44,703:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=7449, solver='auto',
                tol=0.0001)
2023-12-20 16:16:44,703:INFO:create_model() successfully completed......................................
2023-12-20 16:16:44,769:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:44,769:INFO:Creating metrics dataframe
2023-12-20 16:16:44,784:INFO:Initializing Random Forest Classifier
2023-12-20 16:16:44,784:INFO:Total runtime is 0.20743380387624105 minutes
2023-12-20 16:16:44,784:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:44,784:INFO:Initializing create_model()
2023-12-20 16:16:44,784:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:44,784:INFO:Checking exceptions
2023-12-20 16:16:44,784:INFO:Importing libraries
2023-12-20 16:16:44,784:INFO:Copying training dataset
2023-12-20 16:16:44,784:INFO:Defining folds
2023-12-20 16:16:44,784:INFO:Declaring metric variables
2023-12-20 16:16:44,797:INFO:Importing untrained model
2023-12-20 16:16:44,802:INFO:Random Forest Classifier Imported successfully
2023-12-20 16:16:44,802:INFO:Starting cross validation
2023-12-20 16:16:44,802:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:45,281:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:45,281:INFO:Calculating mean and std
2023-12-20 16:16:45,281:INFO:Creating metrics dataframe
2023-12-20 16:16:45,281:INFO:Uploading results into container
2023-12-20 16:16:45,281:INFO:Uploading model into container now
2023-12-20 16:16:45,281:INFO:_master_model_container: 7
2023-12-20 16:16:45,281:INFO:_display_container: 2
2023-12-20 16:16:45,281:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7449, verbose=0, warm_start=False)
2023-12-20 16:16:45,281:INFO:create_model() successfully completed......................................
2023-12-20 16:16:45,366:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:45,366:INFO:Creating metrics dataframe
2023-12-20 16:16:45,366:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 16:16:45,366:INFO:Total runtime is 0.2171198566754659 minutes
2023-12-20 16:16:45,366:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:45,366:INFO:Initializing create_model()
2023-12-20 16:16:45,366:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:45,366:INFO:Checking exceptions
2023-12-20 16:16:45,366:INFO:Importing libraries
2023-12-20 16:16:45,366:INFO:Copying training dataset
2023-12-20 16:16:45,381:INFO:Defining folds
2023-12-20 16:16:45,381:INFO:Declaring metric variables
2023-12-20 16:16:45,381:INFO:Importing untrained model
2023-12-20 16:16:45,381:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 16:16:45,381:INFO:Starting cross validation
2023-12-20 16:16:45,381:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:45,417:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 16:16:45,420:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 16:16:45,424:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 16:16:45,427:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 16:16:45,427:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 16:16:45,446:INFO:Calculating mean and std
2023-12-20 16:16:45,446:INFO:Creating metrics dataframe
2023-12-20 16:16:45,448:INFO:Uploading results into container
2023-12-20 16:16:45,448:INFO:Uploading model into container now
2023-12-20 16:16:45,449:INFO:_master_model_container: 8
2023-12-20 16:16:45,449:INFO:_display_container: 2
2023-12-20 16:16:45,449:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 16:16:45,449:INFO:create_model() successfully completed......................................
2023-12-20 16:16:45,517:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:45,517:INFO:Creating metrics dataframe
2023-12-20 16:16:45,517:INFO:Initializing Ada Boost Classifier
2023-12-20 16:16:45,517:INFO:Total runtime is 0.21963397661844888 minutes
2023-12-20 16:16:45,532:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:45,532:INFO:Initializing create_model()
2023-12-20 16:16:45,532:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:45,532:INFO:Checking exceptions
2023-12-20 16:16:45,533:INFO:Importing libraries
2023-12-20 16:16:45,533:INFO:Copying training dataset
2023-12-20 16:16:45,538:INFO:Defining folds
2023-12-20 16:16:45,538:INFO:Declaring metric variables
2023-12-20 16:16:45,539:INFO:Importing untrained model
2023-12-20 16:16:45,539:INFO:Ada Boost Classifier Imported successfully
2023-12-20 16:16:45,539:INFO:Starting cross validation
2023-12-20 16:16:45,539:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:45,883:INFO:Calculating mean and std
2023-12-20 16:16:45,883:INFO:Creating metrics dataframe
2023-12-20 16:16:45,883:INFO:Uploading results into container
2023-12-20 16:16:45,883:INFO:Uploading model into container now
2023-12-20 16:16:45,883:INFO:_master_model_container: 9
2023-12-20 16:16:45,883:INFO:_display_container: 2
2023-12-20 16:16:45,883:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7449)
2023-12-20 16:16:45,883:INFO:create_model() successfully completed......................................
2023-12-20 16:16:45,968:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:45,968:INFO:Creating metrics dataframe
2023-12-20 16:16:45,968:INFO:Initializing Gradient Boosting Classifier
2023-12-20 16:16:45,968:INFO:Total runtime is 0.22716070810953776 minutes
2023-12-20 16:16:45,968:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:45,968:INFO:Initializing create_model()
2023-12-20 16:16:45,968:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:45,968:INFO:Checking exceptions
2023-12-20 16:16:45,984:INFO:Importing libraries
2023-12-20 16:16:45,984:INFO:Copying training dataset
2023-12-20 16:16:45,984:INFO:Defining folds
2023-12-20 16:16:45,984:INFO:Declaring metric variables
2023-12-20 16:16:45,990:INFO:Importing untrained model
2023-12-20 16:16:45,990:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 16:16:45,990:INFO:Starting cross validation
2023-12-20 16:16:45,990:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:49,098:INFO:Calculating mean and std
2023-12-20 16:16:49,098:INFO:Creating metrics dataframe
2023-12-20 16:16:49,105:INFO:Uploading results into container
2023-12-20 16:16:49,105:INFO:Uploading model into container now
2023-12-20 16:16:49,105:INFO:_master_model_container: 10
2023-12-20 16:16:49,105:INFO:_display_container: 2
2023-12-20 16:16:49,106:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7449, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 16:16:49,106:INFO:create_model() successfully completed......................................
2023-12-20 16:16:49,166:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:49,166:INFO:Creating metrics dataframe
2023-12-20 16:16:49,181:INFO:Initializing Linear Discriminant Analysis
2023-12-20 16:16:49,181:INFO:Total runtime is 0.28070433537165324 minutes
2023-12-20 16:16:49,181:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:49,181:INFO:Initializing create_model()
2023-12-20 16:16:49,181:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:49,181:INFO:Checking exceptions
2023-12-20 16:16:49,181:INFO:Importing libraries
2023-12-20 16:16:49,181:INFO:Copying training dataset
2023-12-20 16:16:49,181:INFO:Defining folds
2023-12-20 16:16:49,181:INFO:Declaring metric variables
2023-12-20 16:16:49,181:INFO:Importing untrained model
2023-12-20 16:16:49,196:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 16:16:49,201:INFO:Starting cross validation
2023-12-20 16:16:49,201:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:49,263:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:49,266:INFO:Calculating mean and std
2023-12-20 16:16:49,268:INFO:Creating metrics dataframe
2023-12-20 16:16:49,270:INFO:Uploading results into container
2023-12-20 16:16:49,270:INFO:Uploading model into container now
2023-12-20 16:16:49,270:INFO:_master_model_container: 11
2023-12-20 16:16:49,270:INFO:_display_container: 2
2023-12-20 16:16:49,270:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 16:16:49,270:INFO:create_model() successfully completed......................................
2023-12-20 16:16:49,347:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:49,349:INFO:Creating metrics dataframe
2023-12-20 16:16:49,356:INFO:Initializing Extra Trees Classifier
2023-12-20 16:16:49,356:INFO:Total runtime is 0.2836249311765035 minutes
2023-12-20 16:16:49,356:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:49,356:INFO:Initializing create_model()
2023-12-20 16:16:49,356:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:49,356:INFO:Checking exceptions
2023-12-20 16:16:49,356:INFO:Importing libraries
2023-12-20 16:16:49,356:INFO:Copying training dataset
2023-12-20 16:16:49,364:INFO:Defining folds
2023-12-20 16:16:49,364:INFO:Declaring metric variables
2023-12-20 16:16:49,364:INFO:Importing untrained model
2023-12-20 16:16:49,368:INFO:Extra Trees Classifier Imported successfully
2023-12-20 16:16:49,374:INFO:Starting cross validation
2023-12-20 16:16:49,374:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:49,757:INFO:Calculating mean and std
2023-12-20 16:16:49,757:INFO:Creating metrics dataframe
2023-12-20 16:16:49,764:INFO:Uploading results into container
2023-12-20 16:16:49,764:INFO:Uploading model into container now
2023-12-20 16:16:49,766:INFO:_master_model_container: 12
2023-12-20 16:16:49,766:INFO:_display_container: 2
2023-12-20 16:16:49,766:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=7449, verbose=0, warm_start=False)
2023-12-20 16:16:49,766:INFO:create_model() successfully completed......................................
2023-12-20 16:16:49,833:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:49,833:INFO:Creating metrics dataframe
2023-12-20 16:16:49,833:INFO:Initializing Extreme Gradient Boosting
2023-12-20 16:16:49,833:INFO:Total runtime is 0.29157771269480387 minutes
2023-12-20 16:16:49,849:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:49,849:INFO:Initializing create_model()
2023-12-20 16:16:49,849:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:49,849:INFO:Checking exceptions
2023-12-20 16:16:49,849:INFO:Importing libraries
2023-12-20 16:16:49,849:INFO:Copying training dataset
2023-12-20 16:16:49,857:INFO:Defining folds
2023-12-20 16:16:49,857:INFO:Declaring metric variables
2023-12-20 16:16:49,868:INFO:Importing untrained model
2023-12-20 16:16:49,878:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 16:16:49,894:INFO:Starting cross validation
2023-12-20 16:16:49,896:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:50,482:INFO:Calculating mean and std
2023-12-20 16:16:50,482:INFO:Creating metrics dataframe
2023-12-20 16:16:50,482:INFO:Uploading results into container
2023-12-20 16:16:50,482:INFO:Uploading model into container now
2023-12-20 16:16:50,482:INFO:_master_model_container: 13
2023-12-20 16:16:50,482:INFO:_display_container: 2
2023-12-20 16:16:50,482:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 16:16:50,482:INFO:create_model() successfully completed......................................
2023-12-20 16:16:50,558:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:50,558:INFO:Creating metrics dataframe
2023-12-20 16:16:50,566:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 16:16:50,566:INFO:Total runtime is 0.30378671884536745 minutes
2023-12-20 16:16:50,566:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:50,566:INFO:Initializing create_model()
2023-12-20 16:16:50,566:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:50,566:INFO:Checking exceptions
2023-12-20 16:16:50,581:INFO:Importing libraries
2023-12-20 16:16:50,581:INFO:Copying training dataset
2023-12-20 16:16:50,581:INFO:Defining folds
2023-12-20 16:16:50,581:INFO:Declaring metric variables
2023-12-20 16:16:50,581:INFO:Importing untrained model
2023-12-20 16:16:50,581:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 16:16:50,596:INFO:Starting cross validation
2023-12-20 16:16:50,597:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:53,602:INFO:Calculating mean and std
2023-12-20 16:16:53,602:INFO:Creating metrics dataframe
2023-12-20 16:16:53,620:INFO:Uploading results into container
2023-12-20 16:16:53,621:INFO:Uploading model into container now
2023-12-20 16:16:53,621:INFO:_master_model_container: 14
2023-12-20 16:16:53,622:INFO:_display_container: 2
2023-12-20 16:16:53,623:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7449, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 16:16:53,623:INFO:create_model() successfully completed......................................
2023-12-20 16:16:53,733:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:53,733:INFO:Creating metrics dataframe
2023-12-20 16:16:53,733:INFO:Initializing Dummy Classifier
2023-12-20 16:16:53,733:INFO:Total runtime is 0.3565729379653931 minutes
2023-12-20 16:16:53,748:INFO:SubProcess create_model() called ==================================
2023-12-20 16:16:53,748:INFO:Initializing create_model()
2023-12-20 16:16:53,748:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000028B178C3810>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:53,748:INFO:Checking exceptions
2023-12-20 16:16:53,748:INFO:Importing libraries
2023-12-20 16:16:53,748:INFO:Copying training dataset
2023-12-20 16:16:53,748:INFO:Defining folds
2023-12-20 16:16:53,748:INFO:Declaring metric variables
2023-12-20 16:16:53,748:INFO:Importing untrained model
2023-12-20 16:16:53,758:INFO:Dummy Classifier Imported successfully
2023-12-20 16:16:53,762:INFO:Starting cross validation
2023-12-20 16:16:53,762:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 16:16:53,792:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:53,794:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:53,796:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:53,797:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:53,798:WARNING:c:\Users\yjg10\anaconda3\Lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 16:16:53,802:INFO:Calculating mean and std
2023-12-20 16:16:53,802:INFO:Creating metrics dataframe
2023-12-20 16:16:53,804:INFO:Uploading results into container
2023-12-20 16:16:53,804:INFO:Uploading model into container now
2023-12-20 16:16:53,804:INFO:_master_model_container: 15
2023-12-20 16:16:53,805:INFO:_display_container: 2
2023-12-20 16:16:53,805:INFO:DummyClassifier(constant=None, random_state=7449, strategy='prior')
2023-12-20 16:16:53,805:INFO:create_model() successfully completed......................................
2023-12-20 16:16:53,878:INFO:SubProcess create_model() end ==================================
2023-12-20 16:16:53,878:INFO:Creating metrics dataframe
2023-12-20 16:16:53,894:INFO:Initializing create_model()
2023-12-20 16:16:53,894:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7449, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:53,894:INFO:Checking exceptions
2023-12-20 16:16:53,895:INFO:Importing libraries
2023-12-20 16:16:53,895:INFO:Copying training dataset
2023-12-20 16:16:53,899:INFO:Defining folds
2023-12-20 16:16:53,900:INFO:Declaring metric variables
2023-12-20 16:16:53,900:INFO:Importing untrained model
2023-12-20 16:16:53,900:INFO:Declaring custom model
2023-12-20 16:16:53,900:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 16:16:53,901:INFO:Cross validation set to False
2023-12-20 16:16:53,901:INFO:Fitting Model
2023-12-20 16:16:53,924:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000554 seconds.
2023-12-20 16:16:53,924:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-12-20 16:16:53,924:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-12-20 16:16:53,924:INFO:[LightGBM] [Info] Total Bins 1785
2023-12-20 16:16:53,924:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 16:16:53,925:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 16:16:53,925:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 16:16:53,925:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 16:16:54,198:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7449, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 16:16:54,198:INFO:create_model() successfully completed......................................
2023-12-20 16:16:54,312:INFO:Initializing create_model()
2023-12-20 16:16:54,312:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7449, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:54,312:INFO:Checking exceptions
2023-12-20 16:16:54,329:INFO:Importing libraries
2023-12-20 16:16:54,329:INFO:Copying training dataset
2023-12-20 16:16:54,333:INFO:Defining folds
2023-12-20 16:16:54,333:INFO:Declaring metric variables
2023-12-20 16:16:54,334:INFO:Importing untrained model
2023-12-20 16:16:54,334:INFO:Declaring custom model
2023-12-20 16:16:54,334:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 16:16:54,334:INFO:Cross validation set to False
2023-12-20 16:16:54,334:INFO:Fitting Model
2023-12-20 16:16:57,481:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7449, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 16:16:57,481:INFO:create_model() successfully completed......................................
2023-12-20 16:16:57,566:INFO:Initializing create_model()
2023-12-20 16:16:57,566:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:57,566:INFO:Checking exceptions
2023-12-20 16:16:57,566:INFO:Importing libraries
2023-12-20 16:16:57,566:INFO:Copying training dataset
2023-12-20 16:16:57,583:INFO:Defining folds
2023-12-20 16:16:57,583:INFO:Declaring metric variables
2023-12-20 16:16:57,583:INFO:Importing untrained model
2023-12-20 16:16:57,583:INFO:Declaring custom model
2023-12-20 16:16:57,583:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 16:16:57,583:INFO:Cross validation set to False
2023-12-20 16:16:57,583:INFO:Fitting Model
2023-12-20 16:16:57,850:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 16:16:57,850:INFO:create_model() successfully completed......................................
2023-12-20 16:16:57,982:INFO:Initializing create_model()
2023-12-20 16:16:57,982:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7449, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:57,982:INFO:Checking exceptions
2023-12-20 16:16:57,987:INFO:Importing libraries
2023-12-20 16:16:57,987:INFO:Copying training dataset
2023-12-20 16:16:57,988:INFO:Defining folds
2023-12-20 16:16:57,988:INFO:Declaring metric variables
2023-12-20 16:16:57,988:INFO:Importing untrained model
2023-12-20 16:16:57,988:INFO:Declaring custom model
2023-12-20 16:16:57,988:INFO:Random Forest Classifier Imported successfully
2023-12-20 16:16:57,988:INFO:Cross validation set to False
2023-12-20 16:16:57,988:INFO:Fitting Model
2023-12-20 16:16:58,196:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7449, verbose=0, warm_start=False)
2023-12-20 16:16:58,196:INFO:create_model() successfully completed......................................
2023-12-20 16:16:58,286:INFO:Initializing create_model()
2023-12-20 16:16:58,286:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7449), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 16:16:58,286:INFO:Checking exceptions
2023-12-20 16:16:58,286:INFO:Importing libraries
2023-12-20 16:16:58,286:INFO:Copying training dataset
2023-12-20 16:16:58,286:INFO:Defining folds
2023-12-20 16:16:58,286:INFO:Declaring metric variables
2023-12-20 16:16:58,286:INFO:Importing untrained model
2023-12-20 16:16:58,286:INFO:Declaring custom model
2023-12-20 16:16:58,286:INFO:Ada Boost Classifier Imported successfully
2023-12-20 16:16:58,286:INFO:Cross validation set to False
2023-12-20 16:16:58,286:INFO:Fitting Model
2023-12-20 16:16:58,548:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7449)
2023-12-20 16:16:58,548:INFO:create_model() successfully completed......................................
2023-12-20 16:16:58,665:INFO:_master_model_container: 15
2023-12-20 16:16:58,665:INFO:_display_container: 2
2023-12-20 16:16:58,666:INFO:[LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7449, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=7449, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=7449, verbose=0, warm_start=False), AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=7449)]
2023-12-20 16:16:58,667:INFO:compare_models() successfully completed......................................
2023-12-20 16:16:58,871:INFO:Initializing evaluate_model()
2023-12-20 16:16:58,871:INFO:evaluate_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7449, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-12-20 16:16:58,887:INFO:Initializing plot_model()
2023-12-20 16:16:58,888:INFO:plot_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000028B16515290>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=7449, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), plot=pipeline, scale=1, save=False, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), fit_kwargs={}, plot_kwargs=None, groups=None, feature_name=None, label=False, verbose=False, system=True, display=None, display_format=None)
2023-12-20 16:16:58,888:INFO:Checking exceptions
2023-12-20 16:16:58,892:INFO:Preloading libraries
2023-12-20 16:16:58,906:INFO:Copying training dataset
2023-12-20 16:16:58,907:INFO:Plot type: pipeline
2023-12-20 16:16:59,042:INFO:Visual Rendered Successfully
2023-12-20 16:16:59,116:INFO:plot_model() successfully completed......................................
2023-12-20 21:25:47,658:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 21:25:47,669:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 21:25:47,669:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 21:25:47,670:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-12-20 21:55:26,475:INFO:PyCaret ClassificationExperiment
2023-12-20 21:55:26,475:INFO:Logging name: clf-default-name
2023-12-20 21:55:26,475:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2023-12-20 21:55:26,476:INFO:version 3.2.0
2023-12-20 21:55:26,476:INFO:Initializing setup()
2023-12-20 21:55:26,477:INFO:self.USI: 4f17
2023-12-20 21:55:26,477:INFO:self._variable_keys: {'X_train', 'n_jobs_param', 'fold_generator', 'idx', 'fold_groups_param', 'USI', 'y', 'is_multiclass', 'gpu_n_jobs_param', 'html_param', 'memory', 'seed', '_available_plots', 'gpu_param', 'fold_shuffle_param', '_ml_usecase', 'exp_id', 'X_test', 'pipeline', 'exp_name_log', 'y_test', 'fix_imbalance', 'target_param', 'y_train', 'log_plots_param', 'X', 'data', 'logging_param'}
2023-12-20 21:55:26,478:INFO:Checking environment
2023-12-20 21:55:26,478:INFO:python_version: 3.10.9
2023-12-20 21:55:26,478:INFO:python_build: ('main', 'Mar  1 2023 18:18:15')
2023-12-20 21:55:26,478:INFO:machine: AMD64
2023-12-20 21:55:26,478:INFO:platform: Windows-10-10.0.22621-SP0
2023-12-20 21:55:26,480:INFO:Memory: svmem(total=34190274560, available=21477920768, percent=37.2, used=12712353792, free=21477920768)
2023-12-20 21:55:26,480:INFO:Physical Core: 24
2023-12-20 21:55:26,480:INFO:Logical Core: 32
2023-12-20 21:55:26,481:INFO:Checking libraries
2023-12-20 21:55:26,481:INFO:System:
2023-12-20 21:55:26,481:INFO:    python: 3.10.9 | packaged by Anaconda, Inc. | (main, Mar  1 2023, 18:18:15) [MSC v.1916 64 bit (AMD64)]
2023-12-20 21:55:26,482:INFO:executable: c:\Users\yjg10\anaconda3\python.exe
2023-12-20 21:55:26,482:INFO:   machine: Windows-10-10.0.22621-SP0
2023-12-20 21:55:26,482:INFO:PyCaret required dependencies:
2023-12-20 21:55:27,005:INFO:                 pip: 22.3.1
2023-12-20 21:55:27,006:INFO:          setuptools: 65.6.3
2023-12-20 21:55:27,006:INFO:             pycaret: 3.2.0
2023-12-20 21:55:27,007:INFO:             IPython: 8.10.0
2023-12-20 21:55:27,007:INFO:          ipywidgets: 7.6.5
2023-12-20 21:55:27,008:INFO:                tqdm: 4.64.1
2023-12-20 21:55:27,008:INFO:               numpy: 1.23.5
2023-12-20 21:55:27,008:INFO:              pandas: 1.5.3
2023-12-20 21:55:27,009:INFO:              jinja2: 3.1.2
2023-12-20 21:55:27,009:INFO:               scipy: 1.10.1
2023-12-20 21:55:27,009:INFO:              joblib: 1.3.2
2023-12-20 21:55:27,009:INFO:             sklearn: 1.2.1
2023-12-20 21:55:27,009:INFO:                pyod: 1.1.2
2023-12-20 21:55:27,010:INFO:            imblearn: 0.10.1
2023-12-20 21:55:27,010:INFO:   category_encoders: 2.6.3
2023-12-20 21:55:27,010:INFO:            lightgbm: 4.1.0
2023-12-20 21:55:27,011:INFO:               numba: 0.56.4
2023-12-20 21:55:27,011:INFO:            requests: 2.28.1
2023-12-20 21:55:27,011:INFO:          matplotlib: 3.6.0
2023-12-20 21:55:27,011:INFO:          scikitplot: 0.3.7
2023-12-20 21:55:27,012:INFO:         yellowbrick: 1.5
2023-12-20 21:55:27,012:INFO:              plotly: 5.9.0
2023-12-20 21:55:27,012:INFO:    plotly-resampler: Not installed
2023-12-20 21:55:27,012:INFO:             kaleido: 0.2.1
2023-12-20 21:55:27,013:INFO:           schemdraw: 0.15
2023-12-20 21:55:27,013:INFO:         statsmodels: 0.13.5
2023-12-20 21:55:27,013:INFO:              sktime: 0.21.1
2023-12-20 21:55:27,013:INFO:               tbats: 1.1.3
2023-12-20 21:55:27,013:INFO:            pmdarima: 2.0.4
2023-12-20 21:55:27,013:INFO:              psutil: 5.9.0
2023-12-20 21:55:27,014:INFO:          markupsafe: 2.1.1
2023-12-20 21:55:27,014:INFO:             pickle5: Not installed
2023-12-20 21:55:27,014:INFO:         cloudpickle: 2.0.0
2023-12-20 21:55:27,014:INFO:         deprecation: 2.1.0
2023-12-20 21:55:27,015:INFO:              xxhash: 3.4.1
2023-12-20 21:55:27,015:INFO:           wurlitzer: Not installed
2023-12-20 21:55:27,015:INFO:PyCaret optional dependencies:
2023-12-20 21:55:27,105:INFO:                shap: Not installed
2023-12-20 21:55:27,105:INFO:           interpret: Not installed
2023-12-20 21:55:27,105:INFO:                umap: Not installed
2023-12-20 21:55:27,105:INFO:     ydata_profiling: Not installed
2023-12-20 21:55:27,106:INFO:  explainerdashboard: Not installed
2023-12-20 21:55:27,106:INFO:             autoviz: Not installed
2023-12-20 21:55:27,106:INFO:           fairlearn: Not installed
2023-12-20 21:55:27,106:INFO:          deepchecks: Not installed
2023-12-20 21:55:27,107:INFO:             xgboost: 2.0.2
2023-12-20 21:55:27,107:INFO:            catboost: Not installed
2023-12-20 21:55:27,107:INFO:              kmodes: Not installed
2023-12-20 21:55:27,107:INFO:             mlxtend: Not installed
2023-12-20 21:55:27,107:INFO:       statsforecast: Not installed
2023-12-20 21:55:27,108:INFO:        tune_sklearn: Not installed
2023-12-20 21:55:27,108:INFO:                 ray: Not installed
2023-12-20 21:55:27,108:INFO:            hyperopt: Not installed
2023-12-20 21:55:27,108:INFO:              optuna: Not installed
2023-12-20 21:55:27,109:INFO:               skopt: Not installed
2023-12-20 21:55:27,109:INFO:              mlflow: Not installed
2023-12-20 21:55:27,109:INFO:              gradio: Not installed
2023-12-20 21:55:27,109:INFO:             fastapi: Not installed
2023-12-20 21:55:27,110:INFO:             uvicorn: Not installed
2023-12-20 21:55:27,110:INFO:              m2cgen: Not installed
2023-12-20 21:55:27,110:INFO:           evidently: Not installed
2023-12-20 21:55:27,110:INFO:               fugue: Not installed
2023-12-20 21:55:27,110:INFO:           streamlit: Not installed
2023-12-20 21:55:27,110:INFO:             prophet: Not installed
2023-12-20 21:55:27,111:INFO:None
2023-12-20 21:55:27,111:INFO:Set up data.
2023-12-20 21:55:27,168:INFO:Set up folding strategy.
2023-12-20 21:55:27,169:INFO:Set up train/test split.
2023-12-20 21:55:27,174:INFO:Set up index.
2023-12-20 21:55:27,175:INFO:Assigning column types.
2023-12-20 21:55:27,177:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-12-20 21:55:27,197:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 21:55:27,200:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 21:55:27,219:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 21:55:27,221:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 21:55:27,241:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-12-20 21:55:27,243:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 21:55:27,257:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 21:55:27,259:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 21:55:27,260:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-12-20 21:55:27,280:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 21:55:27,295:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 21:55:27,297:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 21:55:27,318:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2023-12-20 21:55:27,333:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 21:55:27,334:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 21:55:27,335:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2023-12-20 21:55:27,369:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 21:55:27,372:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 21:55:27,407:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 21:55:27,409:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 21:55:27,411:INFO:Preparing preprocessing pipeline...
2023-12-20 21:55:27,412:INFO:Set up simple imputation.
2023-12-20 21:55:27,413:INFO:Set up column name cleaning.
2023-12-20 21:55:27,437:INFO:Finished creating preprocessing pipeline.
2023-12-20 21:55:27,441:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\yjg10\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['N_Days', 'Age', 'Bilirubin',
                                             'Cholesterol', 'Albumin', 'Copper',
                                             'Alk_Phos', 'SGOT',
                                             'Tryglicerides', 'Platelets',
                                             'Prothrombin', 'Stage',
                                             'Platelets_c', 'Bilirubin_c',
                                             'Cholesterol_c', 'Albumin_c',
                                             'Copper_c', 'Alk_Pho...
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent',
                                                              verbose='deprecated'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2023-12-20 21:55:27,442:INFO:Creating final display dataframe.
2023-12-20 21:55:27,495:INFO:Setup _display_container:                     Description             Value
0                    Session id              2960
1                        Target            Status
2                   Target type        Multiclass
3           Original data shape        (7905, 35)
4        Transformed data shape        (7905, 35)
5   Transformed train set shape        (6324, 35)
6    Transformed test set shape        (1581, 35)
7              Numeric features                34
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                 5
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              4f17
2023-12-20 21:55:27,533:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 21:55:27,535:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 21:55:27,569:INFO:Soft dependency imported: xgboost: 2.0.2
2023-12-20 21:55:27,572:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-12-20 21:55:27,573:INFO:setup() successfully completed in 1.1s...............
2023-12-20 21:55:27,578:INFO:Initializing compare_models()
2023-12-20 21:55:27,579:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=5, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 5, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2023-12-20 21:55:27,579:INFO:Checking exceptions
2023-12-20 21:55:27,582:INFO:Preparing display monitor
2023-12-20 21:55:27,598:INFO:Initializing Logistic Regression
2023-12-20 21:55:27,598:INFO:Total runtime is 1.6661485036214194e-05 minutes
2023-12-20 21:55:27,599:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:27,599:INFO:Initializing create_model()
2023-12-20 21:55:27,600:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=lr, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:27,600:INFO:Checking exceptions
2023-12-20 21:55:27,600:INFO:Importing libraries
2023-12-20 21:55:27,600:INFO:Copying training dataset
2023-12-20 21:55:27,604:INFO:Defining folds
2023-12-20 21:55:27,604:INFO:Declaring metric variables
2023-12-20 21:55:27,606:INFO:Importing untrained model
2023-12-20 21:55:27,608:INFO:Logistic Regression Imported successfully
2023-12-20 21:55:27,613:INFO:Starting cross validation
2023-12-20 21:55:27,614:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:32,395:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:32,403:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:32,410:INFO:Calculating mean and std
2023-12-20 21:55:32,412:INFO:Creating metrics dataframe
2023-12-20 21:55:32,415:INFO:Uploading results into container
2023-12-20 21:55:32,416:INFO:Uploading model into container now
2023-12-20 21:55:32,416:INFO:_master_model_container: 1
2023-12-20 21:55:32,417:INFO:_display_container: 2
2023-12-20 21:55:32,417:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=2960, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2023-12-20 21:55:32,417:INFO:create_model() successfully completed......................................
2023-12-20 21:55:32,460:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:32,461:INFO:Creating metrics dataframe
2023-12-20 21:55:32,466:INFO:Initializing K Neighbors Classifier
2023-12-20 21:55:32,467:INFO:Total runtime is 0.08116874297459921 minutes
2023-12-20 21:55:32,469:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:32,469:INFO:Initializing create_model()
2023-12-20 21:55:32,469:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=knn, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:32,470:INFO:Checking exceptions
2023-12-20 21:55:32,470:INFO:Importing libraries
2023-12-20 21:55:32,470:INFO:Copying training dataset
2023-12-20 21:55:32,474:INFO:Defining folds
2023-12-20 21:55:32,475:INFO:Declaring metric variables
2023-12-20 21:55:32,476:INFO:Importing untrained model
2023-12-20 21:55:32,479:INFO:K Neighbors Classifier Imported successfully
2023-12-20 21:55:32,483:INFO:Starting cross validation
2023-12-20 21:55:32,484:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:35,464:INFO:Calculating mean and std
2023-12-20 21:55:35,466:INFO:Creating metrics dataframe
2023-12-20 21:55:35,470:INFO:Uploading results into container
2023-12-20 21:55:35,470:INFO:Uploading model into container now
2023-12-20 21:55:35,470:INFO:_master_model_container: 2
2023-12-20 21:55:35,470:INFO:_display_container: 2
2023-12-20 21:55:35,470:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2023-12-20 21:55:35,472:INFO:create_model() successfully completed......................................
2023-12-20 21:55:35,514:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:35,514:INFO:Creating metrics dataframe
2023-12-20 21:55:35,520:INFO:Initializing Naive Bayes
2023-12-20 21:55:35,520:INFO:Total runtime is 0.13206087748209636 minutes
2023-12-20 21:55:35,522:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:35,523:INFO:Initializing create_model()
2023-12-20 21:55:35,523:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=nb, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:35,523:INFO:Checking exceptions
2023-12-20 21:55:35,523:INFO:Importing libraries
2023-12-20 21:55:35,525:INFO:Copying training dataset
2023-12-20 21:55:35,529:INFO:Defining folds
2023-12-20 21:55:35,529:INFO:Declaring metric variables
2023-12-20 21:55:35,531:INFO:Importing untrained model
2023-12-20 21:55:35,533:INFO:Naive Bayes Imported successfully
2023-12-20 21:55:35,537:INFO:Starting cross validation
2023-12-20 21:55:35,538:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:38,358:INFO:Calculating mean and std
2023-12-20 21:55:38,360:INFO:Creating metrics dataframe
2023-12-20 21:55:38,362:INFO:Uploading results into container
2023-12-20 21:55:38,363:INFO:Uploading model into container now
2023-12-20 21:55:38,363:INFO:_master_model_container: 3
2023-12-20 21:55:38,364:INFO:_display_container: 2
2023-12-20 21:55:38,364:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2023-12-20 21:55:38,364:INFO:create_model() successfully completed......................................
2023-12-20 21:55:38,407:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:38,408:INFO:Creating metrics dataframe
2023-12-20 21:55:38,413:INFO:Initializing Decision Tree Classifier
2023-12-20 21:55:38,414:INFO:Total runtime is 0.1802902658780416 minutes
2023-12-20 21:55:38,416:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:38,417:INFO:Initializing create_model()
2023-12-20 21:55:38,417:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=dt, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:38,418:INFO:Checking exceptions
2023-12-20 21:55:38,418:INFO:Importing libraries
2023-12-20 21:55:38,418:INFO:Copying training dataset
2023-12-20 21:55:38,423:INFO:Defining folds
2023-12-20 21:55:38,423:INFO:Declaring metric variables
2023-12-20 21:55:38,426:INFO:Importing untrained model
2023-12-20 21:55:38,428:INFO:Decision Tree Classifier Imported successfully
2023-12-20 21:55:38,431:INFO:Starting cross validation
2023-12-20 21:55:38,432:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:41,270:INFO:Calculating mean and std
2023-12-20 21:55:41,271:INFO:Creating metrics dataframe
2023-12-20 21:55:41,273:INFO:Uploading results into container
2023-12-20 21:55:41,274:INFO:Uploading model into container now
2023-12-20 21:55:41,274:INFO:_master_model_container: 4
2023-12-20 21:55:41,275:INFO:_display_container: 2
2023-12-20 21:55:41,275:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=2960, splitter='best')
2023-12-20 21:55:41,276:INFO:create_model() successfully completed......................................
2023-12-20 21:55:41,315:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:41,316:INFO:Creating metrics dataframe
2023-12-20 21:55:41,322:INFO:Initializing SVM - Linear Kernel
2023-12-20 21:55:41,323:INFO:Total runtime is 0.22876718839009602 minutes
2023-12-20 21:55:41,325:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:41,326:INFO:Initializing create_model()
2023-12-20 21:55:41,326:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=svm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:41,326:INFO:Checking exceptions
2023-12-20 21:55:41,326:INFO:Importing libraries
2023-12-20 21:55:41,327:INFO:Copying training dataset
2023-12-20 21:55:41,330:INFO:Defining folds
2023-12-20 21:55:41,330:INFO:Declaring metric variables
2023-12-20 21:55:41,332:INFO:Importing untrained model
2023-12-20 21:55:41,334:INFO:SVM - Linear Kernel Imported successfully
2023-12-20 21:55:41,338:INFO:Starting cross validation
2023-12-20 21:55:41,338:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:44,099:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 21:55:44,147:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 21:55:44,147:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 21:55:44,151:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:44,151:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:44,160:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 21:55:44,161:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\utils\_available_if.py", line 32, in __get__
    if not self.check(obj):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py", line 1236, in _check_proba
    raise AttributeError(
AttributeError: probability estimates are not available for loss='hinge'

  warnings.warn(

2023-12-20 21:55:44,163:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:44,182:INFO:Calculating mean and std
2023-12-20 21:55:44,183:INFO:Creating metrics dataframe
2023-12-20 21:55:44,185:INFO:Uploading results into container
2023-12-20 21:55:44,186:INFO:Uploading model into container now
2023-12-20 21:55:44,186:INFO:_master_model_container: 5
2023-12-20 21:55:44,187:INFO:_display_container: 2
2023-12-20 21:55:44,187:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=2960, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2023-12-20 21:55:44,187:INFO:create_model() successfully completed......................................
2023-12-20 21:55:44,227:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:44,228:INFO:Creating metrics dataframe
2023-12-20 21:55:44,233:INFO:Initializing Ridge Classifier
2023-12-20 21:55:44,234:INFO:Total runtime is 0.2772972544034322 minutes
2023-12-20 21:55:44,236:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:44,237:INFO:Initializing create_model()
2023-12-20 21:55:44,237:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=ridge, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:44,237:INFO:Checking exceptions
2023-12-20 21:55:44,238:INFO:Importing libraries
2023-12-20 21:55:44,238:INFO:Copying training dataset
2023-12-20 21:55:44,242:INFO:Defining folds
2023-12-20 21:55:44,242:INFO:Declaring metric variables
2023-12-20 21:55:44,244:INFO:Importing untrained model
2023-12-20 21:55:44,246:INFO:Ridge Classifier Imported successfully
2023-12-20 21:55:44,250:INFO:Starting cross validation
2023-12-20 21:55:44,252:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:46,904:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 21:55:46,907:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:46,935:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 21:55:46,938:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:46,940:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 21:55:46,942:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:46,949:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 21:55:46,953:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:46,955:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py:190: FitFailedWarning: Metric 'make_scorer(roc_auc_score, needs_proba=True, error_score=0.0, average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 76, in _cached_call
    return cache[method]
KeyError: 'predict_proba'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\metrics.py", line 182, in _score
    return super()._score(
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 316, in _score
    y_pred = method_caller(clf, "predict_proba", X)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_scorer.py", line 78, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "c:\Users\yjg10\anaconda3\lib\site-packages\pycaret\internal\pipeline.py", line 127, in __getattr__
    return getattr(self._final_estimator, name)
AttributeError: 'RidgeClassifier' object has no attribute 'predict_proba'

  warnings.warn(

2023-12-20 21:55:46,958:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:46,967:INFO:Calculating mean and std
2023-12-20 21:55:46,968:INFO:Creating metrics dataframe
2023-12-20 21:55:46,971:INFO:Uploading results into container
2023-12-20 21:55:46,972:INFO:Uploading model into container now
2023-12-20 21:55:46,972:INFO:_master_model_container: 6
2023-12-20 21:55:46,972:INFO:_display_container: 2
2023-12-20 21:55:46,973:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=2960, solver='auto',
                tol=0.0001)
2023-12-20 21:55:46,973:INFO:create_model() successfully completed......................................
2023-12-20 21:55:47,012:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:47,013:INFO:Creating metrics dataframe
2023-12-20 21:55:47,019:INFO:Initializing Random Forest Classifier
2023-12-20 21:55:47,019:INFO:Total runtime is 0.32370394468307495 minutes
2023-12-20 21:55:47,021:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:47,021:INFO:Initializing create_model()
2023-12-20 21:55:47,022:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=rf, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:47,022:INFO:Checking exceptions
2023-12-20 21:55:47,022:INFO:Importing libraries
2023-12-20 21:55:47,022:INFO:Copying training dataset
2023-12-20 21:55:47,026:INFO:Defining folds
2023-12-20 21:55:47,027:INFO:Declaring metric variables
2023-12-20 21:55:47,028:INFO:Importing untrained model
2023-12-20 21:55:47,032:INFO:Random Forest Classifier Imported successfully
2023-12-20 21:55:47,035:INFO:Starting cross validation
2023-12-20 21:55:47,035:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:49,851:INFO:Calculating mean and std
2023-12-20 21:55:49,852:INFO:Creating metrics dataframe
2023-12-20 21:55:49,855:INFO:Uploading results into container
2023-12-20 21:55:49,856:INFO:Uploading model into container now
2023-12-20 21:55:49,856:INFO:_master_model_container: 7
2023-12-20 21:55:49,857:INFO:_display_container: 2
2023-12-20 21:55:49,857:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2960, verbose=0, warm_start=False)
2023-12-20 21:55:49,858:INFO:create_model() successfully completed......................................
2023-12-20 21:55:49,898:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:49,899:INFO:Creating metrics dataframe
2023-12-20 21:55:49,905:INFO:Initializing Quadratic Discriminant Analysis
2023-12-20 21:55:49,905:INFO:Total runtime is 0.371809987227122 minutes
2023-12-20 21:55:49,907:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:49,908:INFO:Initializing create_model()
2023-12-20 21:55:49,908:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=qda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:49,908:INFO:Checking exceptions
2023-12-20 21:55:49,908:INFO:Importing libraries
2023-12-20 21:55:49,909:INFO:Copying training dataset
2023-12-20 21:55:49,913:INFO:Defining folds
2023-12-20 21:55:49,913:INFO:Declaring metric variables
2023-12-20 21:55:49,915:INFO:Importing untrained model
2023-12-20 21:55:49,918:INFO:Quadratic Discriminant Analysis Imported successfully
2023-12-20 21:55:49,921:INFO:Starting cross validation
2023-12-20 21:55:49,922:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:49,953:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 21:55:49,953:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\discriminant_analysis.py:926: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2023-12-20 21:55:49,975:INFO:Calculating mean and std
2023-12-20 21:55:49,976:INFO:Creating metrics dataframe
2023-12-20 21:55:49,978:INFO:Uploading results into container
2023-12-20 21:55:49,979:INFO:Uploading model into container now
2023-12-20 21:55:49,980:INFO:_master_model_container: 8
2023-12-20 21:55:49,980:INFO:_display_container: 2
2023-12-20 21:55:49,980:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2023-12-20 21:55:49,981:INFO:create_model() successfully completed......................................
2023-12-20 21:55:50,021:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:50,022:INFO:Creating metrics dataframe
2023-12-20 21:55:50,028:INFO:Initializing Ada Boost Classifier
2023-12-20 21:55:50,028:INFO:Total runtime is 0.37386295795440677 minutes
2023-12-20 21:55:50,031:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:50,032:INFO:Initializing create_model()
2023-12-20 21:55:50,032:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=ada, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:50,033:INFO:Checking exceptions
2023-12-20 21:55:50,033:INFO:Importing libraries
2023-12-20 21:55:50,033:INFO:Copying training dataset
2023-12-20 21:55:50,037:INFO:Defining folds
2023-12-20 21:55:50,037:INFO:Declaring metric variables
2023-12-20 21:55:50,040:INFO:Importing untrained model
2023-12-20 21:55:50,043:INFO:Ada Boost Classifier Imported successfully
2023-12-20 21:55:50,046:INFO:Starting cross validation
2023-12-20 21:55:50,048:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:50,285:INFO:Calculating mean and std
2023-12-20 21:55:50,287:INFO:Creating metrics dataframe
2023-12-20 21:55:50,289:INFO:Uploading results into container
2023-12-20 21:55:50,291:INFO:Uploading model into container now
2023-12-20 21:55:50,292:INFO:_master_model_container: 9
2023-12-20 21:55:50,292:INFO:_display_container: 2
2023-12-20 21:55:50,292:INFO:AdaBoostClassifier(algorithm='SAMME.R', base_estimator='deprecated',
                   estimator=None, learning_rate=1.0, n_estimators=50,
                   random_state=2960)
2023-12-20 21:55:50,293:INFO:create_model() successfully completed......................................
2023-12-20 21:55:50,332:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:50,333:INFO:Creating metrics dataframe
2023-12-20 21:55:50,339:INFO:Initializing Gradient Boosting Classifier
2023-12-20 21:55:50,340:INFO:Total runtime is 0.37905989885330205 minutes
2023-12-20 21:55:50,343:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:50,343:INFO:Initializing create_model()
2023-12-20 21:55:50,343:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=gbc, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:50,344:INFO:Checking exceptions
2023-12-20 21:55:50,344:INFO:Importing libraries
2023-12-20 21:55:50,344:INFO:Copying training dataset
2023-12-20 21:55:50,348:INFO:Defining folds
2023-12-20 21:55:50,348:INFO:Declaring metric variables
2023-12-20 21:55:50,350:INFO:Importing untrained model
2023-12-20 21:55:50,352:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 21:55:50,356:INFO:Starting cross validation
2023-12-20 21:55:50,357:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:52,443:INFO:Calculating mean and std
2023-12-20 21:55:52,444:INFO:Creating metrics dataframe
2023-12-20 21:55:52,447:INFO:Uploading results into container
2023-12-20 21:55:52,448:INFO:Uploading model into container now
2023-12-20 21:55:52,448:INFO:_master_model_container: 10
2023-12-20 21:55:52,449:INFO:_display_container: 2
2023-12-20 21:55:52,449:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 21:55:52,449:INFO:create_model() successfully completed......................................
2023-12-20 21:55:52,489:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:52,490:INFO:Creating metrics dataframe
2023-12-20 21:55:52,497:INFO:Initializing Linear Discriminant Analysis
2023-12-20 21:55:52,498:INFO:Total runtime is 0.415029791990916 minutes
2023-12-20 21:55:52,500:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:52,501:INFO:Initializing create_model()
2023-12-20 21:55:52,501:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=lda, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:52,501:INFO:Checking exceptions
2023-12-20 21:55:52,502:INFO:Importing libraries
2023-12-20 21:55:52,502:INFO:Copying training dataset
2023-12-20 21:55:52,505:INFO:Defining folds
2023-12-20 21:55:52,506:INFO:Declaring metric variables
2023-12-20 21:55:52,508:INFO:Importing untrained model
2023-12-20 21:55:52,510:INFO:Linear Discriminant Analysis Imported successfully
2023-12-20 21:55:52,513:INFO:Starting cross validation
2023-12-20 21:55:52,514:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:52,568:INFO:Calculating mean and std
2023-12-20 21:55:52,568:INFO:Creating metrics dataframe
2023-12-20 21:55:52,571:INFO:Uploading results into container
2023-12-20 21:55:52,572:INFO:Uploading model into container now
2023-12-20 21:55:52,573:INFO:_master_model_container: 11
2023-12-20 21:55:52,573:INFO:_display_container: 2
2023-12-20 21:55:52,573:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2023-12-20 21:55:52,574:INFO:create_model() successfully completed......................................
2023-12-20 21:55:52,614:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:52,615:INFO:Creating metrics dataframe
2023-12-20 21:55:52,621:INFO:Initializing Extra Trees Classifier
2023-12-20 21:55:52,621:INFO:Total runtime is 0.4170791467030844 minutes
2023-12-20 21:55:52,623:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:52,623:INFO:Initializing create_model()
2023-12-20 21:55:52,623:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=et, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:52,624:INFO:Checking exceptions
2023-12-20 21:55:52,624:INFO:Importing libraries
2023-12-20 21:55:52,624:INFO:Copying training dataset
2023-12-20 21:55:52,627:INFO:Defining folds
2023-12-20 21:55:52,628:INFO:Declaring metric variables
2023-12-20 21:55:52,630:INFO:Importing untrained model
2023-12-20 21:55:52,632:INFO:Extra Trees Classifier Imported successfully
2023-12-20 21:55:52,635:INFO:Starting cross validation
2023-12-20 21:55:52,636:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:52,865:INFO:Calculating mean and std
2023-12-20 21:55:52,866:INFO:Creating metrics dataframe
2023-12-20 21:55:52,868:INFO:Uploading results into container
2023-12-20 21:55:52,868:INFO:Uploading model into container now
2023-12-20 21:55:52,870:INFO:_master_model_container: 12
2023-12-20 21:55:52,870:INFO:_display_container: 2
2023-12-20 21:55:52,871:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=2960, verbose=0, warm_start=False)
2023-12-20 21:55:52,871:INFO:create_model() successfully completed......................................
2023-12-20 21:55:52,911:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:52,912:INFO:Creating metrics dataframe
2023-12-20 21:55:52,920:INFO:Initializing Extreme Gradient Boosting
2023-12-20 21:55:52,921:INFO:Total runtime is 0.4220676024754843 minutes
2023-12-20 21:55:52,923:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:52,923:INFO:Initializing create_model()
2023-12-20 21:55:52,923:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=xgboost, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:52,924:INFO:Checking exceptions
2023-12-20 21:55:52,924:INFO:Importing libraries
2023-12-20 21:55:52,924:INFO:Copying training dataset
2023-12-20 21:55:52,928:INFO:Defining folds
2023-12-20 21:55:52,928:INFO:Declaring metric variables
2023-12-20 21:55:52,930:INFO:Importing untrained model
2023-12-20 21:55:52,933:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 21:55:52,937:INFO:Starting cross validation
2023-12-20 21:55:52,938:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:53,347:INFO:Calculating mean and std
2023-12-20 21:55:53,348:INFO:Creating metrics dataframe
2023-12-20 21:55:53,350:INFO:Uploading results into container
2023-12-20 21:55:53,350:INFO:Uploading model into container now
2023-12-20 21:55:53,350:INFO:_master_model_container: 13
2023-12-20 21:55:53,351:INFO:_display_container: 2
2023-12-20 21:55:53,351:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...)
2023-12-20 21:55:53,352:INFO:create_model() successfully completed......................................
2023-12-20 21:55:53,390:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:53,391:INFO:Creating metrics dataframe
2023-12-20 21:55:53,397:INFO:Initializing Light Gradient Boosting Machine
2023-12-20 21:55:53,398:INFO:Total runtime is 0.43003007173538216 minutes
2023-12-20 21:55:53,400:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:53,401:INFO:Initializing create_model()
2023-12-20 21:55:53,401:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:53,401:INFO:Checking exceptions
2023-12-20 21:55:53,401:INFO:Importing libraries
2023-12-20 21:55:53,402:INFO:Copying training dataset
2023-12-20 21:55:53,405:INFO:Defining folds
2023-12-20 21:55:53,405:INFO:Declaring metric variables
2023-12-20 21:55:53,407:INFO:Importing untrained model
2023-12-20 21:55:53,409:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 21:55:53,412:INFO:Starting cross validation
2023-12-20 21:55:53,414:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:55,926:INFO:Calculating mean and std
2023-12-20 21:55:55,927:INFO:Creating metrics dataframe
2023-12-20 21:55:55,931:INFO:Uploading results into container
2023-12-20 21:55:55,932:INFO:Uploading model into container now
2023-12-20 21:55:55,933:INFO:_master_model_container: 14
2023-12-20 21:55:55,933:INFO:_display_container: 2
2023-12-20 21:55:55,934:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 21:55:55,934:INFO:create_model() successfully completed......................................
2023-12-20 21:55:56,007:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:56,008:INFO:Creating metrics dataframe
2023-12-20 21:55:56,016:INFO:Initializing Dummy Classifier
2023-12-20 21:55:56,017:INFO:Total runtime is 0.47365578015645354 minutes
2023-12-20 21:55:56,018:INFO:SubProcess create_model() called ==================================
2023-12-20 21:55:56,019:INFO:Initializing create_model()
2023-12-20 21:55:56,020:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=dummy, fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197436731F0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:56,020:INFO:Checking exceptions
2023-12-20 21:55:56,020:INFO:Importing libraries
2023-12-20 21:55:56,020:INFO:Copying training dataset
2023-12-20 21:55:56,025:INFO:Defining folds
2023-12-20 21:55:56,026:INFO:Declaring metric variables
2023-12-20 21:55:56,027:INFO:Importing untrained model
2023-12-20 21:55:56,030:INFO:Dummy Classifier Imported successfully
2023-12-20 21:55:56,034:INFO:Starting cross validation
2023-12-20 21:55:56,036:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:55:56,053:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:56,054:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:56,055:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:56,055:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:56,058:WARNING:c:\Users\yjg10\anaconda3\lib\site-packages\sklearn\metrics\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))

2023-12-20 21:55:56,069:INFO:Calculating mean and std
2023-12-20 21:55:56,069:INFO:Creating metrics dataframe
2023-12-20 21:55:56,071:INFO:Uploading results into container
2023-12-20 21:55:56,072:INFO:Uploading model into container now
2023-12-20 21:55:56,072:INFO:_master_model_container: 15
2023-12-20 21:55:56,072:INFO:_display_container: 2
2023-12-20 21:55:56,072:INFO:DummyClassifier(constant=None, random_state=2960, strategy='prior')
2023-12-20 21:55:56,073:INFO:create_model() successfully completed......................................
2023-12-20 21:55:56,112:INFO:SubProcess create_model() end ==================================
2023-12-20 21:55:56,113:INFO:Creating metrics dataframe
2023-12-20 21:55:56,124:INFO:Initializing create_model()
2023-12-20 21:55:56,126:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:56,126:INFO:Checking exceptions
2023-12-20 21:55:56,127:INFO:Importing libraries
2023-12-20 21:55:56,127:INFO:Copying training dataset
2023-12-20 21:55:56,130:INFO:Defining folds
2023-12-20 21:55:56,130:INFO:Declaring metric variables
2023-12-20 21:55:56,131:INFO:Importing untrained model
2023-12-20 21:55:56,131:INFO:Declaring custom model
2023-12-20 21:55:56,132:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 21:55:56,132:INFO:Cross validation set to False
2023-12-20 21:55:56,133:INFO:Fitting Model
2023-12-20 21:55:58,604:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 21:55:58,605:INFO:create_model() successfully completed......................................
2023-12-20 21:55:58,647:INFO:Initializing create_model()
2023-12-20 21:55:58,647:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:58,648:INFO:Checking exceptions
2023-12-20 21:55:58,649:INFO:Importing libraries
2023-12-20 21:55:58,650:INFO:Copying training dataset
2023-12-20 21:55:58,654:INFO:Defining folds
2023-12-20 21:55:58,655:INFO:Declaring metric variables
2023-12-20 21:55:58,655:INFO:Importing untrained model
2023-12-20 21:55:58,655:INFO:Declaring custom model
2023-12-20 21:55:58,656:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 21:55:58,656:INFO:Cross validation set to False
2023-12-20 21:55:58,657:INFO:Fitting Model
2023-12-20 21:55:58,673:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001001 seconds.
2023-12-20 21:55:58,673:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 21:55:58,674:INFO:[LightGBM] [Info] Total Bins 1790
2023-12-20 21:55:58,674:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 21:55:58,675:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 21:55:58,675:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 21:55:58,675:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 21:55:58,974:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 21:55:58,975:INFO:create_model() successfully completed......................................
2023-12-20 21:55:59,052:INFO:Initializing create_model()
2023-12-20 21:55:59,053:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='binary:logistic', ...), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:59,053:INFO:Checking exceptions
2023-12-20 21:55:59,055:INFO:Importing libraries
2023-12-20 21:55:59,056:INFO:Copying training dataset
2023-12-20 21:55:59,062:INFO:Defining folds
2023-12-20 21:55:59,063:INFO:Declaring metric variables
2023-12-20 21:55:59,064:INFO:Importing untrained model
2023-12-20 21:55:59,064:INFO:Declaring custom model
2023-12-20 21:55:59,066:INFO:Extreme Gradient Boosting Imported successfully
2023-12-20 21:55:59,067:INFO:Cross validation set to False
2023-12-20 21:55:59,067:INFO:Fitting Model
2023-12-20 21:55:59,306:INFO:XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...)
2023-12-20 21:55:59,307:INFO:create_model() successfully completed......................................
2023-12-20 21:55:59,382:INFO:Initializing create_model()
2023-12-20 21:55:59,383:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2960, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:59,384:INFO:Checking exceptions
2023-12-20 21:55:59,386:INFO:Importing libraries
2023-12-20 21:55:59,386:INFO:Copying training dataset
2023-12-20 21:55:59,392:INFO:Defining folds
2023-12-20 21:55:59,393:INFO:Declaring metric variables
2023-12-20 21:55:59,393:INFO:Importing untrained model
2023-12-20 21:55:59,393:INFO:Declaring custom model
2023-12-20 21:55:59,395:INFO:Random Forest Classifier Imported successfully
2023-12-20 21:55:59,396:INFO:Cross validation set to False
2023-12-20 21:55:59,396:INFO:Fitting Model
2023-12-20 21:55:59,554:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2960, verbose=0, warm_start=False)
2023-12-20 21:55:59,555:INFO:create_model() successfully completed......................................
2023-12-20 21:55:59,604:INFO:Initializing create_model()
2023-12-20 21:55:59,605:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=2960, verbose=0, warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:55:59,605:INFO:Checking exceptions
2023-12-20 21:55:59,606:INFO:Importing libraries
2023-12-20 21:55:59,607:INFO:Copying training dataset
2023-12-20 21:55:59,611:INFO:Defining folds
2023-12-20 21:55:59,612:INFO:Declaring metric variables
2023-12-20 21:55:59,613:INFO:Importing untrained model
2023-12-20 21:55:59,613:INFO:Declaring custom model
2023-12-20 21:55:59,614:INFO:Extra Trees Classifier Imported successfully
2023-12-20 21:55:59,615:INFO:Cross validation set to False
2023-12-20 21:55:59,615:INFO:Fitting Model
2023-12-20 21:55:59,727:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=2960, verbose=0, warm_start=False)
2023-12-20 21:55:59,729:INFO:create_model() successfully completed......................................
2023-12-20 21:55:59,783:INFO:_master_model_container: 15
2023-12-20 21:55:59,784:INFO:_display_container: 2
2023-12-20 21:55:59,786:INFO:[GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       n_estimators=100, n_jobs=-1, oob_score=False,
                       random_state=2960, verbose=0, warm_start=False), ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     n_estimators=100, n_jobs=-1, oob_score=False,
                     random_state=2960, verbose=0, warm_start=False)]
2023-12-20 21:55:59,787:INFO:compare_models() successfully completed......................................
2023-12-20 21:55:59,836:INFO:Initializing tune_model()
2023-12-20 21:55:59,837:INFO:tune_model(estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>)
2023-12-20 21:55:59,837:INFO:Checking exceptions
2023-12-20 21:55:59,851:INFO:Copying training dataset
2023-12-20 21:55:59,854:INFO:Checking base model
2023-12-20 21:55:59,855:INFO:Base model : Gradient Boosting Classifier
2023-12-20 21:55:59,857:INFO:Declaring metric variables
2023-12-20 21:55:59,859:INFO:Defining Hyperparameters
2023-12-20 21:55:59,901:INFO:Tuning with n_jobs=-1
2023-12-20 21:55:59,902:INFO:Initializing RandomizedSearchCV
2023-12-20 21:56:06,250:INFO:best_params: {'actual_estimator__subsample': 0.25, 'actual_estimator__n_estimators': 140, 'actual_estimator__min_samples_split': 10, 'actual_estimator__min_samples_leaf': 2, 'actual_estimator__min_impurity_decrease': 0.0005, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 10, 'actual_estimator__learning_rate': 0.05}
2023-12-20 21:56:06,250:INFO:Hyperparameter search completed
2023-12-20 21:56:06,251:INFO:SubProcess create_model() called ==================================
2023-12-20 21:56:06,252:INFO:Initializing create_model()
2023-12-20 21:56:06,253:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197432141C0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.25, 'n_estimators': 140, 'min_samples_split': 10, 'min_samples_leaf': 2, 'min_impurity_decrease': 0.0005, 'max_features': 1.0, 'max_depth': 10, 'learning_rate': 0.05})
2023-12-20 21:56:06,253:INFO:Checking exceptions
2023-12-20 21:56:06,254:INFO:Importing libraries
2023-12-20 21:56:06,254:INFO:Copying training dataset
2023-12-20 21:56:06,258:INFO:Defining folds
2023-12-20 21:56:06,259:INFO:Declaring metric variables
2023-12-20 21:56:06,261:INFO:Importing untrained model
2023-12-20 21:56:06,262:INFO:Declaring custom model
2023-12-20 21:56:06,264:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 21:56:06,268:INFO:Starting cross validation
2023-12-20 21:56:06,270:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:56:09,857:INFO:Calculating mean and std
2023-12-20 21:56:09,858:INFO:Creating metrics dataframe
2023-12-20 21:56:09,862:INFO:Finalizing model
2023-12-20 21:56:14,073:INFO:Uploading results into container
2023-12-20 21:56:14,074:INFO:Uploading model into container now
2023-12-20 21:56:14,075:INFO:_master_model_container: 16
2023-12-20 21:56:14,076:INFO:_display_container: 3
2023-12-20 21:56:14,077:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=10,
                           max_features=1.0, max_leaf_nodes=None,
                           min_impurity_decrease=0.0005, min_samples_leaf=2,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=140, n_iter_no_change=None,
                           random_state=2960, subsample=0.25, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 21:56:14,077:INFO:create_model() successfully completed......................................
2023-12-20 21:56:14,116:INFO:SubProcess create_model() end ==================================
2023-12-20 21:56:14,117:INFO:choose_better activated
2023-12-20 21:56:14,119:INFO:SubProcess create_model() called ==================================
2023-12-20 21:56:14,119:INFO:Initializing create_model()
2023-12-20 21:56:14,120:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:56:14,120:INFO:Checking exceptions
2023-12-20 21:56:14,122:INFO:Importing libraries
2023-12-20 21:56:14,122:INFO:Copying training dataset
2023-12-20 21:56:14,126:INFO:Defining folds
2023-12-20 21:56:14,126:INFO:Declaring metric variables
2023-12-20 21:56:14,126:INFO:Importing untrained model
2023-12-20 21:56:14,127:INFO:Declaring custom model
2023-12-20 21:56:14,127:INFO:Gradient Boosting Classifier Imported successfully
2023-12-20 21:56:14,128:INFO:Starting cross validation
2023-12-20 21:56:14,128:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:56:16,197:INFO:Calculating mean and std
2023-12-20 21:56:16,198:INFO:Creating metrics dataframe
2023-12-20 21:56:16,200:INFO:Finalizing model
2023-12-20 21:56:18,671:INFO:Uploading results into container
2023-12-20 21:56:18,673:INFO:Uploading model into container now
2023-12-20 21:56:18,673:INFO:_master_model_container: 17
2023-12-20 21:56:18,674:INFO:_display_container: 4
2023-12-20 21:56:18,675:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 21:56:18,675:INFO:create_model() successfully completed......................................
2023-12-20 21:56:18,715:INFO:SubProcess create_model() end ==================================
2023-12-20 21:56:18,716:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8237
2023-12-20 21:56:18,717:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.05, loss='log_loss', max_depth=10,
                           max_features=1.0, max_leaf_nodes=None,
                           min_impurity_decrease=0.0005, min_samples_leaf=2,
                           min_samples_split=10, min_weight_fraction_leaf=0.0,
                           n_estimators=140, n_iter_no_change=None,
                           random_state=2960, subsample=0.25, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) result for Accuracy is 0.8229
2023-12-20 21:56:18,717:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False) is best model
2023-12-20 21:56:18,717:INFO:choose_better completed
2023-12-20 21:56:18,718:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-12-20 21:56:18,723:INFO:_master_model_container: 17
2023-12-20 21:56:18,723:INFO:_display_container: 3
2023-12-20 21:56:18,724:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=2960, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2023-12-20 21:56:18,725:INFO:tune_model() successfully completed......................................
2023-12-20 21:56:18,800:INFO:Initializing tune_model()
2023-12-20 21:56:18,801:INFO:tune_model(estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>)
2023-12-20 21:56:18,802:INFO:Checking exceptions
2023-12-20 21:56:18,815:INFO:Copying training dataset
2023-12-20 21:56:18,818:INFO:Checking base model
2023-12-20 21:56:18,818:INFO:Base model : Light Gradient Boosting Machine
2023-12-20 21:56:18,822:INFO:Declaring metric variables
2023-12-20 21:56:18,824:INFO:Defining Hyperparameters
2023-12-20 21:56:18,864:INFO:Tuning with n_jobs=-1
2023-12-20 21:56:18,865:INFO:Initializing RandomizedSearchCV
2023-12-20 21:56:35,520:INFO:best_params: {'actual_estimator__reg_lambda': 4, 'actual_estimator__reg_alpha': 0.7, 'actual_estimator__num_leaves': 10, 'actual_estimator__n_estimators': 230, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 21, 'actual_estimator__learning_rate': 0.2, 'actual_estimator__feature_fraction': 0.9, 'actual_estimator__bagging_freq': 1, 'actual_estimator__bagging_fraction': 0.9}
2023-12-20 21:56:35,521:INFO:Hyperparameter search completed
2023-12-20 21:56:35,522:INFO:SubProcess create_model() called ==================================
2023-12-20 21:56:35,523:INFO:Initializing create_model()
2023-12-20 21:56:35,705:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000197431FF790>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'reg_lambda': 4, 'reg_alpha': 0.7, 'num_leaves': 10, 'n_estimators': 230, 'min_split_gain': 0.9, 'min_child_samples': 21, 'learning_rate': 0.2, 'feature_fraction': 0.9, 'bagging_freq': 1, 'bagging_fraction': 0.9})
2023-12-20 21:56:35,705:INFO:Checking exceptions
2023-12-20 21:56:35,706:INFO:Importing libraries
2023-12-20 21:56:35,706:INFO:Copying training dataset
2023-12-20 21:56:35,711:INFO:Defining folds
2023-12-20 21:56:35,712:INFO:Declaring metric variables
2023-12-20 21:56:35,715:INFO:Importing untrained model
2023-12-20 21:56:35,716:INFO:Declaring custom model
2023-12-20 21:56:35,719:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 21:56:35,725:INFO:Starting cross validation
2023-12-20 21:56:35,727:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:56:36,665:INFO:Calculating mean and std
2023-12-20 21:56:36,667:INFO:Creating metrics dataframe
2023-12-20 21:56:36,672:INFO:Finalizing model
2023-12-20 21:56:36,684:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2023-12-20 21:56:36,685:INFO:[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9
2023-12-20 21:56:36,685:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2023-12-20 21:56:36,692:INFO:[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9
2023-12-20 21:56:36,693:INFO:[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9
2023-12-20 21:56:36,693:INFO:[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1
2023-12-20 21:56:36,697:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002341 seconds.
2023-12-20 21:56:36,698:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 21:56:36,699:INFO:[LightGBM] [Info] Total Bins 1790
2023-12-20 21:56:36,699:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 21:56:36,699:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 21:56:36,700:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 21:56:36,700:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 21:56:36,785:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,790:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,792:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,793:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,795:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,796:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,797:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,799:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,800:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,801:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,802:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,803:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,804:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,806:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,808:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,810:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,811:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,812:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,813:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,814:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,815:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,816:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,817:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,818:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,819:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,820:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,822:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,823:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,824:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,825:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,826:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,827:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,828:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,829:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,830:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,831:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,832:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,833:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,834:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,835:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,836:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,837:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,840:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,841:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,842:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,844:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,845:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,846:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,848:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,849:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,850:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,851:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,854:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,855:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,856:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,858:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,859:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,860:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,861:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,862:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,863:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,864:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,865:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,866:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,867:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,868:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,871:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,872:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,873:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,874:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,875:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,875:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,876:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,877:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,878:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,878:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,880:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,880:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,881:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,882:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,883:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,884:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,885:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,886:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,887:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,888:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,888:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,890:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,891:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,892:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,894:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,895:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,896:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,897:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,898:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,899:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,900:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,901:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,902:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,902:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,903:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,903:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,904:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,905:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,905:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,906:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,907:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,908:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,910:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,911:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,911:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,912:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,913:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,914:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,914:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,915:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,916:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,916:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,917:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,918:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,919:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,920:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,921:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,922:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,922:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,923:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,924:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,925:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,925:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,926:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,927:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,927:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,928:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,928:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,930:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,931:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,931:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,932:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,933:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,934:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,935:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,936:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,936:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,937:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,938:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,938:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,939:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,939:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,940:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,940:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,941:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,942:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,943:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,944:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,945:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,945:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,946:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,947:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,948:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,948:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,950:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,951:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,951:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,952:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,953:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,954:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,955:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,955:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,956:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,956:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,957:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,958:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,959:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,960:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,961:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,962:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,963:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,963:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,964:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,964:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,965:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,966:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,967:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,968:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,969:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,969:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,970:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,971:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,971:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,972:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,972:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,973:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,974:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,974:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,975:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,975:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,976:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,977:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,977:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,978:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,978:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,979:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,979:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,980:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,981:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,981:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,982:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,983:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,984:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,986:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,986:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,987:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,987:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,988:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,989:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,990:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,991:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,991:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,992:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,992:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,993:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,994:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,995:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,995:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,996:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,997:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,997:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,998:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,999:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:36,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:36,999:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,000:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,001:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,001:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,002:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,003:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,004:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,005:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,006:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,007:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,007:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,008:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,009:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,010:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,010:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,011:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,012:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,013:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,013:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,014:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,015:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,016:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,016:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,017:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,018:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,020:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,020:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,021:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,021:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,022:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,023:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,023:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,024:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,025:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,025:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,026:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,027:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,027:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,028:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,029:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,030:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,030:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,031:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,032:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,033:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,033:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,034:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,035:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,035:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,036:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,037:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,038:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,038:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,039:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,039:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,040:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,041:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,042:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,043:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,044:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,045:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,045:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,046:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,046:INFO:[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements
2023-12-20 21:56:37,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,047:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,048:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,049:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,050:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,051:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,052:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-12-20 21:56:37,076:INFO:Uploading results into container
2023-12-20 21:56:37,077:INFO:Uploading model into container now
2023-12-20 21:56:37,078:INFO:_master_model_container: 18
2023-12-20 21:56:37,078:INFO:_display_container: 4
2023-12-20 21:56:37,079:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=230, n_jobs=-1, num_leaves=10, objective=None,
               random_state=2960, reg_alpha=0.7, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 21:56:37,079:INFO:create_model() successfully completed......................................
2023-12-20 21:56:37,153:INFO:SubProcess create_model() end ==================================
2023-12-20 21:56:37,153:INFO:choose_better activated
2023-12-20 21:56:37,157:INFO:SubProcess create_model() called ==================================
2023-12-20 21:56:37,158:INFO:Initializing create_model()
2023-12-20 21:56:37,158:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>, estimator=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0), fold=StratifiedKFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2023-12-20 21:56:37,158:INFO:Checking exceptions
2023-12-20 21:56:37,159:INFO:Importing libraries
2023-12-20 21:56:37,159:INFO:Copying training dataset
2023-12-20 21:56:37,165:INFO:Defining folds
2023-12-20 21:56:37,166:INFO:Declaring metric variables
2023-12-20 21:56:37,166:INFO:Importing untrained model
2023-12-20 21:56:37,166:INFO:Declaring custom model
2023-12-20 21:56:37,167:INFO:Light Gradient Boosting Machine Imported successfully
2023-12-20 21:56:37,167:INFO:Starting cross validation
2023-12-20 21:56:37,168:INFO:Cross validating with StratifiedKFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2023-12-20 21:56:40,182:INFO:Calculating mean and std
2023-12-20 21:56:40,183:INFO:Creating metrics dataframe
2023-12-20 21:56:40,186:INFO:Finalizing model
2023-12-20 21:56:40,206:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001646 seconds.
2023-12-20 21:56:40,207:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-12-20 21:56:40,208:INFO:[LightGBM] [Info] Total Bins 1790
2023-12-20 21:56:40,208:INFO:[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 33
2023-12-20 21:56:40,209:INFO:[LightGBM] [Info] Start training from score -0.465082
2023-12-20 21:56:40,210:INFO:[LightGBM] [Info] Start training from score -3.358480
2023-12-20 21:56:40,210:INFO:[LightGBM] [Info] Start training from score -1.087291
2023-12-20 21:56:40,602:INFO:Uploading results into container
2023-12-20 21:56:40,603:INFO:Uploading model into container now
2023-12-20 21:56:40,604:INFO:_master_model_container: 19
2023-12-20 21:56:40,604:INFO:_display_container: 5
2023-12-20 21:56:40,605:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 21:56:40,605:INFO:create_model() successfully completed......................................
2023-12-20 21:56:40,679:INFO:SubProcess create_model() end ==================================
2023-12-20 21:56:40,680:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=2960, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8226
2023-12-20 21:56:40,681:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=230, n_jobs=-1, num_leaves=10, objective=None,
               random_state=2960, reg_alpha=0.7, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) result for Accuracy is 0.8278
2023-12-20 21:56:40,682:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=230, n_jobs=-1, num_leaves=10, objective=None,
               random_state=2960, reg_alpha=0.7, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0) is best model
2023-12-20 21:56:40,682:INFO:choose_better completed
2023-12-20 21:56:40,690:INFO:_master_model_container: 19
2023-12-20 21:56:40,690:INFO:_display_container: 4
2023-12-20 21:56:40,691:INFO:LGBMClassifier(bagging_fraction=0.9, bagging_freq=1, boosting_type='gbdt',
               class_weight=None, colsample_bytree=1.0, feature_fraction=0.9,
               importance_type='split', learning_rate=0.2, max_depth=-1,
               min_child_samples=21, min_child_weight=0.001, min_split_gain=0.9,
               n_estimators=230, n_jobs=-1, num_leaves=10, objective=None,
               random_state=2960, reg_alpha=0.7, reg_lambda=4, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2023-12-20 21:56:40,692:INFO:tune_model() successfully completed......................................
2023-12-20 21:56:40,773:INFO:Initializing tune_model()
2023-12-20 21:56:40,774:INFO:tune_model(estimator=XGBClassifier(base_score=None, booster='gbtree', callbacks=None,
              colsample_bylevel=None, colsample_bynode=None,
              colsample_bytree=None, device='cpu', early_stopping_rounds=None,
              enable_categorical=False, eval_metric=None, feature_types=None,
              gamma=None, grow_policy=None, importance_type=None,
              interaction_constraints=None, learning_rate=None, max_bin=None,
              max_cat_threshold=None, max_cat_to_onehot=None,
              max_delta_step=None, max_depth=None, max_leaves=None,
              min_child_weight=None, missing=nan, monotone_constraints=None,
              multi_strategy=None, n_estimators=None, n_jobs=-1,
              num_parallel_tree=None, objective='multi:softprob', ...), fold=None, round=4, n_iter=10, custom_grid=None, optimize=Accuracy, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000019743699E10>)
2023-12-20 21:56:40,774:INFO:Checking exceptions
2023-12-20 21:56:40,787:INFO:Copying training dataset
2023-12-20 21:56:40,790:INFO:Checking base model
2023-12-20 21:56:40,790:INFO:Base model : Extreme Gradient Boosting
2023-12-20 21:56:40,792:INFO:Declaring metric variables
2023-12-20 21:56:40,795:INFO:Defining Hyperparameters
2023-12-20 21:56:40,838:INFO:Tuning with n_jobs=-1
2023-12-20 21:56:40,840:INFO:Initializing RandomizedSearchCV
